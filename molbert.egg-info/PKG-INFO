Metadata-Version: 2.1
Name: molbert
Version: 0.0.1
Summary: Language modelling on chem/bio sequences
Home-page: https://github.com/BenevolentAI/MolBERT
Author: BenevolentAI
Author-email: chemval@benevolent.ai
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: OS Independent
Description-Content-Type: text/markdown
License-File: LICENSE

# ToxBERT - Modeling DILI by pretraining BERT on invitro data

This repository contains the code of ToxBERT, a pretrained BERT based model with the ability to use biological and chemical data during pretraining stage

<p align="center">
  <img src="ToxBERT.png" width="50%" alt="ToxBERT Architecture"/>
</p>

## Table of Contents
- [Installation](#installation)
- [Data Preparation](#data-preparation)
- [Model Architecture](#model-architecture)
- [Training the Model](#training-the-model)
- [Reproducing Results](#reproducing-results)
- [Making Predictions](#making-predictions)
- [Results](#results)
- [Citation](#citation)

## Installation

1. Clone the repository:
```bash
git clone https://github.com/aidd-msca/ToxBERT.git
cd ToxBERT
```

2. Create and activate a conda environment:
```bash
conda create -y -q -n ToxBERT -c rdkit rdkit=2019.03.1.0 python=3.7.3
conda activate ToxBERT
```
if Aalto University (triton):
```bash
module load mamba # specific to Aalto University (triton)
source activate ToxBERT
```

3. Install dependencies:
```bash
pip install -e . 
```

## Data Preparation

The model requires two types of data:

1. Pretraining data: In-vitro data with binary labels
2. Fine-tuning data: Preclinical and Clinical data with binary labels

### Data Format
The input data should be in pickle (.pkl) format with the following structure:
```python
{
    'SMILES': [...],  # List of SMILES strings
    'property1': [...],  # Binary labels (0 or 1)
    'property2': [...],
    # ... additional properties
}
```

### Example Data Preparation
```python
import pandas as pd

# Load your data
data = pd.read_csv('your_data.csv')

# Process SMILES and properties
processed_data = {
    'SMILES': data['smiles'].values,
    'property1': data['prop1'].values.astype(int),
    'property2': data['prop2'].values.astype(int)
}

# Save as pickle
pd.to_pickle(processed_data, 'processed_data.pkl')
```

Place your prepared data files in the `data/` directory:
```
data/
├── train_set_invitro_1m_300k_ADME_filtered.pkl
├── test_set_invitro_1m_300k_ADME_filtered.pkl
└── pos_weights.csv
```

## Model Architecture

The model consists of three main components:
1. BERT encoder for molecular representation
2. Masked language modeling head for pre-training
3. Task-specific heads for:
   - ADME property prediction
   - Physicochemical property prediction

Architecture details:
- BERT output dimension: 768
- Maximum sequence length: 128
- Hidden layer size: 2048
- Number of attention heads: 12

## Training the Model

1. Configure your training parameters in `config/default_config.yaml`:
```yaml
project_name: "BERT_invitro_pretraining"
model_name: "with_masking_invitro_physchem_heads"
max_epochs: 50
batch_size: 264
lr: 1e-05
```

2. Set up environment variables:
```bash
export MODEL_WEIGHTS_DIR="/path/to/weights"
export DATA_DIR="/path/to/data"
export WANDB_API_KEY="your_wandb_key"  # Optional, for logging
```

3. Start training:
```bash
python scripts/train.py --config config/default_config.yaml
```

## Reproducing Results by using public data

To reproduce our results:

1. Download the preprocessed dataset:
```bash
# Add link to download dataset
wget https://example.com/adme_dataset.zip
unzip adme_dataset.zip -d data/
```

2. Use our exact configuration:
```bash
# Clone specific version
git clone -b v1.0.0 https://github.com/yourusername/bert-invitro-adme.git

# Use provided configuration
cp configs/reproduction_config.yaml config/default_config.yaml
```

3. Train the model:
```bash
python scripts/train.py --config config/default_config.yaml
```

Expected Results:
- ROC-AUC: 0.85 ± 0.02
- PR-AUC: 0.82 ± 0.03
- F1 Score: 0.79 ± 0.02

## Making Predictions

Here's how to use the trained model for predictions:

```python
from src.models import MolbertModel
from src.utils.featurizer import SmilesIndexFeaturizer

# Load model
model = MolbertModel.load_from_checkpoint('path/to/checkpoint.ckpt')
model.eval()

# Prepare input
featurizer = SmilesIndexFeaturizer(max_length=128)
smiles = "CC1=CC=C(C=C1)C2=CC(=NN2C3=CC=C(C=C3)S(N)(=O)=O)C(F)(F)F"
encoded = featurizer.encode(smiles)

# Make prediction
with torch.no_grad():
    prediction = model.predict(encoded)
```

## Results

Our model achieves the following performance on the test set:

| Metric | Value |
|--------|-------|
| ROC-AUC | 0.85 |
| PR-AUC | 0.82 |
| F1 Score | 0.79 |

Detailed results for individual ADME properties:
[Include detailed results table]

## Common Issues and Solutions

1. CUDA Out of Memory
```bash
# Reduce batch size in config/default_config.yaml
batch_size: 132  # Default: 264
```

2. Slow Training
```bash
# Increase number of workers
num_workers: 24
```

## Citation

If you use this code in your research, please cite:
```bibtex
@article{ToxBERT,
    title={ToxBERT - Modeling DILI by pretraining BERT on invitro data},
    author={Muhammad Arslan Masood, Samuel Kaski, Anamya Ajjolli Nagaraja, Katia Belaid, Natalie Mesens, Hugo Ceulemans, Dorota Herman, Markus Heinonen},
    journal={under review},
    year={2025}
}
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Contact

Muhammad Arslan Masood - arslan.asood@aalto.fi
