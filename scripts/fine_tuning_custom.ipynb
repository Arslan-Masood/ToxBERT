{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, yaml\n",
    "from typing import Dict, Tuple, List\n",
    "from argparse import Namespace\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import average_precision_score,precision_recall_curve, auc, roc_curve\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, StepLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "from transformers.modeling_bert import BertEncoder, BertPooler\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_constant_schedule_with_warmup,\n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from molbert.models.base import SuperPositionalBertEmbeddings\n",
    "from molbert.utils.lm_utils import BertConfigExtras\n",
    "from molbert.tasks.tasks import BaseTask, FinetuneTask\n",
    "\n",
    "from molbert.utils.featurizer.molfeaturizer import SmilesIndexFeaturizer\n",
    "from molbert.datasets.finetune import BertFinetuneSmilesDataset\n",
    "from molbert.datasets.dataloading import MolbertDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "MolbertBatchType = Tuple[Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]], torch.Tensor]\n",
    "class SuperPositionalBertModel(BertModel):\n",
    "    \"\"\"\n",
    "    Same as BertModel, BUT\n",
    "    uses SuperPositionalBertEmbeddings instead of BertEmbeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertModel, self).__init__(config)\n",
    "\n",
    "        self.embeddings = SuperPositionalBertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "        self.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuneHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.finetune_net = nn.Sequential(nn.Linear(config.hidden_size, config.output_size))\n",
    "\n",
    "    def forward(self, pooled_output):\n",
    "        return self.finetune_net(pooled_output)\n",
    "    \n",
    "class MolbertModel(pl.LightningModule):\n",
    "    def __init__(self, args: Namespace):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.training_step_ytrue, self.training_step_ypred = [],[]\n",
    "        self.val_step_ytrue, self.val_step_ypred = [],[]\n",
    "\n",
    "        self.hparams = args\n",
    "        self.config = self.get_config()\n",
    "        #self.tasks = self.get_tasks(self.config)\n",
    "        #print(self.tasks)\n",
    "\n",
    "        self.get_creterian(args)\n",
    "        self.bert = SuperPositionalBertModel(self.config)\n",
    "        self.bert.init_weights()\n",
    "        self.bert = self.load_model_weights(\n",
    "                                model=self.bert, \n",
    "                                checkpoint_file=self.hparams.pretrained_model_path)\n",
    "\n",
    "        self.head = FinetuneHead(self.config)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        sequence_output, pooled_output = self.bert(\n",
    "        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        logits = self.head(pooled_output)\n",
    "        return logits\n",
    "        #return {task.name: task(sequence_output, pooled_output) for task in self.tasks}\n",
    "    \n",
    "    def get_creterian(self, config):\n",
    "        # pos weights\n",
    "        \n",
    "        pos_weights = pd.read_csv(config[\"pos_weights\"])\n",
    "        if self.hparams.num_of_tasks == 1:\n",
    "            pos_weights = pos_weights.set_index(\"Targets\").reindex([config[\"selected_tasks\"]]).weights.values\n",
    "        else:\n",
    "            pos_weights = pos_weights.set_index(\"Targets\").reindex(config[\"selected_tasks\"]).weights.values\n",
    "        pos_weights = (config[\"alpha\"] * pos_weights) + (1 - config[\"alpha\"])*1\n",
    "        self.pos_weights = torch.tensor(pos_weights, device = config[\"device\"])\n",
    "\n",
    "        # class weights\n",
    "        if self.hparams.num_of_tasks > 1:\n",
    "            class_weights = pd.read_csv(config[\"class_weights\"])\n",
    "            class_weights = class_weights.set_index(\"Targets\").reindex(config[\"selected_tasks\"]).weights.values\n",
    "            class_weights = (config[\"beta\"] * class_weights) + (1 - config[\"beta\"])*1\n",
    "            self.class_weights = torch.tensor(class_weights, device = config[\"device\"])\n",
    "        else:\n",
    "            self.class_weights = torch.tensor([1.0], device = config[\"device\"])\n",
    "\n",
    "        # train_weighted loss, validation no weights\n",
    "        self.weighted_creterien =  nn.BCEWithLogitsLoss(reduction=\"none\", \n",
    "                                                        pos_weight= self.pos_weights,\n",
    "                                                        weight= self.class_weights)\n",
    "        \n",
    "        self.non_weighted_creterian =  nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "\n",
    "        scheduler = self._initialise_lr_scheduler(optimizer)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def _initialise_lr_scheduler(self, optimizer):\n",
    "\n",
    "        #num_batches = len(self.trainer.train_dataloader) // self.hparams.batch_size\n",
    "        num_batches = self.hparams.num_batches\n",
    "        num_training_steps = num_batches // self.hparams.accumulate_grad_batches * self.hparams.max_epochs\n",
    "        warmup_steps = int(num_training_steps * self.hparams.warmup_proportion)\n",
    "\n",
    "        if self.hparams.learning_rate_scheduler == 'linear_with_warmup':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps\n",
    "            )\n",
    "        elif self.hparams.learning_rate_scheduler == 'cosine_with_hard_restarts_warmup':\n",
    "            scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps, num_cycles=1\n",
    "            )\n",
    "        elif self.hparams.learning_rate_scheduler == 'cosine_schedule_with_warmup':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps\n",
    "            )\n",
    "        elif self.hparams.learning_rate_scheduler == 'constant_schedule_with_warmup':\n",
    "            scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)\n",
    "\n",
    "        elif self.hparams.learning_rate_scheduler == 'cosine_annealing_warm_restarts':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, warmup_steps)\n",
    "        elif self.hparams.learning_rate_scheduler == 'reduce_on_plateau':\n",
    "            scheduler = ReduceLROnPlateau(optimizer)\n",
    "        elif self.hparams.learning_rate_scheduler == 'constant':\n",
    "            scheduler = StepLR(optimizer, 10, gamma=1.0)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'learning_rate_scheduler needs to be one of '\n",
    "                f'linear_with_warmup, cosine_with_hard_restarts_warmup, cosine_schedule_with_warmup, '\n",
    "                f'constant_schedule_with_warmup, cosine_annealing_warm_restarts, reduce_on_plateau, '\n",
    "                f'step_lr. '\n",
    "                f'Given: {self.hparams.learning_rate_scheduler}'\n",
    "            )\n",
    "\n",
    "        return {'scheduler': scheduler, 'monitor': 'valid_loss', 'interval': 'step', 'frequency': 1}\n",
    "    \n",
    "    def l2_regularization(self):\n",
    "        device = torch.device('cuda')\n",
    "        l2_reg = torch.tensor(0., requires_grad=True, device=device)\n",
    "\n",
    "        # Apply only on weights, exclude bias\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                l2_reg = l2_reg + torch.norm(param, p=2)\n",
    "        return l2_reg\n",
    "    \n",
    "    def _compute_loss(self, y, y_hat):\n",
    "        if self.hparams.num_of_tasks == 1:\n",
    "            y = y.unsqueeze(1)\n",
    "        # compute losses, wiht masking\n",
    "        if self.hparams.missing == 'nan':\n",
    "            nan_mask = torch.isnan(y)\n",
    "            y[nan_mask] = -1\n",
    "            #y = torch.nan_to_num(y, nan = -1), for newer version\n",
    "        \n",
    "        # masks\n",
    "        valid_label_mask = (y != -1).float()\n",
    "        pos_label_mask = (y == 1)\n",
    "        negative_label_mask = (y == 0)\n",
    "\n",
    "        weighted_loss = self.weighted_creterien(y_hat, y) * valid_label_mask\n",
    "        Non_weighted_loss = self.non_weighted_creterian(y_hat, y) * valid_label_mask\n",
    "        \n",
    "        # Non_weighted_loss, positive negative loss\n",
    "        pos_loss = Non_weighted_loss * pos_label_mask\n",
    "        neg_loss = Non_weighted_loss * negative_label_mask\n",
    "        pos_loss = pos_loss.sum() / pos_label_mask.sum()\n",
    "        neg_loss = neg_loss.sum() / negative_label_mask.sum()\n",
    "    \n",
    "        # compute mean loss\n",
    "        Non_weighted_loss = Non_weighted_loss.sum() / valid_label_mask.sum()\n",
    "        weighted_loss = weighted_loss.sum() / valid_label_mask.sum()\n",
    "\n",
    "        l2_reg_loss = self.l2_regularization()\n",
    "        l2_reg_loss = self.hparams.l2_lambda*l2_reg_loss\n",
    "        total_loss = weighted_loss + l2_reg_loss\n",
    "\n",
    "        return total_loss, weighted_loss, Non_weighted_loss,l2_reg_loss, pos_loss, neg_loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # compute forward pass\n",
    "        (batch_inputs, batch_labels), _ = batch\n",
    "        y = batch_labels[\"finetune\"].squeeze()\n",
    "        y_hat = self.forward(**batch_inputs)\n",
    "\n",
    "        # compute loss\n",
    "        total_loss, weighted_loss, Non_weighted_loss,l2_reg_loss, pos_loss, neg_loss = self._compute_loss(y, y_hat)  \n",
    "        self.training_step_ytrue.append(y.long().cpu())\n",
    "        self.training_step_ypred.append(torch.sigmoid(y_hat).cpu())\n",
    "\n",
    "        return {\"loss\": total_loss,\n",
    "                \"weighted_loss\":weighted_loss,\n",
    "                \"Non_weighted_loss\":Non_weighted_loss,\n",
    "                \"l2_reg_loss\":l2_reg_loss, \n",
    "                \"pos_loss\":pos_loss, \n",
    "                \"neg_loss\":neg_loss\n",
    "                }\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # compute forward pass\n",
    "        (batch_inputs, batch_labels), _ = batch\n",
    "        y = batch_labels[\"finetune\"].squeeze()\n",
    "        y_hat = self.forward(**batch_inputs)\n",
    "\n",
    "        # compute loss\n",
    "        total_loss, weighted_loss, Non_weighted_loss,l2_reg_loss, pos_loss, neg_loss = self._compute_loss(y, y_hat)  \n",
    "        self.val_step_ytrue.append(y.long().cpu())\n",
    "        self.val_step_ypred.append(torch.sigmoid(y_hat).cpu())\n",
    "\n",
    "        return {\"loss\": total_loss,\n",
    "                \"weighted_loss\":weighted_loss,\n",
    "                \"Non_weighted_loss\":Non_weighted_loss,\n",
    "                \"l2_reg_loss\":l2_reg_loss, \n",
    "                \"pos_loss\":pos_loss, \n",
    "                \"neg_loss\":neg_loss\n",
    "                }\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        avg_weighted_loss = torch.stack([x['weighted_loss'] for x in outputs]).mean()\n",
    "        avg_non_weighted_loss = torch.stack([x['Non_weighted_loss'] for x in outputs]).mean()\n",
    "        avg_l2_reg_loss = torch.stack([x['l2_reg_loss'] for x in outputs]).mean()\n",
    "        avg_pos_loss = torch.stack([x['pos_loss'] for x in outputs]).mean()\n",
    "        avg_neg_loss = torch.stack([x['neg_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\n",
    "                    'train_total_loss': avg_loss,\n",
    "                    'train_weighted_loss': avg_weighted_loss,\n",
    "                    'train_Non_weighted_loss': avg_non_weighted_loss,\n",
    "                    'train_l2_reg_loss': avg_l2_reg_loss,\n",
    "                    'train_pos_loss': avg_pos_loss,\n",
    "                    'train_neg_loss': avg_neg_loss\n",
    "                    }\n",
    "        wandb.log(tensorboard_logs)\n",
    "\n",
    "        # Log the learning rate at the end of each epoch\n",
    "        lr = self.trainer.optimizers[0].param_groups[0]['lr']\n",
    "        wandb.log({'learning_rate': lr})\n",
    "        \n",
    "        # Collect predictions and true labels for the complete training set\n",
    "        train_true = torch.cat(self.training_step_ytrue, dim=0)\n",
    "        train_preds = torch.cat(self.training_step_ypred, dim=0)\n",
    "\n",
    "        score_list =  self.compute_metrics(train_true, train_preds)\n",
    "        metric = ['roc_score', 'blc_acc', 'sensitivity', 'specificity', 'AUPR', 'f1_score', 'average_precision']\n",
    "            \n",
    "        for i, score in enumerate(score_list):\n",
    "                wandb.log({f'train_{metric[i]}':score.item()})\n",
    "        \n",
    "        # Clear the lists to free memory for the next epoch\n",
    "        self.training_step_ytrue.clear()\n",
    "        self.training_step_ypred.clear()\n",
    "        del train_true,train_preds\n",
    "\n",
    "        return {\"avg_loss\":avg_loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        avg_weighted_loss = torch.stack([x['weighted_loss'] for x in outputs]).mean()\n",
    "        avg_non_weighted_loss = torch.stack([x['Non_weighted_loss'] for x in outputs]).mean()\n",
    "        avg_l2_reg_loss = torch.stack([x['l2_reg_loss'] for x in outputs]).mean()\n",
    "        avg_pos_loss = torch.stack([x['pos_loss'] for x in outputs]).mean()\n",
    "        avg_neg_loss = torch.stack([x['neg_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\n",
    "                    'val_total_loss': avg_loss,\n",
    "                    'val_weighted_loss': avg_weighted_loss,\n",
    "                    'val_Non_weighted_loss': avg_non_weighted_loss,\n",
    "                    'val_l2_reg_loss': avg_l2_reg_loss,\n",
    "                    'val_pos_loss': avg_pos_loss,\n",
    "                    'val_neg_loss': avg_neg_loss\n",
    "                    }\n",
    "        wandb.log(tensorboard_logs)\n",
    "\n",
    "        #Collect predictions and true labels for the complete training set\n",
    "        val_true = torch.cat(self.val_step_ytrue, dim=0)\n",
    "        val_preds = torch.cat(self.val_step_ypred, dim=0)\n",
    "\n",
    "        score_list =  self.compute_metrics(val_true,val_preds)\n",
    "        metric = ['roc_score', 'blc_acc', 'sensitivity', 'specificity', 'AUPR', 'f1_score', 'average_precision']\n",
    "            \n",
    "        for i, score in enumerate(score_list):\n",
    "            wandb.log({f'val_{metric[i]}':score.item()})\n",
    "\n",
    "        # Clear the lists to free memory for the next epoch\n",
    "        self.val_step_ytrue.clear()\n",
    "        self.val_step_ypred.clear()\n",
    "        del val_true, val_preds\n",
    "\n",
    "    def compute_metrics(self, y_true, y_pred): \n",
    "        device = torch.device(\"cuda\") \n",
    "        self.eval()\n",
    "\n",
    "        targets =  y_true.cpu().detach().tolist()\n",
    "        preds = y_pred.cpu().detach().tolist()\n",
    "\n",
    "        targets = np.array(targets).reshape(-1,self.hparams.num_of_tasks)\n",
    "        preds = np.array(preds).reshape(-1,self.hparams.num_of_tasks)\n",
    "\n",
    "        if self.hparams.missing == 'nan':\n",
    "            mask = ~np.isnan(targets)\n",
    "\n",
    "        roc_score, blc_acc, sensitivity, specificity, AUPR, f1_score, average_precision = [],[],[],[],[],[],[]\n",
    "        for i in range(self.hparams.num_of_tasks):\n",
    "                \n",
    "                # get valid targets, and convert logits to prob\n",
    "                valid_targets = targets[:,i][mask[:,i]]\n",
    "                valid_preds = expit(preds[:,i][mask[:,i]])\n",
    "                try:\n",
    "                    # ROC_AUC\n",
    "                    fpr, tpr, th = roc_curve(valid_targets, valid_preds)\n",
    "                    roc_score.append(auc(fpr, tpr))\n",
    "\n",
    "                    # Balanced accuracy\n",
    "                    balanced_accuracy = (tpr + (1 - fpr)) / 2\n",
    "                    blc_acc.append(np.max(balanced_accuracy))\n",
    "\n",
    "                    # sensitivity, specificity\n",
    "                    optimal_threshold_index = np.argmax(balanced_accuracy)\n",
    "                    optimal_threshold = th[optimal_threshold_index]\n",
    "                    sensitivity.append(tpr[optimal_threshold_index])\n",
    "                    specificity.append(1 - fpr[optimal_threshold_index])\n",
    "\n",
    "                    # AUPR, F1\n",
    "                    precision, recall, thresholds = precision_recall_curve(valid_targets, valid_preds)\n",
    "                    AUPR.append(auc(recall, precision))\n",
    "                    f1 = [f1_score(valid_targets, self.prob_to_labels(valid_preds, t)) for t in self.thresholds]\n",
    "                    f1_score.append(np.nanmax(f1))\n",
    "                    average_precision.append(average_precision_score(valid_targets, valid_preds))\n",
    "                    \n",
    "                except:\n",
    "                    roc_score.append(np.nan)\n",
    "                    AUPR.append(np.nan)\n",
    "                    average_precision.append(np.nan)\n",
    "                    #print('Performance metric is null')\n",
    "                \n",
    "        self.train()\n",
    "        return np.nanmean(roc_score), np.nanmean(blc_acc), np.nanmean(sensitivity), np.nanmean(specificity), np.nanmean(AUPR), np.nanmean(f1_score), np.nanmean(average_precision)\n",
    "\n",
    "    \n",
    "    def prob_to_labels(self, pred, threshold):\n",
    "\t    return (pred >= threshold).astype('int')\n",
    "    \n",
    "    def load_model_weights(self, model, checkpoint_file):\n",
    "        \"\"\"\n",
    "        PL `load_from_checkpoint` seems to fail to reload model weights. This function loads them manually.\n",
    "        See: https://github.com/PyTorchLightning/pytorch-lightning/issues/525\n",
    "        \"\"\"\n",
    "        print(f'Loading model weights from {checkpoint_file}')\n",
    "        checkpoint = torch.load(checkpoint_file, map_location=lambda storage, loc: storage)\n",
    "\n",
    "        # load weights from checkpoint, strict=False allows to ignore some weights\n",
    "        # e.g. weights of a head that was used during pretraining but isn't present during finetuning\n",
    "        # and also allows to missing keys in the checkpoint, e.g. heads that are used for finetuning\n",
    "        # but weren't present during pretraining\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "        return model\n",
    "\n",
    "    def get_config(self):\n",
    "        if not hasattr(self.hparams, 'vocab_size') or not self.hparams.vocab_size:\n",
    "            self.hparams.vocab_size = 42\n",
    "\n",
    "        if self.hparams.tiny:\n",
    "            config = BertConfigExtras(\n",
    "                vocab_size_or_config_json_file=self.hparams.vocab_size,\n",
    "                hidden_size=16,\n",
    "                num_hidden_layers=2,\n",
    "                num_attention_heads=2,\n",
    "                intermediate_size=32,\n",
    "                max_position_embeddings=self.hparams.max_position_embeddings,\n",
    "                mode=self.hparams.mode,\n",
    "                output_size=self.hparams.output_size,\n",
    "                label_column=self.hparams.label_column,\n",
    "            )\n",
    "        else:\n",
    "            config = BertConfigExtras(\n",
    "                vocab_size_or_config_json_file=self.hparams.vocab_size,\n",
    "                hidden_size=768,\n",
    "                num_hidden_layers=12,\n",
    "                num_attention_heads=12,\n",
    "                intermediate_size=3072,\n",
    "                max_position_embeddings=self.hparams.max_position_embeddings,\n",
    "                mode=self.hparams.mode,\n",
    "                output_size=self.hparams.output_size,\n",
    "                label_column=self.hparams.label_column,\n",
    "            )\n",
    "        return config\n",
    "    \n",
    "    def get_tasks(self, config):\n",
    "        \"\"\" Task list should be converted to nn.ModuleList before, not done here to hide params from torch \"\"\"\n",
    "        tasks: List[BaseTask] = [FinetuneTask(name='finetune', config=config)]\n",
    "\n",
    "        return tasks\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model_weights_dir = '/projects/home/mmasood1/Model_weights/preclinical_clinical/BERT/'\n",
    "pretrained_model_path = '/projects/home/mmasood1/TG GATE/MolBERT/molbert/molbert_100epochs/molbert_100epochs/checkpoints/last.ckpt'\n",
    "data_dir = '/projects/home/mmasood1/arslan_data_repository/Mix_clinical_pre_clinical/Data_for_BERT_finetuning/'\n",
    "pos_weights = \"/projects/home/mmasood1/arslan_data_repository/Mix_clinical_pre_clinical/06_10_2023/pos_weights.csv\"\n",
    "class_weights = \"/projects/home/mmasood1/arslan_data_repository/Mix_clinical_pre_clinical/06_10_2023/target_weights.csv\"\n",
    "metadata_dir = \"/projects/home/mmasood1/trained_model_predictions/SIDER_PreClinical/BERT_finetune/\"\n",
    "model_dir = os.path.dirname(os.path.dirname(pretrained_model_path))\n",
    "hparams_path = os.path.join(model_dir, 'hparams.yaml')\n",
    "\n",
    "# load config\n",
    "with open(hparams_path) as yaml_file:\n",
    "    config_dict = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "config_dict['model_weights_dir'] = model_weights_dir\n",
    "config_dict['pretrained_model_path'] = pretrained_model_path\n",
    "config_dict[\"metadata_dir\"] = metadata_dir\n",
    "config_dict['pos_weights'] = pos_weights\n",
    "config_dict['class_weights'] = class_weights\n",
    "\n",
    "config_dict['data_dir'] = data_dir\n",
    "config_dict['train_file'] = data_dir + \"complete_training_set.csv\"\n",
    "config_dict['valid_file'] = data_dir + \"complete_test_set.csv\"\n",
    "config_dict['test_file'] = data_dir + \"complete_test_set.csv\"\n",
    "\n",
    "config_dict['mode'] = 'classification'\n",
    "config_dict['alpha'] = 1.0\n",
    "config_dict['beta'] = 0\n",
    "config_dict['epochs'] = 1\n",
    "config_dict[\"l2_lambda\"] = 0.0\n",
    "config_dict['missing'] = 'nan'\n",
    "config_dict['compute_metric_after_n_epochs'] = 5\n",
    "config_dict['return_trainer'] = True\n",
    "config_dict['EarlyStopping'] = False\n",
    "config_dict['project_name'] = \"BERT_Testing\"\n",
    "config_dict['model_name'] = \"Test\"\n",
    "\n",
    "config_dict[\"accelerator\"] = \"gpu\"\n",
    "config_dict[\"gpu\"] =  [0]\n",
    "config_dict[\"device\"] = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "data = pd.read_csv(config_dict[\"data_dir\"] + \"train_fold0.csv\")\n",
    "target_names = data.loc[:,\"Cytoplasmic alteration (Basophilic/glycogen depletion)\":\"hepatobiliary_disorders\"].columns.tolist()\n",
    "#target_names = data.loc[:,\"DILI_binary\":\"hepatobiliary_disorders\"].columns.tolist()\n",
    "config_dict[\"output_size\"] = len(target_names)\n",
    "config_dict[\"label_column\"] = target_names\n",
    "\n",
    "config_dict[\"num_of_tasks\"] = len(target_names)\n",
    "config_dict[\"selected_tasks\"] = target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from molbert.datasets.finetune import BertFinetuneSmilesDataset\n",
    "from molbert.utils.featurizer.molfeaturizer import SmilesIndexFeaturizer\n",
    "\n",
    "featurizer = SmilesIndexFeaturizer.bert_smiles_index_featurizer(config_dict[\"max_seq_length\"])\n",
    "\n",
    "\n",
    "train_dataset = BertFinetuneSmilesDataset(\n",
    "            input_path= config_dict['train_file'],\n",
    "            featurizer=featurizer,\n",
    "            single_seq_len=config_dict[\"max_seq_length\"],\n",
    "            total_seq_len=config_dict[\"max_seq_length\"],\n",
    "            label_column=config_dict[\"label_column\"],\n",
    "            is_same=False,\n",
    "            inference_mode=True\n",
    "        )\n",
    "\n",
    "validation_dataset = BertFinetuneSmilesDataset(\n",
    "            input_path= config_dict['valid_file'],\n",
    "            featurizer=featurizer,\n",
    "            single_seq_len=config_dict[\"max_seq_length\"],\n",
    "            total_seq_len=config_dict[\"max_seq_length\"],\n",
    "            label_column=config_dict[\"label_column\"],\n",
    "            is_same=False,\n",
    "            inference_mode=True\n",
    "        )\n",
    "\n",
    "test_dataset = BertFinetuneSmilesDataset(\n",
    "            input_path= config_dict['test_file'],\n",
    "            featurizer=featurizer,\n",
    "            single_seq_len=config_dict[\"max_seq_length\"],\n",
    "            total_seq_len=config_dict[\"max_seq_length\"],\n",
    "            label_column=config_dict[\"label_column\"],\n",
    "            is_same=False,\n",
    "            inference_mode=True,\n",
    ")\n",
    "########################################################################\n",
    "train_dataloader = MolbertDataLoader(train_dataset, \n",
    "                                    batch_size=config_dict[\"batch_size\"],\n",
    "                                    pin_memory=False,\n",
    "                                    num_workers=4, \n",
    "                                    shuffle = True)\n",
    "\n",
    "validation_dataloader = MolbertDataLoader(validation_dataset, \n",
    "                                    batch_size=config_dict[\"batch_size\"],\n",
    "                                    pin_memory=False,\n",
    "                                    num_workers=4, \n",
    "                                    shuffle = False)\n",
    "\n",
    "test_dataloader = MolbertDataLoader(test_dataset, \n",
    "                                    batch_size=config_dict[\"batch_size\"],\n",
    "                                    pin_memory=False,\n",
    "                                    num_workers=4, \n",
    "                                    shuffle = False)\n",
    "\n",
    "config_dict[\"num_batches\"] = len(train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model weights from /projects/home/mmasood1/TG GATE/MolBERT/molbert/molbert_100epochs/molbert_100epochs/checkpoints/last.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "85718834"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MolbertModel(config_dict)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "\n",
    "def wandb_init_model(model, \n",
    "                     config, \n",
    "                     train_dataloader,\n",
    "                     val_dataloader, \n",
    "                     model_type):\n",
    "    if val_dataloader == None:\n",
    "        limit_val_batches = 0.0\n",
    "    else:\n",
    "        limit_val_batches = 1.0\n",
    "    # Init our model\n",
    "    if model_type == 'chemprop':\n",
    "        run = wandb.init(\n",
    "                        project= config.project_name,\n",
    "                        dir = '/projects/home/mmasood1/Model_weights',\n",
    "                        entity=\"arslan_masood\", \n",
    "                        reinit = True, \n",
    "                        config = None,\n",
    "                        name = config.model_name,\n",
    "                        settings=wandb.Settings(start_method=\"fork\"))\n",
    "        \n",
    "        default_root_dir = config.model_weights_dir\n",
    "        use_pretrained_model = config.pretrained_model\n",
    "        use_EarlyStopping = config.EarlyStopping\n",
    "        max_epochs = config.epochs\n",
    "        accelerator =config.accelerator\n",
    "        return_trainer = config.return_trainer\n",
    "    else:\n",
    "        run = wandb.init(\n",
    "                        project= config[\"project_name\"],\n",
    "                        dir = '/projects/home/mmasood1/Model_weights',\n",
    "                        entity=\"arslan_masood\", \n",
    "                        reinit = True, \n",
    "                        config = config,\n",
    "                        name = config[\"model_name\"],\n",
    "                        settings=wandb.Settings(start_method=\"fork\"))\n",
    "        \n",
    "        default_root_dir = config[\"model_weights_dir\"]\n",
    "        use_EarlyStopping = config[\"EarlyStopping\"]\n",
    "        max_epochs = config[\"epochs\"]\n",
    "        accelerator =config[\"accelerator\"]\n",
    "        return_trainer = config[\"return_trainer\"]\n",
    "\n",
    "    #if use_pretrained_model:\n",
    "    #    model = pretrained_model(model,config)\n",
    "    #else:\n",
    "    model = model(config)\n",
    "    wandb_logger = WandbLogger( \n",
    "                        name = config[\"model_name\"],\n",
    "                        save_dir = '/projects/home/mmasood1/Model_weights',\n",
    "                        project= config[\"project_name\"],\n",
    "                        entity=\"arslan_masood\", \n",
    "                        log_model='all',\n",
    "                        #reinit = True, \n",
    "                        #config = config,\n",
    "                        #settings=wandb.Settings(start_method=\"fork\")\n",
    "                        )\n",
    "    wandb_logger.watch(model)\n",
    "    wandb_logger.log_dir = '/projects/home/mmasood1/Model_weights'\n",
    "    \n",
    "    #wandb_logger.watch(model, log=\"all\",log_freq=1)\n",
    "    \n",
    "    if use_EarlyStopping == True:\n",
    "        callback = [EarlyStopping(\n",
    "                                monitor='train_BCE_loss',\n",
    "                                min_delta=1e-5,\n",
    "                                patience=10,\n",
    "                                verbose=False,\n",
    "                                mode='min'\n",
    "                                )]\n",
    "    else:\n",
    "        callback = []\n",
    "\n",
    "    '''\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "                            monitor=None,  # Metric to monitor for saving the best model\n",
    "                            mode='min',          # Minimize the monitored metric\n",
    "                            filepath = '/projects/home/mmasood1/Model_weights/',  # Directory to store checkpoints\n",
    "                            #filename='model-{epoch:02d}-{val_BCE_non_weighted:.2f}',  # Checkpoint filename format\n",
    "                            #filename=config['chkp_file_name'],  # Checkpoint filename format\n",
    "                            #save_top_k=1,\n",
    "                            #save_last = True\n",
    "                            )\n",
    "    callback.append(checkpoint_callback)\n",
    "    '''\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        #callbacks=callback,\n",
    "        max_epochs= int(max_epochs),\n",
    "        #accelerator= accelerator, \n",
    "        #devices= config['gpu'],\n",
    "        #limit_val_batches = limit_val_batches,\n",
    "        #precision=16,\n",
    "        #enable_progress_bar = True,\n",
    "        #profiler=\"simple\",\n",
    "        #enable_model_summary=True,\n",
    "        #auto_select_gpus= True,\n",
    "        gpus = -1,\n",
    "        #logger = wandb_logger,\n",
    "        default_root_dir=default_root_dir)\n",
    "\n",
    "    # model fitting \n",
    "    trainer.fit(model, \n",
    "                train_dataloader = train_dataloader,\n",
    "                val_dataloaders = val_dataloader,\n",
    "                )\n",
    "    if return_trainer:\n",
    "        return model, run, trainer\n",
    "    else:\n",
    "        return model, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "wandb.login(key = \"27edf9c66b032c03f72d30e923276b93aa736429\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model weights from /projects/home/mmasood1/TG GATE/MolBERT/molbert/molbert_100epochs/molbert_100epochs/checkpoints/last.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n",
      "GPU available: True, used: True\n",
      "INFO: GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                   | Type                     | Params\n",
      "--------------------------------------------------------------------\n",
      "0 | weighted_creterien     | BCEWithLogitsLoss        | 0     \n",
      "1 | non_weighted_creterian | BCEWithLogitsLoss        | 0     \n",
      "2 | bert                   | SuperPositionalBertModel | 85 M  \n",
      "3 | head                   | FinetuneHead             | 38 K  \n",
      "INFO: \n",
      "  | Name                   | Type                     | Params\n",
      "--------------------------------------------------------------------\n",
      "0 | weighted_creterien     | BCEWithLogitsLoss        | 0     \n",
      "1 | non_weighted_creterian | BCEWithLogitsLoss        | 0     \n",
      "2 | bert                   | SuperPositionalBertModel | 85 M  \n",
      "3 | head                   | FinetuneHead             | 38 K  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmasood1/.conda/envs/molbert/lib/python3.7/site-packages/ipykernel_launcher.py:310: RuntimeWarning: Mean of empty slice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  78%|███████▊  | 38/49 [00:39<00:11,  1.04s/it, loss=1.247, v_num=21]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch finished. Accessed 39 batches in order to train on 39 batches.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 49/49 [00:44<00:00,  1.10it/s, loss=1.251, v_num=21]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch finished. Accessed 10 batches in order to train on 10 batches.\n",
      "/home/mmasood1/.conda/envs/molbert/lib/python3.7/site-packages/ipykernel_launcher.py:310: RuntimeWarning: Mean of empty slice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 49/49 [00:49<00:00,  1.00s/it, loss=1.251, v_num=21]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmasood1/.conda/envs/molbert/lib/python3.7/site-packages/ipykernel_launcher.py:310: RuntimeWarning: Mean of empty slice\n"
     ]
    }
   ],
   "source": [
    "trained_model, run, trainer = wandb_init_model(model = MolbertModel, \n",
    "                                                                train_dataloader = train_dataloader,\n",
    "                                                                val_dataloader =validation_dataloader,\n",
    "                                                                config = config_dict, \n",
    "                                                                model_type = 'MLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = MolbertModel(config_dict)\n",
    "checkpoint_file = \"/projects/home/mmasood1/Model_weights/preclinical_clinical/Vanilla_MLP/lightning_logs/version_9/checkpoints/epoch=7.ckpt\"\n",
    "checkpoint = torch.load(checkpoint_file, map_location=lambda storage, loc: storage)\n",
    "# load weights from checkpoint, strict=False allows to ignore some weights\n",
    "# e.g. weights of a head that was used during pretraining but isn't present during finetuning\n",
    "# and also allows to missing keys in the checkpoint, e.g. heads that are used for finetuning\n",
    "# but weren't present during pretraining\n",
    "model.load_state_dict(checkpoint['state_dict'], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = config_dict[\"metadata_dir\"] + \"predicitons/\"\n",
    "result_dir = config_dict[\"metadata_dir\"] + \"Results/\"  \n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    os.makedirs(result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch finished. Accessed 10 batches in order to train on 10 batches.\n"
     ]
    }
   ],
   "source": [
    "model = trained_model.eval()\n",
    "#model = MolbertModel(config_dict)\n",
    "#model = model.eval()\n",
    "config = config_dict\n",
    "device = torch.device('cuda')\n",
    "model = model.cpu() \n",
    "\n",
    "y_true_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "for batch in validation_dataloader:\n",
    "    \n",
    "    (batch_inputs, batch_labels), _ = batch\n",
    "    y = batch_labels[\"finetune\"].squeeze()\n",
    "\n",
    "    input_ids = batch_inputs[\"input_ids\"].cpu()\n",
    "    token_type_ids = batch_inputs[\"token_type_ids\"].cpu()\n",
    "    attention_mask = batch_inputs[\"attention_mask\"].cpu()\n",
    "\n",
    "\n",
    "    y_hat = model(input_ids,token_type_ids, attention_mask)\n",
    "\n",
    "    y_true_list.append(y.cpu())\n",
    "    y_pred_list.append(y_hat.cpu())\n",
    "\n",
    "y = torch.cat(y_true_list, dim=0)\n",
    "y_hat = torch.cat(y_pred_list, dim=0)\n",
    "\n",
    "if config[\"num_of_tasks\"] > 1:\n",
    "    y = pd.DataFrame(y.cpu().detach().numpy())\n",
    "    y_hat = pd.DataFrame(y_hat.cpu().detach().numpy())\n",
    "    y.columns = config['selected_tasks']\n",
    "    y_hat.columns = config['selected_tasks']\n",
    "else:\n",
    "    y = pd.DataFrame({config[\"selected_tasks\"]: y.cpu().detach().numpy()})\n",
    "    y_hat = pd.DataFrame({config[\"selected_tasks\"]: y_hat.cpu().detach().numpy().reshape(-1)})\n",
    "\n",
    "y.to_csv(data_dir + 'y_true_test.csv',index=False)\n",
    "y_hat.to_csv(data_dir + 'y_pred_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################3\n",
    "# Compute compute_binary_classification_metrics: Multitask\n",
    "######################################################################################\n",
    "from sklearn.metrics import auc, roc_curve, precision_recall_curve, average_precision_score, balanced_accuracy_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score, matthews_corrcoef, f1_score\n",
    "\n",
    "def prob_to_labels(pred, threshold):\n",
    "\t    return (pred >= threshold).astype('int')\n",
    "\n",
    "def compute_binary_classification_metrics_MT(y_true, y_pred_proba, \n",
    "                                             missing):\n",
    "    \"\"\"\n",
    "    Compute various metrics for binary classification.\n",
    "    \n",
    "    Parameters:\n",
    "        y_true (array-like): Binary labels (0 or 1).\n",
    "        y_pred_proba (array-like): Predictive probabilities for the positive class.\n",
    "        threshold (float, optional): Threshold value for classification. Default is 0.5.\n",
    "    \n",
    "   Returns:\n",
    "        pandas.DataFrame: DataFrame containing the computed metrics for each task (accuracy, ROC AUC, average precision, MCC, F1-score, random precision, gain in average precision).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        num_tasks = y_true.shape[1]  # Get the number of tasks\n",
    "    except:\n",
    "        num_tasks = 1\n",
    "    metrics_list = []\n",
    "\n",
    "    for i in range(num_tasks):\n",
    "        if num_tasks > 1:\n",
    "            y_true_task = y_true[:, i]\n",
    "            y_pred_proba_task = y_pred_proba[:, i]\n",
    "        else:\n",
    "            y_true_task = y_true\n",
    "            y_pred_proba_task = y_pred_proba\n",
    "            \n",
    "        # Apply masking\n",
    "        if missing == 'nan':\n",
    "            mask = ~np.isnan(y_true_task)\n",
    "        if missing == -1:\n",
    "            mask = (y_true_task != -1)\n",
    "\n",
    "        y_true_task = y_true_task[mask]\n",
    "        y_pred_proba_task = y_pred_proba_task[mask]\n",
    "\n",
    "        metrics_task = {}\n",
    "        try:\n",
    "            # ROC AUC\n",
    "            fpr, tpr, th = roc_curve(y_true_task, y_pred_proba_task)\n",
    "            metrics_task['roc_auc'] = auc(fpr, tpr)\n",
    "\n",
    "            # Balanced accuracy\n",
    "            balanced_accuracy = (tpr + (1 - fpr)) / 2\n",
    "            metrics_task['balanced_acc'] = np.max(balanced_accuracy)\n",
    "            \n",
    "            # sensitivity, specificity\n",
    "            optimal_threshold_index = np.argmax(balanced_accuracy)\n",
    "            optimal_threshold = th[optimal_threshold_index]\n",
    "            metrics_task['sensitivity'] = tpr[optimal_threshold_index]\n",
    "            metrics_task['specificity'] = 1 - fpr[optimal_threshold_index]\n",
    "\n",
    "        except:\n",
    "            metrics_task['roc_auc'] = np.nan\n",
    "            metrics_task['sensitivity']= np.nan\n",
    "            metrics_task['specificity']= np.nan\n",
    "        try:\n",
    "            precision, recall, thresholds = precision_recall_curve(y_true_task, y_pred_proba_task)\n",
    "            metrics_task['AUPR'] = auc(recall, precision)\n",
    "            f1 = [f1_score(y_true_task, prob_to_labels(y_pred_proba_task, t)) for t in thresholds]\n",
    "            metrics_task['f1_score'] = np.max(f1)\n",
    "\n",
    "            metrics_task['average_precision'] = average_precision_score(y_true_task, y_pred_proba_task)\n",
    "        except:\n",
    "            metrics_task['AUPR'] = np.nan\n",
    "            metrics_task['f1_score'] = np.nan\n",
    "        \n",
    "\n",
    "        metrics_list.append(metrics_task)\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    col = ['balanced_acc', 'f1_score','specificity','sensitivity', 'roc_auc','AUPR', 'average_precision']\n",
    "    \n",
    "    return metrics_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "data = pd.read_csv(config_dict[\"data_dir\"] + \"train_fold0.csv\")\n",
    "target_names = data.loc[:,\"Cytoplasmic alteration (Basophilic/glycogen depletion)\":\"hepatobiliary_disorders\"].columns.tolist()\n",
    "#target_names = data.loc[:,\"DILI_binary\":\"hepatobiliary_disorders\"].columns.tolist()\n",
    "config[\"num_of_tasks\"] = len(target_names)\n",
    "config[\"selected_tasks\"] = target_names\n",
    "\n",
    "preclinical_tasks = config[\"selected_tasks\"][:20]\n",
    "clinical_tasks = config[\"selected_tasks\"][20:]\n",
    "\n",
    "pathological_tasks = ['Cytoplasmic alteration (Basophilic/glycogen depletion)',\n",
    "                        'Cytoplasmic alteration (Eosinophilic)',\n",
    "                        'Extramedullary Hematopoiesis',\n",
    "                        'Hypertrophy, hepatocellular',\n",
    "                        'Hypertrophy/Hyperplasia',\n",
    "                        'Increased mitoses',\n",
    "                        'Infiltration, Mononuclear',\n",
    "                        'Necrosis',\n",
    "                        'Pigmentation (pigment deposition)',\n",
    "                        'Single Cell Necrosis',\n",
    "                        'Vacuolation',\n",
    "                        'DILI_binary']\n",
    "\n",
    "blood_tasks = ['ALP(IU/L)',\n",
    "                'AST(IU/L)',\n",
    "                'ALT(IU/L)',\n",
    "                'GTP(IU/L)',\n",
    "                'TC(mg/dL)',\n",
    "                'TG(mg/dL)',\n",
    "                'TBIL(mg/dL)',\n",
    "                'DBIL(mg/dL)']\n",
    "config[\"batch_size\"] = 64\n",
    "config[\"seed\"] = 4\n",
    "config[\"split_method\"] = \"StratifiedGroupKFold\"\n",
    "config[\"test_set_creteria\"] = \"most_diverse_fold\"\n",
    "config[\"task_for_stratification\"] = \"DILI_binary\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################3\n",
    "# Compute compute_binary_classification_metrics: Multitask\n",
    "######################################################################################\n",
    "from sklearn.metrics import auc, roc_curve, precision_recall_curve, average_precision_score, balanced_accuracy_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score, matthews_corrcoef, f1_score\n",
    "\n",
    "def prob_to_labels(pred, threshold):\n",
    "\t    return (pred >= threshold).astype('int')\n",
    "\n",
    "def compute_binary_classification_metrics_MT(y_true, y_pred_proba, \n",
    "                                             missing):\n",
    "    \"\"\"\n",
    "    Compute various metrics for binary classification.\n",
    "    \n",
    "    Parameters:\n",
    "        y_true (array-like): Binary labels (0 or 1).\n",
    "        y_pred_proba (array-like): Predictive probabilities for the positive class.\n",
    "        threshold (float, optional): Threshold value for classification. Default is 0.5.\n",
    "    \n",
    "   Returns:\n",
    "        pandas.DataFrame: DataFrame containing the computed metrics for each task (accuracy, ROC AUC, average precision, MCC, F1-score, random precision, gain in average precision).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        num_tasks = y_true.shape[1]  # Get the number of tasks\n",
    "    except:\n",
    "        num_tasks = 1\n",
    "    metrics_list = []\n",
    "\n",
    "    for i in range(num_tasks):\n",
    "        if num_tasks > 1:\n",
    "            y_true_task = y_true[:, i]\n",
    "            y_pred_proba_task = y_pred_proba[:, i]\n",
    "        else:\n",
    "            y_true_task = y_true\n",
    "            y_pred_proba_task = y_pred_proba\n",
    "            \n",
    "        # Apply masking\n",
    "        if missing == 'nan':\n",
    "            mask = ~np.isnan(y_true_task)\n",
    "        if missing == -1:\n",
    "            mask = (y_true_task != -1)\n",
    "\n",
    "        y_true_task = y_true_task[mask]\n",
    "        y_pred_proba_task = y_pred_proba_task[mask]\n",
    "\n",
    "        metrics_task = {}\n",
    "        try:\n",
    "            # ROC AUC\n",
    "            fpr, tpr, th = roc_curve(y_true_task, y_pred_proba_task)\n",
    "            metrics_task['roc_auc'] = auc(fpr, tpr)\n",
    "\n",
    "            # Balanced accuracy\n",
    "            balanced_accuracy = (tpr + (1 - fpr)) / 2\n",
    "            metrics_task['balanced_acc'] = np.max(balanced_accuracy)\n",
    "            \n",
    "            # sensitivity, specificity\n",
    "            optimal_threshold_index = np.argmax(balanced_accuracy)\n",
    "            optimal_threshold = th[optimal_threshold_index]\n",
    "            metrics_task['sensitivity'] = tpr[optimal_threshold_index]\n",
    "            metrics_task['specificity'] = 1 - fpr[optimal_threshold_index]\n",
    "\n",
    "        except:\n",
    "            metrics_task['roc_auc'] = np.nan\n",
    "            metrics_task['sensitivity']= np.nan\n",
    "            metrics_task['specificity']= np.nan\n",
    "        try:\n",
    "            precision, recall, thresholds = precision_recall_curve(y_true_task, y_pred_proba_task)\n",
    "            metrics_task['AUPR'] = auc(recall, precision)\n",
    "            f1 = [f1_score(y_true_task, prob_to_labels(y_pred_proba_task, t)) for t in thresholds]\n",
    "            metrics_task['f1_score'] = np.max(f1)\n",
    "\n",
    "            metrics_task['average_precision'] = average_precision_score(y_true_task, y_pred_proba_task)\n",
    "        except:\n",
    "            metrics_task['AUPR'] = np.nan\n",
    "            metrics_task['f1_score'] = np.nan\n",
    "        \n",
    "\n",
    "        metrics_list.append(metrics_task)\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    col = ['balanced_acc', 'f1_score','specificity','sensitivity', 'roc_auc','AUPR', 'average_precision']\n",
    "    \n",
    "    return metrics_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compute_binary_classification_metrics_MT(y_true = y[config['selected_tasks']].values, \n",
    "                                                    y_pred_proba = y_hat[config['selected_tasks']].values,\n",
    "                                                                        missing = 'nan')\n",
    "metrics.insert(0, 'Tasks', target_names)\n",
    "mean_preformances = {\"pathology_mean\": metrics[metrics.Tasks.isin(pathological_tasks)].iloc[:,1:].mean(),\n",
    "                    \"blood_mean\": metrics[metrics.Tasks.isin(blood_tasks)].iloc[:,1:].mean(),\n",
    "                    \"preclinical_mean\": metrics[metrics.Tasks.isin(preclinical_tasks)].iloc[:,1:].mean(),\n",
    "                    \"clinical_mean\": metrics[metrics.Tasks.isin(clinical_tasks)].iloc[:,1:].mean(),\n",
    "                    \"combined_ex_BM\":metrics[metrics.Tasks.isin(clinical_tasks + pathological_tasks)].iloc[:,1:].mean(),\n",
    "                    \"combined_all\": metrics.iloc[:,1:].mean()}\n",
    "mean_preformances = pd.DataFrame(mean_preformances).T\n",
    "mean_preformances = mean_preformances.rename_axis('Tasks').reset_index()\n",
    "metrics = pd.concat([metrics, mean_preformances], ignore_index=True) \n",
    "metrics.to_csv(result_dir + f'val_metric.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tasks</th>\n",
       "      <th>balanced_acc</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>specificity</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>AUPR</th>\n",
       "      <th>average_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>blood_mean</td>\n",
       "      <td>0.762864</td>\n",
       "      <td>0.387338</td>\n",
       "      <td>0.798049</td>\n",
       "      <td>0.727679</td>\n",
       "      <td>0.729979</td>\n",
       "      <td>0.236936</td>\n",
       "      <td>0.273723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>preclinical_mean</td>\n",
       "      <td>0.721162</td>\n",
       "      <td>0.284449</td>\n",
       "      <td>0.609645</td>\n",
       "      <td>0.832679</td>\n",
       "      <td>0.599679</td>\n",
       "      <td>0.165347</td>\n",
       "      <td>0.190966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>clinical_mean</td>\n",
       "      <td>0.603982</td>\n",
       "      <td>0.317743</td>\n",
       "      <td>0.644504</td>\n",
       "      <td>0.563459</td>\n",
       "      <td>0.562471</td>\n",
       "      <td>0.212864</td>\n",
       "      <td>0.223526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>combined_ex_BM</td>\n",
       "      <td>0.629519</td>\n",
       "      <td>0.288632</td>\n",
       "      <td>0.598658</td>\n",
       "      <td>0.660379</td>\n",
       "      <td>0.548283</td>\n",
       "      <td>0.185652</td>\n",
       "      <td>0.198460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>combined_all</td>\n",
       "      <td>0.650854</td>\n",
       "      <td>0.304425</td>\n",
       "      <td>0.630561</td>\n",
       "      <td>0.671147</td>\n",
       "      <td>0.577354</td>\n",
       "      <td>0.193857</td>\n",
       "      <td>0.210502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Tasks  balanced_acc  f1_score  specificity  sensitivity  \\\n",
       "51        blood_mean      0.762864  0.387338     0.798049     0.727679   \n",
       "52  preclinical_mean      0.721162  0.284449     0.609645     0.832679   \n",
       "53     clinical_mean      0.603982  0.317743     0.644504     0.563459   \n",
       "54    combined_ex_BM      0.629519  0.288632     0.598658     0.660379   \n",
       "55      combined_all      0.650854  0.304425     0.630561     0.671147   \n",
       "\n",
       "     roc_auc      AUPR  average_precision  \n",
       "51  0.729979  0.236936           0.273723  \n",
       "52  0.599679  0.165347           0.190966  \n",
       "53  0.562471  0.212864           0.223526  \n",
       "54  0.548283  0.185652           0.198460  \n",
       "55  0.577354  0.193857           0.210502  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tasks</th>\n",
       "      <th>balanced_acc</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>specificity</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>AUPR</th>\n",
       "      <th>average_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>blood_mean</td>\n",
       "      <td>0.766772</td>\n",
       "      <td>0.396162</td>\n",
       "      <td>0.812115</td>\n",
       "      <td>0.721429</td>\n",
       "      <td>0.736685</td>\n",
       "      <td>0.242744</td>\n",
       "      <td>0.280845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>preclinical_mean</td>\n",
       "      <td>0.741805</td>\n",
       "      <td>0.293917</td>\n",
       "      <td>0.657383</td>\n",
       "      <td>0.826227</td>\n",
       "      <td>0.636400</td>\n",
       "      <td>0.158302</td>\n",
       "      <td>0.186908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>clinical_mean</td>\n",
       "      <td>0.604359</td>\n",
       "      <td>0.316623</td>\n",
       "      <td>0.675486</td>\n",
       "      <td>0.533232</td>\n",
       "      <td>0.561302</td>\n",
       "      <td>0.216686</td>\n",
       "      <td>0.227159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>combined_ex_BM</td>\n",
       "      <td>0.638874</td>\n",
       "      <td>0.290660</td>\n",
       "      <td>0.640841</td>\n",
       "      <td>0.636906</td>\n",
       "      <td>0.563656</td>\n",
       "      <td>0.183921</td>\n",
       "      <td>0.197766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>combined_all</td>\n",
       "      <td>0.659337</td>\n",
       "      <td>0.307541</td>\n",
       "      <td>0.668245</td>\n",
       "      <td>0.650430</td>\n",
       "      <td>0.591341</td>\n",
       "      <td>0.193332</td>\n",
       "      <td>0.211059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Tasks  balanced_acc  f1_score  specificity  sensitivity  \\\n",
       "51        blood_mean      0.766772  0.396162     0.812115     0.721429   \n",
       "52  preclinical_mean      0.741805  0.293917     0.657383     0.826227   \n",
       "53     clinical_mean      0.604359  0.316623     0.675486     0.533232   \n",
       "54    combined_ex_BM      0.638874  0.290660     0.640841     0.636906   \n",
       "55      combined_all      0.659337  0.307541     0.668245     0.650430   \n",
       "\n",
       "     roc_auc      AUPR  average_precision  \n",
       "51  0.736685  0.242744           0.280845  \n",
       "52  0.636400  0.158302           0.186908  \n",
       "53  0.561302  0.216686           0.227159  \n",
       "54  0.563656  0.183921           0.197766  \n",
       "55  0.591341  0.193332           0.211059  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
