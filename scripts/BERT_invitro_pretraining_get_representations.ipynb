{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmasood1/.conda/envs/molbert/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "import gc\n",
    "\n",
    "import os, yaml\n",
    "from argparse import Namespace\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_curve\n",
    "from sklearn.metrics import average_precision_score, f1_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "import wandb\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "wandb.login(key = \"27edf9c66b032c03f72d30e923276b93aa736429\")\n",
    "\n",
    "from molbert.models.smiles import SmilesMolbertModel\n",
    "from molbert.datasets.dataloading import MolbertDataLoader\n",
    "from molbert.datasets.smiles import BertSmilesDataset\n",
    "from molbert.utils.featurizer.molfeaturizer import SmilesIndexFeaturizer\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, StepLR\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_constant_schedule_with_warmup,\n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Tuple, Sequence, Any, Dict, Union, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from molbert.utils.featurizer.molfeaturizer import SmilesIndexFeaturizer\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class MolBertFeaturizer:\n",
    "    \"\"\"\n",
    "    This featurizer takes a molbert model and transforms the input data and\n",
    "    returns the representation in the last layer (pooled output and sequence_output).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        featurizer,\n",
    "        device: str = None,\n",
    "        embedding_type: str = 'pooled',\n",
    "        max_seq_len: Optional[int] = None,\n",
    "        permute: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            checkpoint_path: path or S3 location of trained model checkpoint\n",
    "            device: device for torch\n",
    "            embedding_type: method to reduce MolBERT encoding to an output set of features. Default: 'pooled'\n",
    "                Other options are embeddings summed or concat across layers, and then averaged\n",
    "                Raw sequence and pooled output is also available (set to 'dict')\n",
    "                average-sum-[2|4], average-cat-[2,4], average-[1|2|3|4], average-1-cat-pooled, pooled, dict\n",
    "            max_seq_len: used by the tokenizer, SMILES longer than this will fail to featurize\n",
    "                MolBERT was trained with SuperPositionalEncodings (TransformerXL) to decoupled from the training setup\n",
    "                By default the training config is used (128). If you have long SMILES to featurize, increase this value\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = device or 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.embedding_type = embedding_type\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.permute = permute\n",
    "\n",
    "        # load smiles index featurizer\n",
    "        self.featurizer = featurizer\n",
    "\n",
    "        # load model\n",
    "        self.model = model\n",
    "\n",
    "    def __getstate__(self):\n",
    "        self.__dict__.update({'model': self.model.to('cpu')})\n",
    "        self.__dict__.update({'device': 'cpu'})\n",
    "        return self.__dict__\n",
    "\n",
    "    @property\n",
    "\n",
    "    def transform_single(self, smiles: str) -> Tuple[np.ndarray, bool]:\n",
    "        features, valid = self.transform([smiles])\n",
    "        return features, valid[0]\n",
    "\n",
    "    def transform(self, molecules: Sequence[Any]) -> Tuple[Union[Dict, np.ndarray], np.ndarray]:\n",
    "        input_ids, valid = self.featurizer.transform(molecules)\n",
    "\n",
    "        input_ids = self.trim_batch(input_ids, valid)\n",
    "\n",
    "        token_type_ids = np.zeros_like(input_ids, dtype=np.int64)\n",
    "        attention_mask = np.zeros_like(input_ids, dtype=np.int64)\n",
    "\n",
    "        attention_mask[input_ids != 0] = 1\n",
    "\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long, device=self.device)\n",
    "        token_type_ids = torch.tensor(token_type_ids, dtype=torch.long, device=self.device)\n",
    "        attention_mask = torch.tensor(attention_mask, dtype=torch.long, device=self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.encoder(\n",
    "                input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "        sequence_output, pooled_output = outputs\n",
    "\n",
    "        # set invalid outputs to 0s\n",
    "        valid_tensor = torch.tensor(\n",
    "            valid, dtype=sequence_output.dtype, device=sequence_output.device, requires_grad=False\n",
    "        )\n",
    "\n",
    "        pooled_output = pooled_output * valid_tensor[:, None]\n",
    "        sequence_out = sequence_output * valid_tensor[:, None, None]\n",
    "\n",
    "        sequence_out = sequence_out.detach().cpu().numpy()\n",
    "        pooled_output = pooled_output.detach().cpu().numpy()\n",
    "        out = pooled_output\n",
    "\n",
    "        return out, valid\n",
    "\n",
    "    @staticmethod\n",
    "    def trim_batch(input_ids, valid):\n",
    "\n",
    "        # trim input horizontally if there is at least 1 valid data point\n",
    "        if any(valid):\n",
    "            _, cols = np.where(input_ids[valid] != 0)\n",
    "        # else trim input down to 1 column (avoids empty batch error)\n",
    "        else:\n",
    "            cols = np.array([0])\n",
    "\n",
    "        max_idx: int = int(cols.max().item() + 1)\n",
    "\n",
    "        input_ids = input_ids[:, :max_idx]\n",
    "\n",
    "        return input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_dict\n",
    "model_weights_dir = '/projects/home/mmasood1/Model_weights/invitro/invitro_1million/MolBERT/default_lr_schedular/'\n",
    "pretrained_model_path = '/projects/home/mmasood1/TG GATE/MolBERT/molbert/molbert_100epochs/molbert_100epochs/checkpoints/last.ckpt'\n",
    "data_dir = '/projects/home/mmasood1/arslan_data_repository/invitro/invitro_1m/25_04_2024/'\n",
    "pos_weights = \"/projects/home/mmasood1/arslan_data_repository/invitro/invitro_1m/25_04_2024/pos_weights.csv\"\n",
    "metadata_dir = \"/projects/home/mmasood1/trained_model_predictions/SIDER_PreClinical/BERT_finetune/MF/\"\n",
    "model_dir = os.path.dirname(os.path.dirname(pretrained_model_path))\n",
    "hparams_path = os.path.join(model_dir, 'hparams.yaml')\n",
    "# load config\n",
    "with open(hparams_path) as yaml_file:\n",
    "    config_dict = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "config_dict['project_name'] = \"BERT_invitro_pretraining\"\n",
    "config_dict['model_name'] = \"BERT_invitro_pretraining\"\n",
    "\n",
    "config_dict['model_weights_dir'] = model_weights_dir\n",
    "config_dict['pretrained_model_path'] = pretrained_model_path\n",
    "config_dict[\"metadata_dir\"] = metadata_dir\n",
    "config_dict['pos_weights'] = pos_weights\n",
    "\n",
    "config_dict['data_dir'] = data_dir\n",
    "config_dict['train_file'] = data_dir + \"train_set.pkl\"\n",
    "config_dict['valid_file'] = data_dir + \"test_set.pkl\"\n",
    "config_dict['test_file'] = data_dir + \"test_set.pkl\"\n",
    "\n",
    "config_dict['mode'] = 'classification'\n",
    "config_dict['alpha'] = 0.0\n",
    "config_dict['beta'] = 0.0\n",
    "config_dict['gamma'] = 0.0\n",
    "\n",
    "config_dict['max_epochs'] = 20\n",
    "config_dict['unfreeze_epoch'] = 210\n",
    "config_dict[\"l2_lambda\"] = 0.0\n",
    "config_dict['embedding_size'] = 50\n",
    "config_dict[\"num_physchem_properties\"] = 0\n",
    "\n",
    "config_dict['optim'] = 'AdamW'#SGD\n",
    "config_dict['loss_type'] = 'BCE'# Focal_loss\n",
    "\n",
    "config_dict['lr'] = 3e-05\n",
    "config_dict[\"BERT_lr\"] = 3e-5\n",
    "config_dict[\"batch_size\"] = 64\n",
    "config_dict[\"seed\"] = 42\n",
    "\n",
    "config_dict['missing'] = 'nan'\n",
    "config_dict['compute_metric_after_n_epochs'] = 5\n",
    "config_dict['return_trainer'] = True\n",
    "config_dict['EarlyStopping'] = False\n",
    "\n",
    "config_dict[\"accelerator\"] = \"gpu\"\n",
    "config_dict[\"device\"] = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "data = pd.read_pickle(config_dict['train_file'])\n",
    "data.drop(['SMILES'], axis = 1, inplace = True)\n",
    "target_names = data.columns.tolist()\n",
    "\n",
    "config_dict[\"output_size\"] = len(target_names)\n",
    "config_dict[\"num_invitro_tasks\"] = len(target_names)\n",
    "config_dict[\"num_of_tasks\"] = len(target_names)\n",
    "\n",
    "config_dict[\"label_column\"] = target_names\n",
    "config_dict[\"selected_tasks\"] = target_names\n",
    "config_dict['num_mols'] = data.shape[0]\n",
    "config_dict['max_seq_length'] = 512\n",
    "config_dict['bert_output_dim'] = 876\n",
    "\n",
    "config_dict['pretrained_model'] = False\n",
    "config_dict['freeze_level'] = False\n",
    "config_dict[\"gpu\"] = -1\n",
    "config_dict[\"precision\"] = 32\n",
    "config_dict[\"distributed_backend\"] = \"dp\"\n",
    "config_dict[\"pretrained_crash_model\"] = None#\"/projects/home/mmasood1/Model_weights/invitro/invitro_1million/MolBERT/epoch=1-val_f1_score=0.00.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "epoch_start = 7\n",
    "for epoch in range(epoch_start, epoch_start + 1):\n",
    "    print(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = SmilesIndexFeaturizer.bert_smiles_index_featurizer(config_dict[\"max_seq_length\"], permute = False)\n",
    "elements = featurizer.load_periodic_table()[0]\n",
    "featurizer = SmilesIndexFeaturizer.bert_smiles_index_featurizer(max_length=config_dict[\"max_seq_length\"], \n",
    "                                                                allowed_elements=tuple(elements),\n",
    "                                                                permute = False)\n",
    "config_dict[\"vocab_size\"] = featurizer.vocab_size\n",
    "train_dataset = BertSmilesDataset(\n",
    "            input_path= config_dict['train_file'],\n",
    "            featurizer= featurizer,\n",
    "            single_seq_len= config_dict[\"max_seq_length\"],\n",
    "            total_seq_len= config_dict[\"max_seq_length\"],\n",
    "            label_column= config_dict[\"label_column\"],\n",
    "            is_same= config_dict[\"is_same_smiles\"],\n",
    "            num_invitro_tasks = config_dict[\"num_invitro_tasks\"],\n",
    "            num_physchem= config_dict[\"num_physchem_properties\"],\n",
    "            permute= config_dict[\"permute\"],\n",
    "            named_descriptor_set=config_dict[\"named_descriptor_set\"],\n",
    "            inference_mode = False\n",
    "        )\n",
    "\n",
    "validation_dataset = BertSmilesDataset(\n",
    "            input_path= config_dict['valid_file'],\n",
    "            featurizer= featurizer,\n",
    "            single_seq_len= config_dict[\"max_seq_length\"],\n",
    "            total_seq_len= config_dict[\"max_seq_length\"],\n",
    "            label_column= config_dict[\"label_column\"],\n",
    "            is_same= config_dict[\"is_same_smiles\"],\n",
    "            num_invitro_tasks = config_dict[\"num_invitro_tasks\"],\n",
    "            num_physchem= config_dict[\"num_physchem_properties\"],\n",
    "            permute= config_dict[\"permute\"],\n",
    "            named_descriptor_set=config_dict[\"named_descriptor_set\"],\n",
    "            inference_mode = True\n",
    "\n",
    "        )\n",
    "\n",
    "test_dataset = BertSmilesDataset(\n",
    "            input_path= config_dict['test_file'],\n",
    "            featurizer= featurizer,\n",
    "            single_seq_len= config_dict[\"max_seq_length\"],\n",
    "            total_seq_len= config_dict[\"max_seq_length\"],\n",
    "            label_column= config_dict[\"label_column\"],\n",
    "            is_same= config_dict[\"is_same_smiles\"],\n",
    "            num_invitro_tasks = config_dict[\"num_invitro_tasks\"],\n",
    "            num_physchem= config_dict[\"num_physchem_properties\"],\n",
    "            permute= config_dict[\"permute\"],\n",
    "            named_descriptor_set=config_dict[\"named_descriptor_set\"],\n",
    "            inference_mode = True\n",
    "\n",
    ")\n",
    "\n",
    "train_dataloader = MolbertDataLoader(train_dataset, \n",
    "                                    batch_size=config_dict[\"batch_size\"],\n",
    "                                    pin_memory=False,\n",
    "                                    num_workers=16, \n",
    "                                    shuffle = True)\n",
    "\n",
    "validation_dataloader = MolbertDataLoader(validation_dataset, \n",
    "                                    batch_size=config_dict[\"batch_size\"],\n",
    "                                    pin_memory=False,\n",
    "                                    num_workers=16, \n",
    "                                    shuffle = False)\n",
    "\n",
    "test_dataloader = MolbertDataLoader(test_dataset, \n",
    "                                    batch_size=config_dict[\"batch_size\"],\n",
    "                                    pin_memory=False,\n",
    "                                    num_workers=16, \n",
    "                                    shuffle = False)\n",
    "\n",
    "config_dict[\"num_batches\"] = len(train_dataloader)\n",
    "                                                     \n",
    "\n",
    "def compute_ece(y_true, y_prob, n_bins=10, equal_intervals = True):\n",
    "    # Calculate bin boundaries\n",
    "    if equal_intervals == True: # ECE\n",
    "        bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    else:                       # ACE\n",
    "        bin_boundaries = np.percentile(y_prob, np.linspace(0, 100, n_bins + 1))\n",
    "    \n",
    "    # Calculate bin indices\n",
    "    bin_indices = np.digitize(y_prob, bin_boundaries[1:-1])\n",
    "    \n",
    "    ece = 0\n",
    "    total_samples = len(y_true)\n",
    "    \n",
    "    # Calculate ECE\n",
    "    for bin_idx in range(n_bins):\n",
    "        # Filter samples within the bin\n",
    "        bin_mask = bin_indices == bin_idx\n",
    "        bin_samples = np.sum(bin_mask)\n",
    "        \n",
    "        if bin_samples > 0:\n",
    "            # Calculate accuracy and confidence for the bin\n",
    "            bin_accuracy = np.mean(y_true[bin_mask])\n",
    "            bin_confidence = np.mean(y_prob[bin_mask])\n",
    "        \n",
    "            # Update ECE\n",
    "            ece += (bin_samples / total_samples) * np.abs(bin_accuracy - bin_confidence)\n",
    "\n",
    "    return ece\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma, pos_weight):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.w_p = pos_weight\n",
    "\n",
    "\n",
    "    def forward(self,y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Focal Loss function for binary classification.\n",
    "\n",
    "        Arguments:\n",
    "        y_true -- true binary labels (0 or 1), torch.Tensor\n",
    "        y_pred -- predicted probabilities for the positive class, torch.Tensor\n",
    "\n",
    "        Returns:\n",
    "        Focal Loss\n",
    "        \"\"\"\n",
    "        # Compute class weight\n",
    "        p = torch.sigmoid(y_pred)\n",
    "\n",
    "        # Compute focal loss for positive and negative examples\n",
    "        focal_loss_pos = - self.w_p * (1 - p) ** self.gamma * y_true * torch.log(p.clamp(min=1e-8))\n",
    "        focal_loss_pos_neg = - p ** self.gamma * (1 - y_true) * torch.log((1 - p).clamp(min=1e-8))\n",
    "\n",
    "        return focal_loss_pos + focal_loss_pos_neg\n",
    "    \n",
    "class MolbertModel(pl.LightningModule):\n",
    "    def __init__(self, args: Namespace):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.training_step_ytrue, self.training_step_ypred = [],[]\n",
    "        self.val_step_ytrue, self.val_step_ypred = [],[]\n",
    "\n",
    "        self.hparams = args\n",
    "        self.get_creterian(args)\n",
    "\n",
    "        # get model, load pretrained weights, and freeze encoder        \n",
    "        model = SmilesMolbertModel(self.hparams)\n",
    "        #if self.hparams.pretrained_model:\n",
    "        #    checkpoint = torch.load(self.hparams.pretrained_model_path, map_location=lambda storage, loc: storage)\n",
    "        #    model.load_state_dict(checkpoint['state_dict'], strict = False)\n",
    "        \n",
    "        if self.hparams.freeze_level:\n",
    "            # Freeze model\n",
    "            MolbertModel.freeze_network(model, self.hparams.freeze_level)\n",
    "\n",
    "        self.encoder = model.model.bert\n",
    "        self.Masked_LM_task = model.model.tasks[0]\n",
    "        self.invitro_task = model.model.tasks[1]\n",
    "\n",
    "        #3checkpoint = torch.load(self.hparams.pretrained_crash_model, map_location=lambda storage, loc: storage)\n",
    "        #self.load_state_dict(checkpoint['state_dict'], strict = True)\n",
    "\n",
    "    def forward(self, batch_inputs):\n",
    "        #input_ids =  batch_inputs[\"input_ids\"]\n",
    "        #token_type_ids = batch_inputs[\"token_type_ids\"]\n",
    "        #attention_mask = batch_inputs[\"attention_mask\"]\n",
    "\n",
    "        sequence_output, pooled_output = self.encoder(**batch_inputs)\n",
    "        Masked_token_pred = self.Masked_LM_task(sequence_output, pooled_output)\n",
    "        invitro_pred = self.invitro_task(sequence_output, pooled_output)\n",
    "\n",
    "        return Masked_token_pred, invitro_pred\n",
    "    \n",
    "    def get_creterian(self, config):\n",
    "        # pos weights\n",
    "        \n",
    "        pos_weights = pd.read_csv(config[\"pos_weights\"])\n",
    "        if self.hparams.num_of_tasks == 1:\n",
    "            pos_weights = pos_weights.set_index(\"Targets\").reindex([config[\"selected_tasks\"]]).weights.values\n",
    "        else:\n",
    "            pos_weights = pos_weights.set_index(\"Targets\").reindex(config[\"selected_tasks\"]).weights.values\n",
    "        pos_weights = (config[\"alpha\"] * pos_weights) + (1 - config[\"alpha\"])*1\n",
    "        self.pos_weights = torch.tensor(pos_weights, device = config[\"device\"])\n",
    "\n",
    "        # class weights\n",
    "        if self.hparams.beta > 0:\n",
    "            if self.hparams.num_of_tasks > 1:\n",
    "                class_weights = pd.read_csv(config[\"class_weights\"])\n",
    "                class_weights = class_weights.set_index(\"Targets\").reindex(config[\"selected_tasks\"]).weights.values\n",
    "                class_weights = (config[\"beta\"] * class_weights) + (1 - config[\"beta\"])*1\n",
    "                self.class_weights = torch.tensor(class_weights, device = config[\"device\"])\n",
    "            else:\n",
    "                self.class_weights = torch.tensor([1.0], device = config[\"device\"])\n",
    "\n",
    "            # train_weighted loss, validation no weights\n",
    "            self.weighted_creterien =  nn.BCEWithLogitsLoss(reduction=\"none\", \n",
    "                                                            pos_weight= self.pos_weights,\n",
    "                                                            weight= self.class_weights)\n",
    "        else:\n",
    "            self.weighted_creterien =  nn.BCEWithLogitsLoss(reduction=\"none\", \n",
    "                                                            pos_weight= self.pos_weights)\n",
    "        \n",
    "        self.FL = FocalLoss(gamma=config['gamma'], pos_weight= self.pos_weights)\n",
    "        self.non_weighted_creterian =  nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "    \n",
    "    def l2_regularization(self):\n",
    "        device = torch.device('cuda')\n",
    "        l2_reg = torch.tensor(0., requires_grad=True, device=device)\n",
    "        \n",
    "        # Apply only on weights, exclude bias\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                l2_reg = l2_reg + torch.norm(param, p=2)\n",
    "        return l2_reg\n",
    "    \n",
    "    def add_weight_decay(self, skip_list=()):\n",
    "        decay = []\n",
    "        no_decay = []\n",
    "        for name, param in self.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "            if len(param.shape) == 1 or name in skip_list:\n",
    "                no_decay.append(param)\n",
    "            else:\n",
    "                decay.append(param)\n",
    "            \n",
    "        return [\n",
    "            {'params': no_decay, 'weight_decay': 0.},\n",
    "            {'params': decay, 'weight_decay': self.hparams.l2_lambda}]\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer_grouped_parameters = self.add_weight_decay(skip_list=())\n",
    "\n",
    "        if self.hparams.optim == 'SGD':\n",
    "            optimizer = torch.optim.SGD(optimizer_grouped_parameters, \n",
    "                                             lr=self.hparams.learning_rate)\n",
    "        if self.hparams.optim == 'Adam':\n",
    "            optimizer = torch.optim.Adam(optimizer_grouped_parameters, \n",
    "                                             lr=self.hparams.learning_rate)\n",
    "        if self.hparams.optim == 'AdamW':    \n",
    "            optimizer = AdamW(optimizer_grouped_parameters, \n",
    "                                lr=self.hparams.learning_rate, \n",
    "                                eps=self.hparams.adam_epsilon)\n",
    "        \n",
    "        scheduler = self._initialise_lr_scheduler(optimizer)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def _initialise_lr_scheduler(self, optimizer):\n",
    "\n",
    "        \n",
    "        num_training_steps = self.hparams.num_batches // self.hparams.accumulate_grad_batches * self.hparams.max_epochs\n",
    "        warmup_steps = int(num_training_steps * self.hparams.warmup_proportion)\n",
    "\n",
    "        if self.hparams.learning_rate_scheduler == 'linear_with_warmup':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps\n",
    "            )\n",
    "        elif self.hparams.learning_rate_scheduler == 'cosine_with_hard_restarts_warmup':\n",
    "            scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps, num_cycles=1\n",
    "            )\n",
    "        elif self.hparams.learning_rate_scheduler == 'cosine_schedule_with_warmup':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps\n",
    "            )\n",
    "        elif self.hparams.learning_rate_scheduler == 'constant_schedule_with_warmup':\n",
    "            scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)\n",
    "\n",
    "        elif self.hparams.learning_rate_scheduler == 'cosine_annealing_warm_restarts':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, warmup_steps)\n",
    "        elif self.hparams.learning_rate_scheduler == 'reduce_on_plateau':\n",
    "            scheduler = ReduceLROnPlateau(optimizer)\n",
    "        elif self.hparams.learning_rate_scheduler == 'constant':\n",
    "            scheduler = StepLR(optimizer, 10, gamma=1.0)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'learning_rate_scheduler needs to be one of '\n",
    "                f'linear_with_warmup, cosine_with_hard_restarts_warmup, cosine_schedule_with_warmup, '\n",
    "                f'constant_schedule_with_warmup, cosine_annealing_warm_restarts, reduce_on_plateau, '\n",
    "                f'step_lr. '\n",
    "                f'Given: {self.hparams.learning_rate_scheduler}'\n",
    "            )\n",
    "\n",
    "        logger.info(\n",
    "            f'SCHEDULER: {self.hparams.learning_rate_scheduler} '\n",
    "            f'num_batches={self.hparams.num_batches} '\n",
    "            f'num_training_steps={num_training_steps} '\n",
    "            f'warmup_steps={warmup_steps}'\n",
    "        )\n",
    "\n",
    "        return {'scheduler': scheduler, 'monitor': 'valid_loss', 'interval': 'step', 'frequency': 1}\n",
    "    \n",
    "    def _compute_loss(self, y, y_hat):\n",
    "        if self.hparams.num_of_tasks == 1:\n",
    "            y = y.unsqueeze(1)\n",
    "        # compute losses, wiht masking\n",
    "        if self.hparams.missing == 'nan':\n",
    "            nan_mask = torch.isnan(y)\n",
    "            y[nan_mask] = -1\n",
    "            #y = torch.nan_to_num(y, nan = -1), for newer version\n",
    "        \n",
    "        # masks\n",
    "        valid_label_mask = (y != -1).float()\n",
    "        pos_label_mask = (y == 1)\n",
    "        negative_label_mask = (y == 0)\n",
    "\n",
    "        if self.hparams.loss_type == \"BCE\":\n",
    "            weighted_loss = self.weighted_creterien(y_hat, y) * valid_label_mask\n",
    "        if self.hparams.loss_type == \"Focal_loss\":\n",
    "            weighted_loss = self.FL(y_hat, y)* valid_label_mask\n",
    "        Non_weighted_loss = self.non_weighted_creterian(y_hat, y) * valid_label_mask\n",
    "        \n",
    "        # Non_weighted_loss, positive negative loss\n",
    "        pos_loss = Non_weighted_loss * pos_label_mask\n",
    "        neg_loss = Non_weighted_loss * negative_label_mask\n",
    "        pos_loss = pos_loss.sum() / pos_label_mask.sum()\n",
    "        neg_loss = neg_loss.sum() / negative_label_mask.sum()\n",
    "    \n",
    "        # compute mean loss\n",
    "        Non_weighted_loss = Non_weighted_loss.sum() / valid_label_mask.sum()\n",
    "        weighted_loss = weighted_loss.sum() / valid_label_mask.sum()\n",
    "\n",
    "        return weighted_loss, Non_weighted_loss, pos_loss, neg_loss\n",
    "    \n",
    "    def MaskedLM_loss(self, batch_labels, batch_predictions):\n",
    "\n",
    "        loss_fn = CrossEntropyLoss(ignore_index=-1)\n",
    "        vocab_size = self.hparams.vocab_size\n",
    "        loss = loss_fn(batch_predictions.view(-1, vocab_size), \n",
    "                batch_labels['lm_label_ids'].view(-1))  \n",
    "        return loss  \n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        step_name = \"train\"\n",
    "        # compute forward pass\n",
    "        (batch_inputs, batch_labels), _ = batch\n",
    "        y_invitro = batch_labels[\"invitro\"].squeeze()\n",
    "        Masked_token_pred, invitro_pred = self.forward(batch_inputs)\n",
    "\n",
    "        # classification loss\n",
    "        weighted_loss, Non_weighted_loss, pos_loss, neg_loss = self._compute_loss(y_invitro, invitro_pred) \n",
    "        masking_loss = self.MaskedLM_loss(batch_labels, Masked_token_pred) \n",
    "        total_loss = weighted_loss + masking_loss\n",
    "        \n",
    "        self.training_step_ytrue.append(y_invitro.long().detach().cpu())\n",
    "        self.training_step_ypred.append(torch.sigmoid(invitro_pred.detach().cpu()))\n",
    "\n",
    "        self.logger.log_metrics({f\"{step_name}_total_loss\":total_loss.item()}, step = True)\n",
    "        self.logger.log_metrics({f\"{step_name}_weighted_loss\":weighted_loss.item()}, step = True)\n",
    "        self.logger.log_metrics({f\"{step_name}_masking_loss\":masking_loss.item()}, step = True)\n",
    "        self.logger.log_metrics({f\"{step_name}_Non_weighted_loss\":Non_weighted_loss.item()}, step = True)\n",
    "        \n",
    "        #self.logger.log_metrics({f\"{step_name}_pos_loss\":pos_loss}, step = True)\n",
    "        #self.logger.log_metrics({f\"{step_name}_neg_loss\":neg_loss}, step = True)\n",
    "\n",
    "\n",
    "        return {\"loss\": total_loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        step_name = \"val\"\n",
    "        with torch.no_grad():\n",
    "            (batch_inputs, batch_labels), _ = batch\n",
    "            y_invitro = batch_labels[\"invitro\"].squeeze()\n",
    "            Masked_token_pred, invitro_pred = self.forward(batch_inputs)\n",
    "\n",
    "            # classification loss\n",
    "            weighted_loss, Non_weighted_loss, pos_loss, neg_loss = self._compute_loss(y_invitro, invitro_pred) \n",
    "            masking_loss = self.MaskedLM_loss(batch_labels, Masked_token_pred) \n",
    "            total_loss = weighted_loss + masking_loss\n",
    "\n",
    "            self.val_step_ytrue.append(y_invitro.long().detach().cpu())\n",
    "            self.val_step_ypred.append(torch.sigmoid(invitro_pred).detach().cpu())\n",
    "\n",
    "        self.logger.log_metrics({f\"{step_name}_total_loss\":total_loss.item()}, step = True)\n",
    "        self.logger.log_metrics({f\"{step_name}_weighted_loss\":weighted_loss.item()}, step = True)\n",
    "        self.logger.log_metrics({f\"{step_name}_masking_loss\":masking_loss.item()}, step = True)\n",
    "        self.logger.log_metrics({f\"{step_name}_Non_weighted_loss\":Non_weighted_loss.item()}, step = True)\n",
    "        #self.logger.log_metrics({f\"{step_name}_pos_loss\":pos_loss}, step = True)\n",
    "        #self.logger.log_metrics({f\"{step_name}_neg_loss\":neg_loss}, step = True)\n",
    "        return {f'{step_name}_loss':total_loss}\n",
    "    '''\n",
    "    def on_epoch_start(self):\n",
    "        # Check if current epoch is greater than or equal to the desired epoch to unfreeze\n",
    "        if self.current_epoch >= self.hparams.unfreeze_epoch:\n",
    "            self.unfreeze_model()\n",
    "\n",
    "            # Decrease the learning rate\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = self.hparams.BERT_lr\n",
    "            \n",
    "            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, \n",
    "                                                                        last_epoch = self.scheduler.last_epoch,\n",
    "                                                                        T_max = 10, \n",
    "                                                                        eta_min=1e-6)\n",
    "    '''        \n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        wandb.log({f'train_loss_epoch':avg_loss.item()})\n",
    "\n",
    "        # Log the learning rate at the end of each epoch\n",
    "        lr = self.trainer.optimizers[0].param_groups[0]['lr']\n",
    "        wandb.log({'learning_rate': lr})\n",
    "        \n",
    "        # Collect predictions and true labels for the complete training set\n",
    "        train_true = torch.cat(self.training_step_ytrue, dim=0)\n",
    "        train_preds = torch.cat(self.training_step_ypred, dim=0)\n",
    "\n",
    "        score_list =  self.compute_metrics(train_true, train_preds)\n",
    "        metric = ['roc_score', 'blc_acc', 'sensitivity', 'specificity', 'AUPR', 'f1_score', 'average_precision','ECE_score','ACE_score']\n",
    "            \n",
    "        for i, score in enumerate(score_list):\n",
    "            wandb.log({f'train_{metric[i]}':score.item()})\n",
    "        \n",
    "        # Clear the lists to free memory for the next epoch\n",
    "        self.training_step_ytrue.clear()\n",
    "        self.training_step_ypred.clear()\n",
    "        del train_true,train_preds\n",
    "\n",
    "        return {'train_loss':avg_loss}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        wandb.log({f'val_loss_epoch':avg_loss.item()})\n",
    "        #Collect predictions and true labels for the complete training set\n",
    "        val_true = torch.cat(self.val_step_ytrue, dim=0)\n",
    "        val_preds = torch.cat(self.val_step_ypred, dim=0)\n",
    "\n",
    "        score_list =  self.compute_metrics(val_true,val_preds)\n",
    "        metric = ['roc_score', 'blc_acc', 'sensitivity', 'specificity', 'AUPR', 'f1_score', 'average_precision','ECE_score','ACE_score']\n",
    "            \n",
    "        for i, score in enumerate(score_list):\n",
    "            wandb.log({f'val_{metric[i]}':score.item()})\n",
    "\n",
    "        # Clear the lists to free memory for the next epoch\n",
    "        self.val_step_ytrue.clear()\n",
    "        self.val_step_ypred.clear()\n",
    "        del val_true, val_preds\n",
    "\n",
    "        return {'val_loss':avg_loss}\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "\n",
    "        weight_norm = self.l2_regularization()\n",
    "        tensorboard_logs = {'weight_norm': weight_norm}\n",
    "        wandb.log(tensorboard_logs)\n",
    "\n",
    "       # Then clean the cache\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        for gpu_id in range(num_gpus):\n",
    "            with torch.cuda.device(f'cuda:{gpu_id}'):\n",
    "                torch.cuda.empty_cache()\n",
    "        # then collect the garbage\n",
    "        gc.collect()\n",
    "        print(\"!!!!!!!!! ALL CLEAR !!!!!!!!!!!!!!!!!\")\n",
    "\n",
    "    def compute_metrics(self, y_true, y_pred): \n",
    "        self.eval()\n",
    "\n",
    "        targets =  y_true.cpu().detach().tolist()\n",
    "        preds = y_pred.cpu().detach().tolist()\n",
    "\n",
    "        targets = np.array(targets).reshape(-1,self.hparams.num_of_tasks)\n",
    "        preds = np.array(preds).reshape(-1,self.hparams.num_of_tasks)\n",
    "\n",
    "        #if self.hparams.missing == 'nan':\n",
    "        #    mask = ~np.isnan(targets)\n",
    "        \n",
    "        mask = (targets != -1)\n",
    "\n",
    "        roc_score, blc_acc, sensitivity, specificity, AUPR, f1, average_precision = [],[],[],[],[],[],[]\n",
    "        ECE_score, ACE_score = [],[]\n",
    "\n",
    "        for i in range(self.hparams.num_of_tasks):\n",
    "            \n",
    "            try:\n",
    "                # get valid targets, and convert logits to prob\n",
    "                valid_targets = targets[:,i][mask[:,i]]\n",
    "                valid_preds = expit(preds[:,i][mask[:,i]])\n",
    "                ECE= compute_ece(valid_targets, valid_preds, n_bins=10, equal_intervals = True)\n",
    "                ACE = compute_ece(valid_targets, valid_preds, n_bins=10, equal_intervals = False)\n",
    "                ECE_score.append(ECE)\n",
    "                ACE_score.append(ACE)\n",
    "            except:\n",
    "                ECE_score.append(np.nan)\n",
    "                ACE_score.append(np.nan)\n",
    "\n",
    "            try:\n",
    "                # ROC_AUC\n",
    "                fpr, tpr, th = roc_curve(valid_targets, valid_preds)\n",
    "                roc_score.append(auc(fpr, tpr))\n",
    "\n",
    "                # Balanced accuracy\n",
    "                balanced_accuracy = (tpr + (1 - fpr)) / 2\n",
    "                blc_acc.append(np.max(balanced_accuracy))\n",
    "\n",
    "                # sensitivity, specificity\n",
    "                optimal_threshold_index = np.argmax(balanced_accuracy)\n",
    "                optimal_threshold = th[optimal_threshold_index]\n",
    "                sensitivity.append(tpr[optimal_threshold_index])\n",
    "                specificity.append(1 - fpr[optimal_threshold_index])\n",
    "\n",
    "                # AUPR, F1\n",
    "                precision, recall, thresholds = precision_recall_curve(valid_targets, valid_preds)\n",
    "                AUPR.append(auc(recall, precision))\n",
    "                f1_sc = f1_score(valid_targets, self.prob_to_labels(valid_preds, optimal_threshold))\n",
    "                f1.append(f1_sc)\n",
    "                average_precision.append(average_precision_score(valid_targets, valid_preds))\n",
    "                \n",
    "            except:\n",
    "                roc_score.append(np.nan)\n",
    "                AUPR.append(np.nan)\n",
    "                average_precision.append(np.nan)\n",
    "                #print('Performance metric is null')\n",
    "                \n",
    "        self.train()\n",
    "        return np.nanmean(roc_score), np.nanmean(blc_acc), np.nanmean(sensitivity), np.nanmean(specificity), np.nanmean(AUPR), np.nanmean(f1), np.nanmean(average_precision),np.nanmean(ECE_score),np.nanmean(ACE_score)\n",
    "\n",
    "    \n",
    "    def prob_to_labels(self, pred, threshold):\n",
    "\t    return (pred >= threshold).astype('int')\n",
    "\n",
    "    def unfreeze_model(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    @staticmethod\n",
    "    def freeze_network(model, freeze_level: int):\n",
    "        \"\"\"\n",
    "        Freezes specific layers of the model depending on the freeze_level argument:\n",
    "\n",
    "         0: freeze nothing\n",
    "        -1: freeze all BERT weights but not the task head\n",
    "        -2: freeze the pooling layer\n",
    "        -3: freeze the embedding layer\n",
    "        -4: freeze the task head but not the base layer\n",
    "        n>0: freeze the bottom n layers of the base model.\n",
    "        \"\"\"\n",
    "\n",
    "        model_bert = model.model.bert\n",
    "        model_tasks = model.model.tasks\n",
    "\n",
    "        model_bert_encoder = model.model.bert.encoder\n",
    "        model_bert_pooler = model.model.bert.pooler\n",
    "        model_bert_embeddings = model.model.bert.embeddings\n",
    "\n",
    "        if freeze_level == 0:\n",
    "            # freeze nothing\n",
    "            return\n",
    "\n",
    "        elif freeze_level > 0:\n",
    "            # freeze the encoder/transformer\n",
    "            n_encoder_layers = len(model_bert_encoder.layer)\n",
    "\n",
    "            # we'll always freeze layers bottom up - starting from layers closest to the embeddings\n",
    "            frozen_layers = min(freeze_level, n_encoder_layers)\n",
    "            #\n",
    "            for i in range(frozen_layers):\n",
    "                layer = model_bert_encoder.layer[i]\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        elif freeze_level == -1:\n",
    "            # freeze everything bert\n",
    "            for param in model_bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        elif freeze_level == -2:\n",
    "            # freeze the pooling layer\n",
    "            for param in model_bert_pooler.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        elif freeze_level == -3:\n",
    "            # freeze the embedding layer\n",
    "            for param in model_bert_embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        elif freeze_level == -4:\n",
    "            # freeze the task head\n",
    "            for param in model_tasks.parameters():\n",
    "                param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1105/1653 [01:19<01:25,  6.41it/s]WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '(', 'C', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', '=', 'C', 'N', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '1', ')', 'N', 'C', '(', 'C', ')', '=', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'C', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', 'C', '2', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', '2', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', 'C', '2', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', '2', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', 'C', '2', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', '2', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'C', '(', 'N', ')', '=', 'O', '[SEP]']\n",
      "WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '(', 'C', ')', 'C', '1', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '2', 'C', 'S', 'S', 'C', 'C', '3', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '4', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '4', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '4', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '4', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', '(', 'C', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', 'S', 'C', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '4', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '4', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '3', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'S', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', 'C', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'O', ')', 'C', 'S', 'S', 'C', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '3', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '3', 'C', 'C', 'C', 'N', '3', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', '3', 'C', 'C', 'C', 'C', '3', 'C', '(', '=', 'O', ')', 'N', '3', 'C', 'C', 'C', 'C', '3', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', 'C', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', '3', 'C', 'C', 'C', 'C', '3', 'C', '(', '=', 'O', ')', 'N', '2', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '2', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '2', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '2', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '2', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '2', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '2', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', '(', 'C', ')', 'C', 'C', ')', 'N', 'C', '1', '=', 'O', '[SEP]']\n",
      " 67%|██████▋   | 1107/1653 [01:19<01:05,  8.33it/s]WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '(', 'C', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'S', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', '=', 'C', 'N', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', 'C', ')', 'C', 'C', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'C', '[SEP]']\n",
      " 67%|██████▋   | 1110/1653 [01:19<01:17,  7.04it/s]WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', 'C', 'C', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '1', 'C', 'C', 'C', 'N', '1', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '1', 'C', 'S', 'S', 'C', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '2', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '2', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '2', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '2', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '2', 'C', 'S', 'S', 'C', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', ')', 'C', '(', 'C', ')', 'C', 'C', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'S', 'S', 'C', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '3', '=', 'C', 'N', 'C', '=', 'N', '3', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '3', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'N', 'C', '=', 'N', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', 'C', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', 'C', ')', 'C', 'C', ')', 'C', '(', '=', 'O', ')', 'N', '2', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'O', '[SEP]']\n",
      " 68%|██████▊   | 1130/1653 [01:21<00:46, 11.14it/s]WARNING: SMILES is too long! ['[CLS]', 'C', 'O', 'C', 'C', 'O', 'C', '1', 'C', '(', 'O', ')', 'C', '(', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '2', 'C', '(', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '3', 'C', '(', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '4', 'C', '(', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '5', 'C', '(', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '6', 'C', 'C', '(', 'N', '7', 'C', '=', 'C', '(', 'C', ')', 'C', '(', 'N', ')', '=', 'N', 'C', '7', '=', 'O', ')', 'O', 'C', '6', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '6', 'C', 'C', '(', 'N', '7', 'C', '=', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '7', '=', 'O', ')', 'O', 'C', '6', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '6', 'C', 'C', '(', 'N', '7', 'C', '=', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '7', '=', 'O', ')', 'O', 'C', '6', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '6', 'C', 'C', '(', 'N', '7', 'C', '=', 'C', '(', 'C', ')', 'C', '(', 'N', ')', '=', 'N', 'C', '7', '=', 'O', ')', 'O', 'C', '6', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '6', 'C', 'C', '(', 'N', '7', 'C', '=', 'N', 'C', '8', '=', 'C', '7', 'N', '=', 'C', '(', 'N', ')', 'N', 'C', '8', '=', 'O', ')', 'O', 'C', '6', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '6', 'C', 'C', '(', 'N', '7', 'C', '=', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '7', '=', 'O', ')', 'O', 'C', '6', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '6', 'C', 'C', '(', 'N', '7', 'C', '=', 'C', '(', 'C', ')', 'C', '(', 'N', ')', '=', 'N', 'C', '7', '=', 'O', ')', 'O', 'C', '6', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '6', 'C', 'C', '(', 'N', '7', 'C', '=', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '7', '=', 'O', ')', 'O', 'C', '6', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '6', 'C', 'C', '(', 'N', '7', 'C', '=', 'N', 'C', '8', '=', 'C', '7', 'N', '=', 'C', '(', 'N', ')', 'N', 'C', '8', '=', 'O', ')', 'O', 'C', '6', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '6', 'C', 'C', '(', 'N', '7', 'C', '=', 'N', 'C', '8', '=', 'C', '(', 'N', ')', 'N', '=', 'C', 'N', '=', 'C', '7', '8', ')', 'O', 'C', '6', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '6', 'C', '(', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '7', 'C', '(', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '8', 'C', '(', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '9', 'C', '(', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '%', '1', '0', 'C', '(', 'C', 'O', ')', 'O', 'C', '(', 'N', '%', '1', '1', 'C', '=', 'N', 'C', '%', '1', '2', '=', 'C', '%', '1', '1', 'N', '=', 'C', '(', 'N', ')', 'N', 'C', '%', '1', '2', '=', 'O', ')', 'C', '%', '1', '0', 'O', 'C', 'C', 'O', 'C', ')', 'O', 'C', '(', 'N', '%', '1', '0', 'C', '=', 'C', '(', 'C', ')', 'C', '(', 'N', ')', '=', 'N', 'C', '%', '1', '0', '=', 'O', ')', 'C', '9', 'O', 'C', 'C', 'O', 'C', ')', 'O', 'C', '(', 'N', '9', 'C', '=', 'C', '(', 'C', ')', 'C', '(', 'N', ')', '=', 'N', 'C', '9', '=', 'O', ')', 'C', '8', 'O', 'C', 'C', 'O', 'C', ')', 'O', 'C', '(', 'N', '8', 'C', '=', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '8', '=', 'O', ')', 'C', '7', 'O', 'C', 'C', 'O', 'C', ')', 'O', 'C', '(', 'N', '7', 'C', '=', 'C', '(', 'C', ')', 'C', '(', 'N', ')', '=', 'N', 'C', '7', '=', 'O', ')', 'C', '6', 'O', 'C', 'C', 'O', 'C', ')', 'O', 'C', '(', 'N', '6', 'C', '=', 'N', 'C', '7', '=', 'C', '6', 'N', '=', 'C', '(', 'N', ')', 'N', 'C', '7', '=', 'O', ')', 'C', '5', 'O', 'C', 'C', 'O', 'C', ')', 'O', 'C', '(', 'N', '5', 'C', '=', 'C', '(', 'C', ')', 'C', '(', 'N', ')', '=', 'N', 'C', '5', '=', 'O', ')', 'C', '4', 'O', 'C', 'C', 'O', 'C', ')', 'O', 'C', '(', 'N', '4', 'C', '=', 'N', 'C', '5', '=', 'C', '(', 'N', ')', 'N', '=', 'C', 'N', '=', 'C', '4', '5', ')', 'C', '3', 'O', 'C', 'C', 'O', 'C', ')', 'O', 'C', '(', 'N', '3', 'C', '=', 'C', '(', 'C', ')', 'C', '(', 'N', ')', '=', 'N', 'C', '3', '=', 'O', ')', 'C', '2', 'O', 'C', 'C', 'O', 'C', ')', 'O', 'C', '1', 'N', '1', 'C', '=', 'C', '(', 'C', ')', 'C', '(', 'N', ')', '=', 'N', 'C', '1', '=', 'O', '[SEP]']\n",
      " 98%|█████████▊| 1624/1653 [01:54<00:03,  7.35it/s]WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '(', 'C', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '1', 'C', 'S', 'S', 'C', 'C', '2', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', '(', 'C', ')', 'C', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', '(', 'C', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', 'S', 'C', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '3', '=', 'C', 'N', '=', 'C', 'N', '3', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '3', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'N', '=', 'C', 'N', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', '3', 'C', 'C', 'C', 'C', '3', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', 'S', 'S', 'C', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '3', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '3', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '2', '=', 'O', ')', 'N', 'C', '1', '=', 'O', ')', 'C', '(', 'C', ')', 'C', '[SEP]']\n",
      "WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '(', 'C', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', 'O', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'S', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', 'C', '=', 'N', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', 'C', '=', 'N', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'S', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', 'C', '2', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', '2', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', 'C', '=', 'N', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', 'C', ')', 'C', '[SEP]']\n",
      "WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '(', 'C', ')', 'C', '1', 'N', 'C', '(', '=', 'O', ')', 'C', '2', 'C', 'S', 'S', 'C', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '3', 'C', 'S', 'S', 'C', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '4', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '4', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', 'C', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'S', 'S', 'C', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', '3', 'C', 'C', 'C', 'C', '3', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', '3', 'C', 'C', 'C', 'C', '3', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'N', '=', 'C', 'N', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', '3', 'C', 'C', 'C', 'C', '3', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', 'C', ')', 'C', 'C', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '1', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', '2', '[SEP]']\n",
      " 99%|█████████▊| 1629/1653 [01:55<00:02,  8.11it/s]WARNING: SMILES is too long! ['[CLS]', 'C', 'S', 'C', 'C', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', '=', 'C', 'N', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', 'C', '2', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', '2', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '1', ')', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', 'C', ')', 'C', '[SEP]']\n",
      " 99%|█████████▊| 1631/1653 [01:55<00:02,  8.71it/s]WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '(', 'C', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', '=', 'C', 'N', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'S', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', '=', 'C', 'N', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '1', 'C', 'C', 'C', 'N', '1', 'C', '(', '=', 'O', ')', 'C', '1', 'C', 'C', 'C', 'N', '1', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', 'O', ')', 'C', '(', 'C', ')', 'C', 'C', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', 'N', ')', '=', 'O', '[SEP]']\n",
      " 99%|█████████▉| 1633/1653 [01:55<00:02,  7.42it/s]WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '(', 'C', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '1', 'C', 'C', 'C', 'N', '1', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', '=', 'C', 'N', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '1', ')', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', 'C', ')', 'O', '[SEP]']\n",
      " 99%|█████████▉| 1635/1653 [01:56<00:02,  6.61it/s]WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '(', 'C', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'S', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', 'C', '1', '=', 'C', 'N', '=', 'C', 'N', '1', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', 'C', '2', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', '2', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', 'N', ')', '=', 'O', '[SEP]']\n",
      "WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '(', 'C', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'S', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', 'C', '1', '=', 'C', 'N', 'C', '=', 'N', '1', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', 'C', '2', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', '2', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', 'N', ')', '=', 'O', '[SEP]']\n",
      "100%|█████████▉| 1647/1653 [01:57<00:00,  9.04it/s]WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '(', 'C', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '1', 'C', 'S', 'S', 'C', 'C', '2', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', '(', 'C', ')', 'C', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', '(', 'C', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', 'S', 'C', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '3', '=', 'C', 'N', '=', 'C', 'N', '3', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '3', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'N', '=', 'C', 'N', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', '3', 'C', 'C', 'C', 'C', '3', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', 'S', 'S', 'C', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '3', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '3', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '2', '=', 'O', ')', 'N', 'C', '1', '=', 'O', ')', 'C', '(', 'C', ')', 'C', '[SEP]']\n",
      "WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '=', 'C', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'S', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', 'C', ')', 'C', 'C', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'C', 'C', '[SEP]']\n",
      "100%|█████████▉| 1649/1653 [01:57<00:00, 10.76it/s]WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '(', 'C', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', 'C', '2', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', '2', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '1', 'C', 'C', 'C', 'N', '1', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', '=', 'C', 'N', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '1', 'C', 'C', 'C', 'N', '1', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', '=', 'C', 'N', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'S', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', 'C', ')', 'C', 'C', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'S', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'S', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'S', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'S', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', 'C', ')', 'O', '[SEP]']\n",
      "100%|██████████| 1653/1653 [01:57<00:00, 14.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6\n",
      "147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1105/1653 [01:18<01:21,  6.68it/s]WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '(', 'C', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', '=', 'C', 'N', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '1', ')', 'N', 'C', '(', 'C', ')', '=', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'C', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', 'C', '2', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', '2', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', 'C', '2', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', '2', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', 'C', '2', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', '2', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'C', '(', 'N', ')', '=', 'O', '[SEP]']\n",
      "WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '(', 'C', ')', 'C', '1', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '2', 'C', 'S', 'S', 'C', 'C', '3', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '4', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '4', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '4', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '4', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', '(', 'C', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', 'S', 'C', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '4', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '4', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '3', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'S', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', 'C', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'O', ')', 'C', 'S', 'S', 'C', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '3', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '3', 'C', 'C', 'C', 'N', '3', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', '3', 'C', 'C', 'C', 'C', '3', 'C', '(', '=', 'O', ')', 'N', '3', 'C', 'C', 'C', 'C', '3', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', 'C', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', '3', 'C', 'C', 'C', 'C', '3', 'C', '(', '=', 'O', ')', 'N', '2', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '2', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '2', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '2', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '2', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '2', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '2', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', '(', 'C', ')', 'C', 'C', ')', 'N', 'C', '1', '=', 'O', '[SEP]']\n",
      " 67%|██████▋   | 1107/1653 [01:18<01:04,  8.48it/s]WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '(', 'C', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'S', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', '=', 'C', 'N', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', 'C', ')', 'C', 'C', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'C', '[SEP]']\n",
      " 67%|██████▋   | 1110/1653 [01:18<01:15,  7.19it/s]WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', 'C', 'C', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '1', 'C', 'C', 'C', 'N', '1', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '1', 'C', 'S', 'S', 'C', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '2', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '2', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '2', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '2', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '2', 'C', 'S', 'S', 'C', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', ')', 'C', '(', 'C', ')', 'C', 'C', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'S', 'S', 'C', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '3', '=', 'C', 'N', 'C', '=', 'N', '3', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '3', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'N', 'C', '=', 'N', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', 'C', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', 'C', ')', 'C', 'C', ')', 'C', '(', '=', 'O', ')', 'N', '2', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'O', '[SEP]']\n",
      " 68%|██████▊   | 1130/1653 [01:20<00:47, 10.91it/s]WARNING: SMILES is too long! ['[CLS]', 'C', 'O', 'C', 'C', 'O', 'C', '1', 'C', '(', 'O', ')', 'C', '(', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '2', 'C', '(', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '3', 'C', '(', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '4', 'C', '(', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '5', 'C', '(', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '6', 'C', 'C', '(', 'N', '7', 'C', '=', 'C', '(', 'C', ')', 'C', '(', 'N', ')', '=', 'N', 'C', '7', '=', 'O', ')', 'O', 'C', '6', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '6', 'C', 'C', '(', 'N', '7', 'C', '=', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '7', '=', 'O', ')', 'O', 'C', '6', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '6', 'C', 'C', '(', 'N', '7', 'C', '=', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '7', '=', 'O', ')', 'O', 'C', '6', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '6', 'C', 'C', '(', 'N', '7', 'C', '=', 'C', '(', 'C', ')', 'C', '(', 'N', ')', '=', 'N', 'C', '7', '=', 'O', ')', 'O', 'C', '6', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '6', 'C', 'C', '(', 'N', '7', 'C', '=', 'N', 'C', '8', '=', 'C', '7', 'N', '=', 'C', '(', 'N', ')', 'N', 'C', '8', '=', 'O', ')', 'O', 'C', '6', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '6', 'C', 'C', '(', 'N', '7', 'C', '=', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '7', '=', 'O', ')', 'O', 'C', '6', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '6', 'C', 'C', '(', 'N', '7', 'C', '=', 'C', '(', 'C', ')', 'C', '(', 'N', ')', '=', 'N', 'C', '7', '=', 'O', ')', 'O', 'C', '6', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '6', 'C', 'C', '(', 'N', '7', 'C', '=', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '7', '=', 'O', ')', 'O', 'C', '6', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '6', 'C', 'C', '(', 'N', '7', 'C', '=', 'N', 'C', '8', '=', 'C', '7', 'N', '=', 'C', '(', 'N', ')', 'N', 'C', '8', '=', 'O', ')', 'O', 'C', '6', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '6', 'C', 'C', '(', 'N', '7', 'C', '=', 'N', 'C', '8', '=', 'C', '(', 'N', ')', 'N', '=', 'C', 'N', '=', 'C', '7', '8', ')', 'O', 'C', '6', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '6', 'C', '(', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '7', 'C', '(', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '8', 'C', '(', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '9', 'C', '(', 'C', 'O', 'P', '(', '=', 'O', ')', '(', '[', 'O', '-', ']', ')', 'S', 'C', '%', '1', '0', 'C', '(', 'C', 'O', ')', 'O', 'C', '(', 'N', '%', '1', '1', 'C', '=', 'N', 'C', '%', '1', '2', '=', 'C', '%', '1', '1', 'N', '=', 'C', '(', 'N', ')', 'N', 'C', '%', '1', '2', '=', 'O', ')', 'C', '%', '1', '0', 'O', 'C', 'C', 'O', 'C', ')', 'O', 'C', '(', 'N', '%', '1', '0', 'C', '=', 'C', '(', 'C', ')', 'C', '(', 'N', ')', '=', 'N', 'C', '%', '1', '0', '=', 'O', ')', 'C', '9', 'O', 'C', 'C', 'O', 'C', ')', 'O', 'C', '(', 'N', '9', 'C', '=', 'C', '(', 'C', ')', 'C', '(', 'N', ')', '=', 'N', 'C', '9', '=', 'O', ')', 'C', '8', 'O', 'C', 'C', 'O', 'C', ')', 'O', 'C', '(', 'N', '8', 'C', '=', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '8', '=', 'O', ')', 'C', '7', 'O', 'C', 'C', 'O', 'C', ')', 'O', 'C', '(', 'N', '7', 'C', '=', 'C', '(', 'C', ')', 'C', '(', 'N', ')', '=', 'N', 'C', '7', '=', 'O', ')', 'C', '6', 'O', 'C', 'C', 'O', 'C', ')', 'O', 'C', '(', 'N', '6', 'C', '=', 'N', 'C', '7', '=', 'C', '6', 'N', '=', 'C', '(', 'N', ')', 'N', 'C', '7', '=', 'O', ')', 'C', '5', 'O', 'C', 'C', 'O', 'C', ')', 'O', 'C', '(', 'N', '5', 'C', '=', 'C', '(', 'C', ')', 'C', '(', 'N', ')', '=', 'N', 'C', '5', '=', 'O', ')', 'C', '4', 'O', 'C', 'C', 'O', 'C', ')', 'O', 'C', '(', 'N', '4', 'C', '=', 'N', 'C', '5', '=', 'C', '(', 'N', ')', 'N', '=', 'C', 'N', '=', 'C', '4', '5', ')', 'C', '3', 'O', 'C', 'C', 'O', 'C', ')', 'O', 'C', '(', 'N', '3', 'C', '=', 'C', '(', 'C', ')', 'C', '(', 'N', ')', '=', 'N', 'C', '3', '=', 'O', ')', 'C', '2', 'O', 'C', 'C', 'O', 'C', ')', 'O', 'C', '1', 'N', '1', 'C', '=', 'C', '(', 'C', ')', 'C', '(', 'N', ')', '=', 'N', 'C', '1', '=', 'O', '[SEP]']\n",
      " 98%|█████████▊| 1624/1653 [01:53<00:03,  7.65it/s]WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '(', 'C', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '1', 'C', 'S', 'S', 'C', 'C', '2', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', '(', 'C', ')', 'C', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', '(', 'C', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', 'S', 'C', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '3', '=', 'C', 'N', '=', 'C', 'N', '3', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '3', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'N', '=', 'C', 'N', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', '3', 'C', 'C', 'C', 'C', '3', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', 'S', 'S', 'C', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '3', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '3', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '2', '=', 'O', ')', 'N', 'C', '1', '=', 'O', ')', 'C', '(', 'C', ')', 'C', '[SEP]']\n",
      "WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '(', 'C', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', 'O', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'S', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', 'C', '=', 'N', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', 'C', '=', 'N', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'S', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', 'C', '2', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', '2', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', 'C', '=', 'N', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', 'C', ')', 'C', '[SEP]']\n",
      "WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '(', 'C', ')', 'C', '1', 'N', 'C', '(', '=', 'O', ')', 'C', '2', 'C', 'S', 'S', 'C', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '3', 'C', 'S', 'S', 'C', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '4', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '4', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', 'C', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'S', 'S', 'C', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', '3', 'C', 'C', 'C', 'C', '3', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', '3', 'C', 'C', 'C', 'C', '3', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'N', '=', 'C', 'N', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', '3', 'C', 'C', 'C', 'C', '3', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', 'C', ')', 'C', 'C', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '1', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', '2', '[SEP]']\n",
      " 99%|█████████▊| 1629/1653 [01:53<00:02,  8.46it/s]WARNING: SMILES is too long! ['[CLS]', 'C', 'S', 'C', 'C', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', '=', 'C', 'N', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', 'C', '2', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', '2', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '1', ')', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', 'C', ')', 'C', '[SEP]']\n",
      " 99%|█████████▊| 1631/1653 [01:53<00:02,  9.08it/s]WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '(', 'C', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', '=', 'C', 'N', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'S', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', '=', 'C', 'N', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '1', 'C', 'C', 'C', 'N', '1', 'C', '(', '=', 'O', ')', 'C', '1', 'C', 'C', 'C', 'N', '1', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', 'O', ')', 'C', '(', 'C', ')', 'C', 'C', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', 'N', ')', '=', 'O', '[SEP]']\n",
      " 99%|█████████▉| 1633/1653 [01:54<00:02,  7.82it/s]WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '(', 'C', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '1', 'C', 'C', 'C', 'N', '1', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', '=', 'C', 'N', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '1', ')', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', 'C', ')', 'O', '[SEP]']\n",
      " 99%|█████████▉| 1635/1653 [01:54<00:02,  6.94it/s]WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '(', 'C', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'S', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', 'C', '1', '=', 'C', 'N', '=', 'C', 'N', '1', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', 'C', '2', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', '2', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', 'N', ')', '=', 'O', '[SEP]']\n",
      "WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '(', 'C', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'S', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', 'C', '1', '=', 'C', 'N', 'C', '=', 'N', '1', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', 'C', '2', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', '2', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', '1', 'C', 'C', 'C', 'C', '1', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', 'N', ')', '=', 'O', '[SEP]']\n",
      "100%|█████████▉| 1646/1653 [01:55<00:00, 10.08it/s]WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '(', 'C', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '1', 'C', 'S', 'S', 'C', 'C', '2', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', '(', 'C', ')', 'C', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', '(', 'C', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', 'S', 'C', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '3', '=', 'C', 'N', '=', 'C', 'N', '3', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '3', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'N', '=', 'C', 'N', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', '3', 'C', 'C', 'C', 'C', '3', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'O', ')', 'C', 'S', 'S', 'C', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '3', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '3', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '3', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '2', '=', 'O', ')', 'N', 'C', '1', '=', 'O', ')', 'C', '(', 'C', ')', 'C', '[SEP]']\n",
      "100%|█████████▉| 1648/1653 [01:55<00:00, 10.48it/s]WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '=', 'C', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'S', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', 'C', ')', 'C', 'C', ')', 'C', '(', 'C', ')', 'C', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', 'C', ')', 'C', 'C', '[SEP]']\n",
      "WARNING: SMILES is too long! ['[CLS]', 'C', 'C', 'C', '(', 'C', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', 'C', '2', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', '2', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '1', 'C', 'C', 'C', 'N', '1', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', '=', 'C', 'N', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '1', 'C', 'C', 'C', 'N', '1', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'S', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'O', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', '1', '=', 'C', 'N', '=', 'C', 'N', '1', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', 'C', 'S', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'C', ')', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', ')', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', 'C', ')', 'C', 'C', ')', 'C', '(', 'C', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', '(', 'O', ')', 'C', '=', 'C', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'S', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'N', ')', '=', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', 'C', ')', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'S', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'C', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'S', 'C', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'S', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', 'C', 'N', 'C', '(', '=', 'N', ')', 'N', ')', 'C', '(', '=', 'O', ')', 'N', 'C', '(', 'C', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', 'C', ')', 'O', '[SEP]']\n",
      "100%|██████████| 1653/1653 [01:55<00:00, 14.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n"
     ]
    }
   ],
   "source": [
    "# featurizer\n",
    "featurizer = SmilesIndexFeaturizer.bert_smiles_index_featurizer(config_dict[\"max_seq_length\"], permute = False)\n",
    "elements = featurizer.load_periodic_table()[0]\n",
    "featurizer = SmilesIndexFeaturizer.bert_smiles_index_featurizer(max_length=config_dict[\"max_seq_length\"], \n",
    "                                                                allowed_elements=tuple(elements),\n",
    "                                                                permute = False)\n",
    "\n",
    "device = \"cpu\"\n",
    "BERT_dir = \"/projects/home/mmasood1/arslan_data_repository/Mix_clinical_pre_clinical/02_05_2024/Representations_BERT_invitro_pretrained/default_lr_schedular/\"\n",
    "invitro_pretrained_weight_dir = \"/projects/home/mmasood1/Model_weights/invitro/invitro_1million/MolBERT/default_lr_schedular/\"\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(\"epoch\", epoch)\n",
    "\n",
    "    # if representations does not exists & model weights are availble \n",
    "    representaitons_path = BERT_dir + f\"invitro_pretrained_{epoch}.csv\"\n",
    "    model_weights = invitro_pretrained_weight_dir + f\"epoch={epoch}-val_f1_score=0.00.ckpt\"\n",
    "    if (os.path.exists(representaitons_path) == False) & (os.path.exists(model_weights) == True):\n",
    "        seed_everything(config_dict[\"seed\"])\n",
    "        model = MolbertModel(config_dict)\n",
    "\n",
    "        # load weights\n",
    "        checkpoint = torch.load(invitro_pretrained_weight_dir + f\"epoch={epoch}-val_f1_score=0.00.ckpt\", map_location=lambda storage, loc: storage)\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict = True)\n",
    "\n",
    "        model.eval()\n",
    "        model.freeze()\n",
    "        model = model.to(device)\n",
    "\n",
    "        # get preclinical_clinical data\n",
    "        clinical_pre_clinical_data = pd.read_csv(\"/projects/home/mmasood1/arslan_data_repository/Mix_clinical_pre_clinical/02_05_2024/clinical_pre_clinical_with_blood_marker.csv\")\n",
    "        SMILES = clinical_pre_clinical_data.SMILES.tolist()\n",
    "\n",
    "\n",
    "        # get representaions\n",
    "        f = MolBertFeaturizer(model = model,\n",
    "                        featurizer= featurizer,\n",
    "                        device = \"cpu\")\n",
    "\n",
    "        features_all, masks_all = [],[]\n",
    "        for s in tqdm(SMILES):\n",
    "            features, masks = f.transform([s])\n",
    "            features_all.append(features.squeeze())\n",
    "            masks_all.append(masks)\n",
    "\n",
    "        # Filter invalids\n",
    "        filtered = [mask[0] for mask in masks_all]\n",
    "        filtered_data = clinical_pre_clinical_data[filtered].reset_index(drop = True)\n",
    "        SMILES = filtered_data[\"SMILES\"].tolist()\n",
    "\n",
    "        features = pd.DataFrame(features_all)\n",
    "        represntations = features[filtered]\n",
    "\n",
    "        represntations = pd.DataFrame(represntations)\n",
    "        represntations.insert(0, \"SMILES\", SMILES)\n",
    "\n",
    "        # compared with previous MOLBERT and drop the extra molecules\n",
    "        old_data = pd.read_csv(\"/projects/home/mmasood1/trained_model_predictions/SIDER_PreClinical/Reproduceability/clinical_pre_clinical_with_blood_marker_filtered.csv\")\n",
    "        filtered_data = filtered_data[filtered_data.SMILES.isin(old_data.SMILES)].reset_index(drop = True)\n",
    "        represntations = represntations[represntations.SMILES.isin(old_data.SMILES)].reset_index(drop = True)\n",
    "\n",
    "        represntations.to_csv(representaitons_path, index= False)\n",
    "        filtered_data.to_csv(BERT_dir + \"clinical_pre_clinical_with_blood_marker_filtered.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmasood1/.conda/envs/molbert/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "import gc\n",
    "\n",
    "import os, yaml\n",
    "from argparse import Namespace\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_curve\n",
    "from sklearn.metrics import average_precision_score, f1_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "import wandb\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "wandb.login(key = \"27edf9c66b032c03f72d30e923276b93aa736429\")\n",
    "\n",
    "from molbert.models.smiles import SmilesMolbertModel\n",
    "from molbert.datasets.dataloading import MolbertDataLoader\n",
    "from molbert.datasets.smiles import BertSmilesDataset\n",
    "from molbert.utils.featurizer.molfeaturizer import SmilesIndexFeaturizer\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, StepLR\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_constant_schedule_with_warmup,\n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import logging\n",
    "from typing import Tuple, Sequence, Any, Dict, Union, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from molbert.utils.featurizer.molfeaturizer import SmilesIndexFeaturizer\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class MolBertFeaturizer:\n",
    "    \"\"\"\n",
    "    This featurizer takes a molbert model and transforms the input data and\n",
    "    returns the representation in the last layer (pooled output and sequence_output).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        featurizer,\n",
    "        device: str = None,\n",
    "        embedding_type: str = 'pooled',\n",
    "        max_seq_len: Optional[int] = None,\n",
    "        permute: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            checkpoint_path: path or S3 location of trained model checkpoint\n",
    "            device: device for torch\n",
    "            embedding_type: method to reduce MolBERT encoding to an output set of features. Default: 'pooled'\n",
    "                Other options are embeddings summed or concat across layers, and then averaged\n",
    "                Raw sequence and pooled output is also available (set to 'dict')\n",
    "                average-sum-[2|4], average-cat-[2,4], average-[1|2|3|4], average-1-cat-pooled, pooled, dict\n",
    "            max_seq_len: used by the tokenizer, SMILES longer than this will fail to featurize\n",
    "                MolBERT was trained with SuperPositionalEncodings (TransformerXL) to decoupled from the training setup\n",
    "                By default the training config is used (128). If you have long SMILES to featurize, increase this value\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = device or 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.embedding_type = embedding_type\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.permute = permute\n",
    "\n",
    "        # load smiles index featurizer\n",
    "        self.featurizer = featurizer\n",
    "\n",
    "        # load model\n",
    "        self.model = model\n",
    "\n",
    "    def __getstate__(self):\n",
    "        self.__dict__.update({'model': self.model.to('cpu')})\n",
    "        self.__dict__.update({'device': 'cpu'})\n",
    "        return self.__dict__\n",
    "\n",
    "    @property\n",
    "\n",
    "    def transform_single(self, smiles: str) -> Tuple[np.ndarray, bool]:\n",
    "        features, valid = self.transform([smiles])\n",
    "        return features, valid[0]\n",
    "\n",
    "    def transform(self, molecules: Sequence[Any]) -> Tuple[Union[Dict, np.ndarray], np.ndarray]:\n",
    "        input_ids, valid = self.featurizer.transform(molecules)\n",
    "\n",
    "        input_ids = self.trim_batch(input_ids, valid)\n",
    "\n",
    "        token_type_ids = np.zeros_like(input_ids, dtype=np.int64)\n",
    "        attention_mask = np.zeros_like(input_ids, dtype=np.int64)\n",
    "\n",
    "        attention_mask[input_ids != 0] = 1\n",
    "\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long, device=self.device)\n",
    "        token_type_ids = torch.tensor(token_type_ids, dtype=torch.long, device=self.device)\n",
    "        attention_mask = torch.tensor(attention_mask, dtype=torch.long, device=self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.encoder(\n",
    "                input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "        sequence_output, pooled_output = outputs\n",
    "\n",
    "        # set invalid outputs to 0s\n",
    "        valid_tensor = torch.tensor(\n",
    "            valid, dtype=sequence_output.dtype, device=sequence_output.device, requires_grad=False\n",
    "        )\n",
    "\n",
    "        pooled_output = pooled_output * valid_tensor[:, None]\n",
    "        sequence_out = sequence_output * valid_tensor[:, None, None]\n",
    "\n",
    "        sequence_out = sequence_out.detach().cpu().numpy()\n",
    "        pooled_output = pooled_output.detach().cpu().numpy()\n",
    "        out = pooled_output\n",
    "\n",
    "        return out, valid\n",
    "\n",
    "    @staticmethod\n",
    "    def trim_batch(input_ids, valid):\n",
    "\n",
    "        # trim input horizontally if there is at least 1 valid data point\n",
    "        if any(valid):\n",
    "            _, cols = np.where(input_ids[valid] != 0)\n",
    "        # else trim input down to 1 column (avoids empty batch error)\n",
    "        else:\n",
    "            cols = np.array([0])\n",
    "\n",
    "        max_idx: int = int(cols.max().item() + 1)\n",
    "\n",
    "        input_ids = input_ids[:, :max_idx]\n",
    "\n",
    "        return input_ids\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# config_dict\n",
    "#model_weights_dir = '/projects/home/mmasood1/Model_weights/invitro/invitro_1million/MolBERT/default_lr_schedular/'\n",
    "model_weights_dir = '/projects/home/mmasood1/Model_weights/invitro/invitro_1million/MolBERT/Retrain_on_top_of_BERT/complete_1m_300k/'\n",
    "representation_dir = '/projects/home/mmasood1/arslan_data_repository/Mix_clinical_pre_clinical/02_05_2024/Representations_BERT_invitro_pretrained/Retrain_on_top_of_BERT/'\n",
    "\n",
    "pretrained_model_path = '/projects/home/mmasood1/TG GATE/MolBERT/molbert/molbert_100epochs/molbert_100epochs/checkpoints/last.ckpt'\n",
    "data_dir = '/projects/home/mmasood1/arslan_data_repository/invitro/invitro_1m/25_04_2024/'\n",
    "pos_weights = \"/projects/home/mmasood1/arslan_data_repository/invitro/invitro_1m/25_04_2024/pos_weights.csv\"\n",
    "metadata_dir = \"/projects/home/mmasood1/trained_model_predictions/SIDER_PreClinical/BERT_finetune/MF/\"\n",
    "model_dir = os.path.dirname(os.path.dirname(pretrained_model_path))\n",
    "hparams_path = os.path.join(model_dir, 'hparams.yaml')\n",
    "# load config\n",
    "with open(hparams_path) as yaml_file:\n",
    "    config_dict = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "config_dict['project_name'] = \"BERT_invitro_pretraining\"\n",
    "config_dict['model_name'] = \"BERT_invitro_pretraining\"\n",
    "\n",
    "config_dict['model_weights_dir'] = model_weights_dir\n",
    "config_dict['pretrained_model_path'] = pretrained_model_path\n",
    "config_dict['representation_dir'] = representation_dir\n",
    "\n",
    "config_dict[\"metadata_dir\"] = metadata_dir\n",
    "config_dict['pos_weights'] = pos_weights\n",
    "config_dict['data_dir'] = data_dir\n",
    "config_dict['train_file'] = data_dir + \"train_set_invitro_1m_plus_300k_filtered.pkl\"\n",
    "config_dict['valid_file'] = data_dir + \"test_set_invitro_1m_plus_300k_filtered.pkl\"\n",
    "config_dict['test_file'] = data_dir + \"test_set_invitro_1m_plus_300k_filtered.pkl\"\n",
    "\n",
    "config_dict['mode'] = 'classification'\n",
    "config_dict['alpha'] = 0.0\n",
    "config_dict['beta'] = 0.0\n",
    "config_dict['gamma'] = 0.0\n",
    "\n",
    "config_dict['max_epochs'] = 20\n",
    "config_dict['unfreeze_epoch'] = 210\n",
    "config_dict[\"l2_lambda\"] = 0.0\n",
    "config_dict['embedding_size'] = 50\n",
    "config_dict[\"num_physchem_properties\"] = 0\n",
    "\n",
    "config_dict['optim'] = 'AdamW'#SGD\n",
    "config_dict['loss_type'] = 'BCE'# Focal_loss\n",
    "\n",
    "config_dict['lr'] = 1e-05\n",
    "config_dict[\"BERT_lr\"] = 3e-5\n",
    "config_dict[\"batch_size\"] = 512\n",
    "config_dict[\"seed\"] = 42\n",
    "\n",
    "\n",
    "\n",
    "config_dict['missing'] = 'nan'\n",
    "config_dict['compute_metric_after_n_epochs'] = 5\n",
    "config_dict['return_trainer'] = True\n",
    "config_dict['EarlyStopping'] = False\n",
    "\n",
    "config_dict[\"accelerator\"] = \"gpu\"\n",
    "config_dict[\"device\"] = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "data = pd.read_pickle(config_dict['train_file'])\n",
    "data.drop(['SMILES'], axis = 1, inplace = True)\n",
    "target_names = data.columns.tolist()\n",
    "\n",
    "config_dict[\"output_size\"] = len(target_names)\n",
    "config_dict[\"num_invitro_tasks\"] = len(target_names)\n",
    "config_dict[\"num_of_tasks\"] = len(target_names)\n",
    "\n",
    "config_dict[\"label_column\"] = target_names\n",
    "config_dict[\"selected_tasks\"] = target_names\n",
    "config_dict['num_mols'] = data.shape[0]\n",
    "config_dict['max_seq_length'] = 128\n",
    "config_dict['bert_output_dim'] = 768\n",
    "config_dict['invitro_head_hidden_layer'] = 2048\n",
    "\n",
    "config_dict['pretrained_model'] = False\n",
    "config_dict['freeze_level'] = False\n",
    "config_dict[\"gpu\"] = -1\n",
    "config_dict[\"precision\"] = 32\n",
    "config_dict[\"distributed_backend\"] = \"dp\"\n",
    "config_dict[\"pretrained_crash_model\"] = None#\"/projects/home/mmasood1/Model_weights/invitro/invitro_1million/MolBERT/default_lr_schedular/epoch=6-val_f1_score=0.00.ckpt\"\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "featurizer = SmilesIndexFeaturizer.bert_smiles_index_featurizer(config_dict[\"max_seq_length\"], permute = False)\n",
    "config_dict[\"vocab_size\"] = featurizer.vocab_size\n",
    "train_dataset = BertSmilesDataset(\n",
    "            input_path= config_dict['train_file'],\n",
    "            featurizer= featurizer,\n",
    "            single_seq_len= config_dict[\"max_seq_length\"],\n",
    "            total_seq_len= config_dict[\"max_seq_length\"],\n",
    "            label_column= config_dict[\"label_column\"],\n",
    "            is_same= config_dict[\"is_same_smiles\"],\n",
    "            num_invitro_tasks = config_dict[\"num_invitro_tasks\"],\n",
    "            num_physchem= config_dict[\"num_physchem_properties\"],\n",
    "            permute= config_dict[\"permute\"],\n",
    "            named_descriptor_set=config_dict[\"named_descriptor_set\"],\n",
    "            inference_mode = False\n",
    "        )\n",
    "\n",
    "validation_dataset = BertSmilesDataset(\n",
    "            input_path= config_dict['valid_file'],\n",
    "            featurizer= featurizer,\n",
    "            single_seq_len= config_dict[\"max_seq_length\"],\n",
    "            total_seq_len= config_dict[\"max_seq_length\"],\n",
    "            label_column= config_dict[\"label_column\"],\n",
    "            is_same= config_dict[\"is_same_smiles\"],\n",
    "            num_invitro_tasks = config_dict[\"num_invitro_tasks\"],\n",
    "            num_physchem= config_dict[\"num_physchem_properties\"],\n",
    "            permute= config_dict[\"permute\"],\n",
    "            named_descriptor_set=config_dict[\"named_descriptor_set\"],\n",
    "            inference_mode = True\n",
    "\n",
    "        )\n",
    "\n",
    "test_dataset = BertSmilesDataset(\n",
    "            input_path= config_dict['test_file'],\n",
    "            featurizer= featurizer,\n",
    "            single_seq_len= config_dict[\"max_seq_length\"],\n",
    "            total_seq_len= config_dict[\"max_seq_length\"],\n",
    "            label_column= config_dict[\"label_column\"],\n",
    "            is_same= config_dict[\"is_same_smiles\"],\n",
    "            num_invitro_tasks = config_dict[\"num_invitro_tasks\"],\n",
    "            num_physchem= config_dict[\"num_physchem_properties\"],\n",
    "            permute= config_dict[\"permute\"],\n",
    "            named_descriptor_set=config_dict[\"named_descriptor_set\"],\n",
    "            inference_mode = True\n",
    "\n",
    ")\n",
    "\n",
    "train_dataloader = MolbertDataLoader(train_dataset, \n",
    "                                    batch_size=config_dict[\"batch_size\"],\n",
    "                                    pin_memory=False,\n",
    "                                    num_workers=24, \n",
    "                                    shuffle = True)\n",
    "\n",
    "validation_dataloader = MolbertDataLoader(validation_dataset, \n",
    "                                    batch_size=config_dict[\"batch_size\"],\n",
    "                                    pin_memory=False,\n",
    "                                    num_workers=24, \n",
    "                                    shuffle = False)\n",
    "\n",
    "test_dataloader = MolbertDataLoader(test_dataset, \n",
    "                                    batch_size=config_dict[\"batch_size\"],\n",
    "                                    pin_memory=False,\n",
    "                                    num_workers=24, \n",
    "                                    shuffle = False)\n",
    "\n",
    "config_dict[\"num_batches\"] = len(train_dataloader)\n",
    "                                                     \n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def compute_ece(y_true, y_prob, n_bins=10, equal_intervals = True):\n",
    "    # Calculate bin boundaries\n",
    "    if equal_intervals == True: # ECE\n",
    "        bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    else:                       # ACE\n",
    "        bin_boundaries = np.percentile(y_prob, np.linspace(0, 100, n_bins + 1))\n",
    "    \n",
    "    # Calculate bin indices\n",
    "    bin_indices = np.digitize(y_prob, bin_boundaries[1:-1])\n",
    "    \n",
    "    ece = 0\n",
    "    total_samples = len(y_true)\n",
    "    \n",
    "    # Calculate ECE\n",
    "    for bin_idx in range(n_bins):\n",
    "        # Filter samples within the bin\n",
    "        bin_mask = bin_indices == bin_idx\n",
    "        bin_samples = np.sum(bin_mask)\n",
    "        \n",
    "        if bin_samples > 0:\n",
    "            # Calculate accuracy and confidence for the bin\n",
    "            bin_accuracy = np.mean(y_true[bin_mask])\n",
    "            bin_confidence = np.mean(y_prob[bin_mask])\n",
    "        \n",
    "            # Update ECE\n",
    "            ece += (bin_samples / total_samples) * np.abs(bin_accuracy - bin_confidence)\n",
    "\n",
    "    return ece\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma, pos_weight):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.w_p = pos_weight\n",
    "\n",
    "\n",
    "    def forward(self,y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Focal Loss function for binary classification.\n",
    "\n",
    "        Arguments:\n",
    "        y_true -- true binary labels (0 or 1), torch.Tensor\n",
    "        y_pred -- predicted probabilities for the positive class, torch.Tensor\n",
    "\n",
    "        Returns:\n",
    "        Focal Loss\n",
    "        \"\"\"\n",
    "        # Compute class weight\n",
    "        p = torch.sigmoid(y_pred)\n",
    "\n",
    "        # Compute focal loss for positive and negative examples\n",
    "        focal_loss_pos = - self.w_p * (1 - p) ** self.gamma * y_true * torch.log(p.clamp(min=1e-8))\n",
    "        focal_loss_pos_neg = - p ** self.gamma * (1 - y_true) * torch.log((1 - p).clamp(min=1e-8))\n",
    "\n",
    "        return focal_loss_pos + focal_loss_pos_neg\n",
    "    \n",
    "class MolbertModel(pl.LightningModule):\n",
    "    def __init__(self, args: Namespace):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.training_step_ytrue, self.training_step_ypred = [],[]\n",
    "        self.val_step_ytrue, self.val_step_ypred = [],[]\n",
    "\n",
    "        self.hparams = args\n",
    "        self.get_creterian(args)\n",
    "\n",
    "        # get model, load pretrained weights, and freeze encoder        \n",
    "        model = SmilesMolbertModel(self.hparams)\n",
    "        if self.hparams.pretrained_model:\n",
    "            checkpoint = torch.load(self.hparams.pretrained_model_path, map_location=lambda storage, loc: storage)\n",
    "            model.load_state_dict(checkpoint['state_dict'], strict = False)\n",
    "        \n",
    "        if self.hparams.freeze_level:\n",
    "            # Freeze model\n",
    "            MolbertModel.freeze_network(model, self.hparams.freeze_level)\n",
    "\n",
    "        self.encoder = model.model.bert\n",
    "        self.Masked_LM_task = model.model.tasks[0]\n",
    "        self.invitro_task = model.model.tasks[1]\n",
    "\n",
    "        #3checkpoint = torch.load(self.hparams.pretrained_crash_model, map_location=lambda storage, loc: storage)\n",
    "        #self.load_state_dict(checkpoint['state_dict'], strict = True)\n",
    "\n",
    "    def forward(self, batch_inputs):\n",
    "        #input_ids =  batch_inputs[\"input_ids\"]\n",
    "        #token_type_ids = batch_inputs[\"token_type_ids\"]\n",
    "        #attention_mask = batch_inputs[\"attention_mask\"]\n",
    "\n",
    "        sequence_output, pooled_output = self.encoder(**batch_inputs)\n",
    "        Masked_token_pred = self.Masked_LM_task(sequence_output, pooled_output)\n",
    "        invitro_pred = self.invitro_task(sequence_output, pooled_output)\n",
    "\n",
    "        return Masked_token_pred, invitro_pred\n",
    "    \n",
    "    def get_creterian(self, config):\n",
    "        # pos weights\n",
    "    \n",
    "        if self.hparams.alpha > 0:\n",
    "            pos_weights = pd.read_csv(config[\"pos_weights\"])\n",
    "            if self.hparams.num_of_tasks == 1:\n",
    "                pos_weights = pos_weights.set_index(\"Targets\").reindex([config[\"selected_tasks\"]]).weights.values\n",
    "            else:\n",
    "                pos_weights = pos_weights.set_index(\"Targets\").reindex(config[\"selected_tasks\"]).weights.values\n",
    "            pos_weights = (config[\"alpha\"] * pos_weights) + (1 - config[\"alpha\"])*1\n",
    "            self.pos_weights = torch.tensor(pos_weights, device = config[\"device\"])\n",
    "        else:\n",
    "            self.pos_weights = torch.tensor([1.0]* config_dict[\"num_of_tasks\"], device = config[\"device\"])\n",
    "\n",
    "        alpha_null = torch.isnan(self.pos_weights).any()\n",
    "        assert not alpha_null, \"There are null values in the pos_weight tensor\"\n",
    "\n",
    "        # class weights\n",
    "        if self.hparams.beta > 0:\n",
    "            if self.hparams.num_of_tasks > 1:\n",
    "                class_weights = pd.read_csv(config[\"class_weights\"])\n",
    "                class_weights = class_weights.set_index(\"Targets\").reindex(config[\"selected_tasks\"]).weights.values\n",
    "                class_weights = (config[\"beta\"] * class_weights) + (1 - config[\"beta\"])*1\n",
    "                self.class_weights = torch.tensor(class_weights, device = config[\"device\"])\n",
    "            else:\n",
    "                self.class_weights = torch.tensor([1.0], device = config[\"device\"])\n",
    "\n",
    "            beta_null = torch.isnan(self.class_weights).any()\n",
    "            assert not beta_null, \"There are null values in the class_weight tensor\"\n",
    "\n",
    "            # train_weighted loss, validation no weights\n",
    "            self.weighted_creterien =  nn.BCEWithLogitsLoss(reduction=\"none\", \n",
    "                                                            pos_weight= self.pos_weights,\n",
    "                                                            weight= self.class_weights)\n",
    "        else:\n",
    "            self.weighted_creterien =  nn.BCEWithLogitsLoss(reduction=\"none\", \n",
    "                                                            pos_weight= self.pos_weights)\n",
    "        \n",
    "        self.FL = FocalLoss(gamma=config['gamma'], pos_weight= self.pos_weights)\n",
    "        self.non_weighted_creterian =  nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "    \n",
    "    def l2_regularization(self):\n",
    "        device = torch.device('cuda')\n",
    "        l2_reg = torch.tensor(0., requires_grad=True, device=device)\n",
    "        \n",
    "        # Apply only on weights, exclude bias\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                l2_reg = l2_reg + torch.norm(param, p=2)\n",
    "        return l2_reg\n",
    "    \n",
    "    def add_weight_decay(self, skip_list=()):\n",
    "        decay = []\n",
    "        no_decay = []\n",
    "        for name, param in self.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "            if len(param.shape) == 1 or name in skip_list:\n",
    "                no_decay.append(param)\n",
    "            else:\n",
    "                decay.append(param)\n",
    "            \n",
    "        return [\n",
    "            {'params': no_decay, 'weight_decay': 0.},\n",
    "            {'params': decay, 'weight_decay': self.hparams.l2_lambda}]\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer_grouped_parameters = self.add_weight_decay(skip_list=())\n",
    "\n",
    "        if self.hparams.optim == 'SGD':\n",
    "            optimizer = torch.optim.SGD(optimizer_grouped_parameters, \n",
    "                                             lr=self.hparams.learning_rate)\n",
    "        if self.hparams.optim == 'Adam':\n",
    "            optimizer = torch.optim.Adam(optimizer_grouped_parameters, \n",
    "                                             lr=self.hparams.learning_rate)\n",
    "        if self.hparams.optim == 'AdamW':    \n",
    "            optimizer = AdamW(optimizer_grouped_parameters, \n",
    "                                lr=self.hparams.learning_rate, \n",
    "                                eps=self.hparams.adam_epsilon)\n",
    "        \n",
    "        scheduler = self._initialise_lr_scheduler(optimizer)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def _initialise_lr_scheduler(self, optimizer):\n",
    "\n",
    "        \n",
    "        num_training_steps = self.hparams.num_batches // self.hparams.accumulate_grad_batches * self.hparams.max_epochs\n",
    "        warmup_steps = int(num_training_steps * self.hparams.warmup_proportion)\n",
    "\n",
    "        if self.hparams.learning_rate_scheduler == 'linear_with_warmup':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps\n",
    "            )\n",
    "        elif self.hparams.learning_rate_scheduler == 'cosine_with_hard_restarts_warmup':\n",
    "            scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps, num_cycles=1\n",
    "            )\n",
    "        elif self.hparams.learning_rate_scheduler == 'cosine_schedule_with_warmup':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps\n",
    "            )\n",
    "        elif self.hparams.learning_rate_scheduler == 'constant_schedule_with_warmup':\n",
    "            scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)\n",
    "\n",
    "        elif self.hparams.learning_rate_scheduler == 'cosine_annealing_warm_restarts':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, warmup_steps)\n",
    "        elif self.hparams.learning_rate_scheduler == 'reduce_on_plateau':\n",
    "            scheduler = ReduceLROnPlateau(optimizer)\n",
    "        elif self.hparams.learning_rate_scheduler == 'constant':\n",
    "            scheduler = StepLR(optimizer, 10, gamma=1.0)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'learning_rate_scheduler needs to be one of '\n",
    "                f'linear_with_warmup, cosine_with_hard_restarts_warmup, cosine_schedule_with_warmup, '\n",
    "                f'constant_schedule_with_warmup, cosine_annealing_warm_restarts, reduce_on_plateau, '\n",
    "                f'step_lr. '\n",
    "                f'Given: {self.hparams.learning_rate_scheduler}'\n",
    "            )\n",
    "\n",
    "        logger.info(\n",
    "            f'SCHEDULER: {self.hparams.learning_rate_scheduler} '\n",
    "            f'num_batches={self.hparams.num_batches} '\n",
    "            f'num_training_steps={num_training_steps} '\n",
    "            f'warmup_steps={warmup_steps}'\n",
    "        )\n",
    "\n",
    "        return {'scheduler': scheduler, 'monitor': 'valid_loss', 'interval': 'step', 'frequency': 1}\n",
    "    \n",
    "    def _compute_loss(self, y, y_hat):\n",
    "        if self.hparams.num_of_tasks == 1:\n",
    "            y = y.unsqueeze(1)\n",
    "        # compute losses, wiht masking\n",
    "        if self.hparams.missing == 'nan':\n",
    "            nan_mask = torch.isnan(y)\n",
    "            y[nan_mask] = -1\n",
    "            #y = torch.nan_to_num(y, nan = -1), for newer version\n",
    "        \n",
    "        # masks\n",
    "        valid_label_mask = (y != -1).float()\n",
    "        pos_label_mask = (y == 1)\n",
    "        negative_label_mask = (y == 0)\n",
    "\n",
    "        if self.hparams.loss_type == \"BCE\":\n",
    "            weighted_loss = self.weighted_creterien(y_hat, y) * valid_label_mask\n",
    "        if self.hparams.loss_type == \"Focal_loss\":\n",
    "            weighted_loss = self.FL(y_hat, y)* valid_label_mask\n",
    "        Non_weighted_loss = self.non_weighted_creterian(y_hat, y) * valid_label_mask\n",
    "        \n",
    "        # Non_weighted_loss, positive negative loss\n",
    "        pos_loss = Non_weighted_loss * pos_label_mask\n",
    "        neg_loss = Non_weighted_loss * negative_label_mask\n",
    "        pos_loss = pos_loss.sum() / pos_label_mask.sum()\n",
    "        neg_loss = neg_loss.sum() / negative_label_mask.sum()\n",
    "    \n",
    "        # compute mean loss\n",
    "        Non_weighted_loss = Non_weighted_loss.sum() / valid_label_mask.sum()\n",
    "        weighted_loss = weighted_loss.sum() / valid_label_mask.sum()\n",
    "\n",
    "        return weighted_loss, Non_weighted_loss, pos_loss, neg_loss\n",
    "    \n",
    "    def MaskedLM_loss(self, batch_labels, batch_predictions):\n",
    "\n",
    "        loss_fn = CrossEntropyLoss(ignore_index=-1)\n",
    "        vocab_size = self.hparams.vocab_size\n",
    "        loss = loss_fn(batch_predictions.view(-1, vocab_size), \n",
    "                batch_labels['lm_label_ids'].view(-1))  \n",
    "        return loss  \n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        step_name = \"train\"\n",
    "        # compute forward pass\n",
    "        (batch_inputs, batch_labels), valid = batch\n",
    "        self.logger.log_metrics({f\"{step_name}_valid_SMILES\":valid.sum().item()}, step = True)\n",
    "\n",
    "        y_invitro = batch_labels[\"invitro\"].squeeze()\n",
    "        Masked_token_pred, invitro_pred = self.forward(batch_inputs)\n",
    "\n",
    "        # classification loss\n",
    "        weighted_loss, Non_weighted_loss, pos_loss, neg_loss = self._compute_loss(y_invitro, invitro_pred) \n",
    "        masking_loss = self.MaskedLM_loss(batch_labels, Masked_token_pred) \n",
    "        total_loss = weighted_loss + masking_loss\n",
    "        \n",
    "        self.training_step_ytrue.append(y_invitro.long().detach().cpu())\n",
    "        self.training_step_ypred.append(torch.sigmoid(invitro_pred.detach().cpu()))\n",
    "\n",
    "        self.logger.log_metrics({f\"{step_name}_total_loss\":total_loss.item()}, step = True)\n",
    "        self.logger.log_metrics({f\"{step_name}_weighted_loss\":weighted_loss.item()}, step = True)\n",
    "        self.logger.log_metrics({f\"{step_name}_masking_loss\":masking_loss.item()}, step = True)\n",
    "        self.logger.log_metrics({f\"{step_name}_Non_weighted_loss\":Non_weighted_loss.item()}, step = True)\n",
    "        \n",
    "        #self.logger.log_metrics({f\"{step_name}_pos_loss\":pos_loss}, step = True)\n",
    "        #self.logger.log_metrics({f\"{step_name}_neg_loss\":neg_loss}, step = True)\n",
    "\n",
    "\n",
    "        return {\"loss\": total_loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        step_name = \"val\"\n",
    "        with torch.no_grad():\n",
    "            (batch_inputs, batch_labels), _ = batch\n",
    "            y_invitro = batch_labels[\"invitro\"].squeeze()\n",
    "            Masked_token_pred, invitro_pred = self.forward(batch_inputs)\n",
    "\n",
    "            # classification loss\n",
    "            weighted_loss, Non_weighted_loss, pos_loss, neg_loss = self._compute_loss(y_invitro, invitro_pred) \n",
    "            masking_loss = self.MaskedLM_loss(batch_labels, Masked_token_pred) \n",
    "            total_loss = weighted_loss + masking_loss\n",
    "\n",
    "            self.val_step_ytrue.append(y_invitro.long().detach().cpu())\n",
    "            self.val_step_ypred.append(torch.sigmoid(invitro_pred).detach().cpu())\n",
    "\n",
    "        self.logger.log_metrics({f\"{step_name}_total_loss\":total_loss.item()}, step = True)\n",
    "        self.logger.log_metrics({f\"{step_name}_weighted_loss\":weighted_loss.item()}, step = True)\n",
    "        self.logger.log_metrics({f\"{step_name}_masking_loss\":masking_loss.item()}, step = True)\n",
    "        self.logger.log_metrics({f\"{step_name}_Non_weighted_loss\":Non_weighted_loss.item()}, step = True)\n",
    "        #self.logger.log_metrics({f\"{step_name}_pos_loss\":pos_loss}, step = True)\n",
    "        #self.logger.log_metrics({f\"{step_name}_neg_loss\":neg_loss}, step = True)\n",
    "        return {f'{step_name}_loss':total_loss}\n",
    "    '''\n",
    "    def on_epoch_start(self):\n",
    "        # Check if current epoch is greater than or equal to the desired epoch to unfreeze\n",
    "        if self.current_epoch >= self.hparams.unfreeze_epoch:\n",
    "            self.unfreeze_model()\n",
    "\n",
    "            # Decrease the learning rate\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = self.hparams.BERT_lr\n",
    "            \n",
    "            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, \n",
    "                                                                        last_epoch = self.scheduler.last_epoch,\n",
    "                                                                        T_max = 10, \n",
    "                                                                        eta_min=1e-6)\n",
    "    '''        \n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        wandb.log({f'train_loss_epoch':avg_loss.item()})\n",
    "\n",
    "        # Log the learning rate at the end of each epoch\n",
    "        lr = self.trainer.optimizers[0].param_groups[0]['lr']\n",
    "        wandb.log({'learning_rate': lr})\n",
    "        \n",
    "        # Collect predictions and true labels for the complete training set\n",
    "        train_true = torch.cat(self.training_step_ytrue, dim=0)\n",
    "        train_preds = torch.cat(self.training_step_ypred, dim=0)\n",
    "\n",
    "        score_list =  self.compute_metrics(train_true, train_preds)\n",
    "        metric = ['roc_score', 'blc_acc', 'sensitivity', 'specificity', 'AUPR', 'f1_score', 'average_precision','ECE_score','ACE_score']\n",
    "            \n",
    "        for i, score in enumerate(score_list):\n",
    "            wandb.log({f'train_{metric[i]}':score.item()})\n",
    "        \n",
    "        # Clear the lists to free memory for the next epoch\n",
    "        self.training_step_ytrue.clear()\n",
    "        self.training_step_ypred.clear()\n",
    "        del train_true,train_preds\n",
    "\n",
    "        return {'train_loss':avg_loss}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        wandb.log({f'val_loss_epoch':avg_loss.item()})\n",
    "        #Collect predictions and true labels for the complete training set\n",
    "        val_true = torch.cat(self.val_step_ytrue, dim=0)\n",
    "        val_preds = torch.cat(self.val_step_ypred, dim=0)\n",
    "\n",
    "        score_list =  self.compute_metrics(val_true,val_preds)\n",
    "        metric = ['roc_score', 'blc_acc', 'sensitivity', 'specificity', 'AUPR', 'f1_score', 'average_precision','ECE_score','ACE_score']\n",
    "            \n",
    "        for i, score in enumerate(score_list):\n",
    "            wandb.log({f'val_{metric[i]}':score.item()})\n",
    "\n",
    "        # Clear the lists to free memory for the next epoch\n",
    "        self.val_step_ytrue.clear()\n",
    "        self.val_step_ypred.clear()\n",
    "        del val_true, val_preds\n",
    "\n",
    "        return {'val_loss':avg_loss}\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "\n",
    "        weight_norm = self.l2_regularization()\n",
    "        tensorboard_logs = {'weight_norm': weight_norm}\n",
    "        wandb.log(tensorboard_logs)\n",
    "\n",
    "       # Then clean the cache\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        for gpu_id in range(num_gpus):\n",
    "            with torch.cuda.device(f'cuda:{gpu_id}'):\n",
    "                torch.cuda.empty_cache()\n",
    "        # then collect the garbage\n",
    "        gc.collect()\n",
    "        print(\"!!!!!!!!! ALL CLEAR !!!!!!!!!!!!!!!!!\")\n",
    "\n",
    "    def compute_metrics(self, y_true, y_pred): \n",
    "        self.eval()\n",
    "\n",
    "        targets =  y_true.cpu().detach().tolist()\n",
    "        preds = y_pred.cpu().detach().tolist()\n",
    "\n",
    "        targets = np.array(targets).reshape(-1,self.hparams.num_of_tasks)\n",
    "        preds = np.array(preds).reshape(-1,self.hparams.num_of_tasks)\n",
    "\n",
    "        #if self.hparams.missing == 'nan':\n",
    "        #    mask = ~np.isnan(targets)\n",
    "        \n",
    "        mask = (targets != -1)\n",
    "\n",
    "        roc_score, blc_acc, sensitivity, specificity, AUPR, f1, average_precision = [],[],[],[],[],[],[]\n",
    "        ECE_score, ACE_score = [],[]\n",
    "\n",
    "        for i in range(self.hparams.num_of_tasks):\n",
    "            \n",
    "            try:\n",
    "                # get valid targets, and convert logits to prob\n",
    "                valid_targets = targets[:,i][mask[:,i]]\n",
    "                valid_preds = expit(preds[:,i][mask[:,i]])\n",
    "                ECE= compute_ece(valid_targets, valid_preds, n_bins=10, equal_intervals = True)\n",
    "                ACE = compute_ece(valid_targets, valid_preds, n_bins=10, equal_intervals = False)\n",
    "                ECE_score.append(ECE)\n",
    "                ACE_score.append(ACE)\n",
    "            except:\n",
    "                ECE_score.append(np.nan)\n",
    "                ACE_score.append(np.nan)\n",
    "\n",
    "            try:\n",
    "                # ROC_AUC\n",
    "                fpr, tpr, th = roc_curve(valid_targets, valid_preds)\n",
    "                roc_score.append(auc(fpr, tpr))\n",
    "\n",
    "                # Balanced accuracy\n",
    "                balanced_accuracy = (tpr + (1 - fpr)) / 2\n",
    "                blc_acc.append(np.max(balanced_accuracy))\n",
    "\n",
    "                # sensitivity, specificity\n",
    "                optimal_threshold_index = np.argmax(balanced_accuracy)\n",
    "                optimal_threshold = th[optimal_threshold_index]\n",
    "                sensitivity.append(tpr[optimal_threshold_index])\n",
    "                specificity.append(1 - fpr[optimal_threshold_index])\n",
    "\n",
    "                # AUPR, F1\n",
    "                precision, recall, thresholds = precision_recall_curve(valid_targets, valid_preds)\n",
    "                AUPR.append(auc(recall, precision))\n",
    "                f1_sc = f1_score(valid_targets, self.prob_to_labels(valid_preds, optimal_threshold))\n",
    "                f1.append(f1_sc)\n",
    "                average_precision.append(average_precision_score(valid_targets, valid_preds))\n",
    "                \n",
    "            except:\n",
    "                roc_score.append(np.nan)\n",
    "                AUPR.append(np.nan)\n",
    "                average_precision.append(np.nan)\n",
    "                #print('Performance metric is null')\n",
    "                \n",
    "        self.train()\n",
    "        return np.nanmean(roc_score), np.nanmean(blc_acc), np.nanmean(sensitivity), np.nanmean(specificity), np.nanmean(AUPR), np.nanmean(f1), np.nanmean(average_precision),np.nanmean(ECE_score),np.nanmean(ACE_score)\n",
    "\n",
    "    \n",
    "    def prob_to_labels(self, pred, threshold):\n",
    "\t    return (pred >= threshold).astype('int')\n",
    "\n",
    "    def unfreeze_model(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    @staticmethod\n",
    "    def freeze_network(model, freeze_level: int):\n",
    "        \"\"\"\n",
    "        Freezes specific layers of the model depending on the freeze_level argument:\n",
    "\n",
    "         0: freeze nothing\n",
    "        -1: freeze all BERT weights but not the task head\n",
    "        -2: freeze the pooling layer\n",
    "        -3: freeze the embedding layer\n",
    "        -4: freeze the task head but not the base layer\n",
    "        n>0: freeze the bottom n layers of the base model.\n",
    "        \"\"\"\n",
    "\n",
    "        model_bert = model.model.bert\n",
    "        model_tasks = model.model.tasks\n",
    "\n",
    "        model_bert_encoder = model.model.bert.encoder\n",
    "        model_bert_pooler = model.model.bert.pooler\n",
    "        model_bert_embeddings = model.model.bert.embeddings\n",
    "\n",
    "        if freeze_level == 0:\n",
    "            # freeze nothing\n",
    "            return\n",
    "\n",
    "        elif freeze_level > 0:\n",
    "            # freeze the encoder/transformer\n",
    "            n_encoder_layers = len(model_bert_encoder.layer)\n",
    "\n",
    "            # we'll always freeze layers bottom up - starting from layers closest to the embeddings\n",
    "            frozen_layers = min(freeze_level, n_encoder_layers)\n",
    "            #\n",
    "            for i in range(frozen_layers):\n",
    "                layer = model_bert_encoder.layer[i]\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        elif freeze_level == -1:\n",
    "            # freeze everything bert\n",
    "            for param in model_bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        elif freeze_level == -2:\n",
    "            # freeze the pooling layer\n",
    "            for param in model_bert_pooler.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        elif freeze_level == -3:\n",
    "            # freeze the embedding layer\n",
    "            for param in model_bert_embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        elif freeze_level == -4:\n",
    "            # freeze the task head\n",
    "            for param in model_tasks.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "for epoch in range(1):\n",
    "\n",
    "    print(\"epoch\", epoch)\n",
    "\n",
    "    # if representations does not exists & model weights are availble \n",
    "    representaitons_path = config_dict['representation_dir'] + f\"invitro_pretrained_{epoch}.csv\"\n",
    "    model_weights = config_dict['model_weights_dir'] + f\"epoch={epoch}-val_f1_score=0.00.ckpt\"\n",
    "\n",
    "    if (os.path.exists(representaitons_path) == False) & (os.path.exists(model_weights) == True):\n",
    "        seed_everything(config_dict[\"seed\"])\n",
    "        model = MolbertModel(config_dict)\n",
    "\n",
    "        # load weights\n",
    "        checkpoint = torch.load(model_weights, map_location=lambda storage, loc: storage)\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict = True)\n",
    "\n",
    "        model.eval()\n",
    "        model.freeze()\n",
    "        model = model.to(device)\n",
    "\n",
    "        # get preclinical_clinical data\n",
    "        clinical_pre_clinical_data = pd.read_csv(\"/projects/home/mmasood1/arslan_data_repository/Mix_clinical_pre_clinical/02_05_2024/clinical_pre_clinical_with_blood_marker_filtered.csv\")\n",
    "        SMILES = clinical_pre_clinical_data.SMILES.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Filter invalids\\nfiltered = [mask[0] for mask in masks_all]\\nfiltered_data = clinical_pre_clinical_data[filtered].reset_index(drop = True)\\nSMILES = filtered_data[\"SMILES\"].tolist()\\n\\nfeatures = pd.DataFrame(features_all)\\nrepresntations = features[filtered]\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SMILES = clinical_pre_clinical_data.SMILES.tolist()[:100]\n",
    "# get representaions\n",
    "f = MolBertFeaturizer(model = model,\n",
    "                featurizer= featurizer,\n",
    "                device = \"cpu\")\n",
    "\n",
    "features_all, masks_all = [],[]\n",
    "#for s in tqdm(SMILES):\n",
    "features, masks = f.transform([s])\n",
    "    #features_all.append(features.squeeze())\n",
    "    masks_all.append(masks)\n",
    "\n",
    "'''\n",
    "# Filter invalids\n",
    "filtered = [mask[0] for mask in masks_all]\n",
    "filtered_data = clinical_pre_clinical_data[filtered].reset_index(drop = True)\n",
    "SMILES = filtered_data[\"SMILES\"].tolist()\n",
    "\n",
    "features = pd.DataFrame(features_all)\n",
    "represntations = features[filtered]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(6,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "# Filter invalids\n",
    "filtered = [mask[0] for mask in masks_all]\n",
    "filtered_data = clinical_pre_clinical_data[filtered].reset_index(drop = True)\n",
    "SMILES = filtered_data[\"SMILES\"].tolist()\n",
    "\n",
    "features = pd.DataFrame(features_all)\n",
    "represntations = features[filtered]\n",
    "\n",
    "represntations = pd.DataFrame(represntations)\n",
    "represntations.insert(0, \"SMILES\", SMILES)\n",
    "\n",
    "# compared with previous MOLBERT and drop the extra molecules\n",
    "old_data = pd.read_csv(\"/projects/home/mmasood1/trained_model_predictions/SIDER_PreClinical/Reproduceability/clinical_pre_clinical_with_blood_marker_filtered.csv\")\n",
    "filtered_data = filtered_data[filtered_data.SMILES.isin(old_data.SMILES)].reset_index(drop = True)\n",
    "represntations = represntations[represntations.SMILES.isin(old_data.SMILES)].reset_index(drop = True)\n",
    "\n",
    "represntations.to_csv(representaitons_path, index= False)\n",
    "filtered_data.to_csv(config_dict['representation_dir'] + \"clinical_pre_clinical_with_blood_marker_filtered.csv\", index= False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "import gc\n",
    "\n",
    "import os, yaml\n",
    "from argparse import Namespace\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_curve\n",
    "from sklearn.metrics import average_precision_score, f1_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "import wandb\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "wandb.login(key = \"27edf9c66b032c03f72d30e923276b93aa736429\")\n",
    "\n",
    "from molbert.models.smiles import SmilesMolbertModel\n",
    "from molbert.datasets.dataloading import MolbertDataLoader\n",
    "from molbert.datasets.smiles import BertSmilesDataset\n",
    "from molbert.utils.featurizer.molfeaturizer import SmilesIndexFeaturizer\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, StepLR\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_constant_schedule_with_warmup,\n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import logging\n",
    "from typing import Tuple, Sequence, Any, Dict, Union, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from molbert.utils.featurizer.molfeaturizer import SmilesIndexFeaturizer\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class MolBertFeaturizer:\n",
    "    \"\"\"\n",
    "    This featurizer takes a molbert model and transforms the input data and\n",
    "    returns the representation in the last layer (pooled output and sequence_output).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        featurizer,\n",
    "        device: str = None,\n",
    "        embedding_type: str = 'pooled',\n",
    "        max_seq_len: Optional[int] = None,\n",
    "        permute: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            checkpoint_path: path or S3 location of trained model checkpoint\n",
    "            device: device for torch\n",
    "            embedding_type: method to reduce MolBERT encoding to an output set of features. Default: 'pooled'\n",
    "                Other options are embeddings summed or concat across layers, and then averaged\n",
    "                Raw sequence and pooled output is also available (set to 'dict')\n",
    "                average-sum-[2|4], average-cat-[2,4], average-[1|2|3|4], average-1-cat-pooled, pooled, dict\n",
    "            max_seq_len: used by the tokenizer, SMILES longer than this will fail to featurize\n",
    "                MolBERT was trained with SuperPositionalEncodings (TransformerXL) to decoupled from the training setup\n",
    "                By default the training config is used (128). If you have long SMILES to featurize, increase this value\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = device or 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.embedding_type = embedding_type\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.permute = permute\n",
    "\n",
    "        # load smiles index featurizer\n",
    "        self.featurizer = featurizer\n",
    "\n",
    "        # load model\n",
    "        self.model = model\n",
    "\n",
    "    def __getstate__(self):\n",
    "        self.__dict__.update({'model': self.model.to('cpu')})\n",
    "        self.__dict__.update({'device': 'cpu'})\n",
    "        return self.__dict__\n",
    "\n",
    "    @property\n",
    "\n",
    "    def transform_single(self, smiles: str) -> Tuple[np.ndarray, bool]:\n",
    "        features, valid = self.transform([smiles])\n",
    "        return features, valid[0]\n",
    "\n",
    "    def transform(self, molecules: Sequence[Any]) -> Tuple[Union[Dict, np.ndarray], np.ndarray]:\n",
    "        input_ids, valid = self.featurizer.transform(molecules)\n",
    "\n",
    "        input_ids = self.trim_batch(input_ids, valid)\n",
    "\n",
    "        token_type_ids = np.zeros_like(input_ids, dtype=np.int64)\n",
    "        attention_mask = np.zeros_like(input_ids, dtype=np.int64)\n",
    "\n",
    "        attention_mask[input_ids != 0] = 1\n",
    "\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long, device=self.device)\n",
    "        token_type_ids = torch.tensor(token_type_ids, dtype=torch.long, device=self.device)\n",
    "        attention_mask = torch.tensor(attention_mask, dtype=torch.long, device=self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.encoder(\n",
    "                input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "        sequence_output, pooled_output = outputs\n",
    "\n",
    "        # set invalid outputs to 0s\n",
    "        valid_tensor = torch.tensor(\n",
    "            valid, dtype=sequence_output.dtype, device=sequence_output.device, requires_grad=False\n",
    "        )\n",
    "\n",
    "        pooled_output = pooled_output * valid_tensor[:, None]\n",
    "        sequence_out = sequence_output * valid_tensor[:, None, None]\n",
    "\n",
    "        sequence_out = sequence_out.detach().cpu().numpy()\n",
    "        pooled_output = pooled_output.detach().cpu().numpy()\n",
    "        out = pooled_output\n",
    "\n",
    "        return out, valid\n",
    "\n",
    "    @staticmethod\n",
    "    def trim_batch(input_ids, valid):\n",
    "\n",
    "        # trim input horizontally if there is at least 1 valid data point\n",
    "        if any(valid):\n",
    "            _, cols = np.where(input_ids[valid] != 0)\n",
    "        # else trim input down to 1 column (avoids empty batch error)\n",
    "        else:\n",
    "            cols = np.array([0])\n",
    "\n",
    "        max_idx: int = int(cols.max().item() + 1)\n",
    "\n",
    "        input_ids = input_ids[:, :max_idx]\n",
    "\n",
    "        return input_ids\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# config_dict\n",
    "#model_weights_dir = '/projects/home/mmasood1/Model_weights/invitro/invitro_1million/MolBERT/default_lr_schedular/'\n",
    "#model_weights_dir = '/projects/home/mmasood1/Model_weights/invitro/invitro_1million/MolBERT/Retrain_on_top_of_BERT/complete_1m_300k/'\n",
    "#data_dir = '/projects/home/mmasood1/arslan_data_repository/invitro/invitro_1m/25_04_2024/SMILES_len_th_100/'\n",
    "#pos_weights = \"/projects/home/mmasood1/arslan_data_repository/invitro/invitro_1m/25_04_2024/pos_weights.csv\"\n",
    "\n",
    "model_weights_dir = '/projects/home/mmasood1/Model_weights/invitro/invitro_1million/MolBERT/Retrain_on_top_of_BERT/complete_1m_300k_ADME/with_physchem_head/'\n",
    "representation_dir = '/projects/home/mmasood1/arslan_data_repository/Mix_clinical_pre_clinical/02_05_2024/Representations_BERT_invitro_pretrained/Retrain_top_of_BERT_invitro_ADME_with_Physchem/'\n",
    "pretrained_model_path = '/projects/home/mmasood1/TG GATE/MolBERT/molbert/molbert_100epochs/molbert_100epochs/checkpoints/last.ckpt'\n",
    "\n",
    "#data_dir = '/projects/home/mmasood1/arslan_data_repository/invitro/invitro_1m/25_04_2024/SMILES_len_th_100/'\n",
    "#pos_weights = \"/projects/home/mmasood1/arslan_data_repository/invitro/invitro_1m/25_04_2024/pos_weights.csv\"\n",
    "data_dir = '/projects/home/mmasood1/arslan_data_repository/invitro/invitro_1m/25_04_2024/SMILES_len_th_128/'\n",
    "pos_weights = \"/projects/home/mmasood1/arslan_data_repository/invitro/invitro_1m/25_04_2024/pos_weights.csv\"\n",
    "\n",
    "metadata_dir = \"/projects/home/mmasood1/trained_model_predictions/SIDER_PreClinical/BERT_finetune/MF/\"\n",
    "model_dir = os.path.dirname(os.path.dirname(pretrained_model_path))\n",
    "hparams_path = os.path.join(model_dir, 'hparams.yaml')\n",
    "# load config\n",
    "with open(hparams_path) as yaml_file:\n",
    "    config_dict = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "config_dict['project_name'] = \"BERT_invitro_pretraining\"\n",
    "config_dict['model_name'] = \"BERT_invitro_pretraining\"\n",
    "\n",
    "config_dict['model_weights_dir'] = model_weights_dir\n",
    "config_dict['representation_dir'] = representation_dir\n",
    "\n",
    "config_dict['pretrained_model_path'] = pretrained_model_path\n",
    "config_dict[\"metadata_dir\"] = metadata_dir\n",
    "config_dict['pos_weights'] = pos_weights\n",
    "config_dict['data_dir'] = data_dir\n",
    "#config_dict['train_file'] = data_dir + \"train_set_invitro_1m_plus_300k_filtered.pkl\"\n",
    "#config_dict['valid_file'] = data_dir + \"test_set_invitro_1m_plus_300k_filtered.pkl\"\n",
    "#config_dict['test_file'] = data_dir + \"test_set_invitro_1m_plus_300k_filtered.pkl\"\n",
    "\n",
    "config_dict['train_file'] = data_dir + \"train_set_invitro_1m_300k_ADME_filtered.pkl\"\n",
    "config_dict['valid_file'] = data_dir + \"test_set_invitro_1m_300k_ADME_filtered.pkl\"\n",
    "config_dict['test_file'] = data_dir + \"test_set_invitro_1m_300k_ADME_filtered.pkl\"\n",
    "\n",
    "config_dict['mode'] = 'classification'\n",
    "config_dict['alpha'] = 0.0\n",
    "config_dict['beta'] = 0.0\n",
    "config_dict['gamma'] = 0.0\n",
    "\n",
    "config_dict['max_epochs'] = 50\n",
    "config_dict['unfreeze_epoch'] = 210\n",
    "config_dict[\"l2_lambda\"] = 0.0\n",
    "config_dict['embedding_size'] = 50\n",
    "config_dict[\"num_physchem_properties\"] = 200\n",
    "\n",
    "config_dict['optim'] = 'AdamW'#SGD\n",
    "config_dict['loss_type'] = 'BCE'# Focal_loss\n",
    "\n",
    "config_dict['lr'] = 1e-05\n",
    "config_dict[\"BERT_lr\"] = 3e-5\n",
    "config_dict[\"batch_size\"] = 264\n",
    "config_dict[\"seed\"] = 42\n",
    "\n",
    "\n",
    "\n",
    "config_dict['missing'] = 'nan'\n",
    "config_dict['compute_metric_after_n_epochs'] = 5\n",
    "config_dict['return_trainer'] = True\n",
    "config_dict['EarlyStopping'] = False\n",
    "\n",
    "config_dict[\"accelerator\"] = \"gpu\"\n",
    "config_dict[\"device\"] = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "data = pd.read_pickle(config_dict['train_file'])\n",
    "data.drop(['SMILES'], axis = 1, inplace = True)\n",
    "target_names = data.columns.tolist()\n",
    "\n",
    "config_dict[\"output_size\"] = len(target_names)\n",
    "config_dict[\"num_invitro_tasks\"] = len(target_names)\n",
    "config_dict[\"num_of_tasks\"] = len(target_names)\n",
    "\n",
    "config_dict[\"label_column\"] = target_names\n",
    "config_dict[\"selected_tasks\"] = target_names\n",
    "config_dict['num_mols'] = data.shape[0]\n",
    "config_dict['max_seq_length'] = 128\n",
    "config_dict['bert_output_dim'] = 768\n",
    "config_dict['invitro_head_hidden_layer'] = 2048\n",
    "\n",
    "config_dict[\"permute\"] = False\n",
    "\n",
    "config_dict['pretrained_model'] = True\n",
    "config_dict['freeze_level'] = False\n",
    "config_dict[\"gpu\"] = -1\n",
    "config_dict[\"precision\"] = 32\n",
    "config_dict[\"distributed_backend\"] = \"dp\"\n",
    "config_dict[\"pretrained_crash_model\"] = None #\"/projects/home/mmasood1/Model_weights/invitro/invitro_1million/MolBERT/Retrain_on_top_of_BERT/complete_1m_300k/invitro_with_PhysChem/epoch=2-val_f1_score=0.00.ckpt\"\n",
    "\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "featurizer = SmilesIndexFeaturizer.bert_smiles_index_featurizer(config_dict[\"max_seq_length\"], permute = False)\n",
    "config_dict[\"vocab_size\"] = featurizer.vocab_size\n",
    "train_dataset = BertSmilesDataset(\n",
    "            input_path= config_dict['train_file'],\n",
    "            featurizer= featurizer,\n",
    "            single_seq_len= config_dict[\"max_seq_length\"],\n",
    "            total_seq_len= config_dict[\"max_seq_length\"],\n",
    "            label_column= config_dict[\"label_column\"],\n",
    "            is_same= config_dict[\"is_same_smiles\"],\n",
    "            num_invitro_tasks = config_dict[\"num_invitro_tasks\"],\n",
    "            num_physchem= config_dict[\"num_physchem_properties\"],\n",
    "            permute= config_dict[\"permute\"],\n",
    "            named_descriptor_set=config_dict[\"named_descriptor_set\"],\n",
    "            inference_mode = False\n",
    "        )\n",
    "\n",
    "validation_dataset = BertSmilesDataset(\n",
    "            input_path= config_dict['valid_file'],\n",
    "            featurizer= featurizer,\n",
    "            single_seq_len= config_dict[\"max_seq_length\"],\n",
    "            total_seq_len= config_dict[\"max_seq_length\"],\n",
    "            label_column= config_dict[\"label_column\"],\n",
    "            is_same= config_dict[\"is_same_smiles\"],\n",
    "            num_invitro_tasks = config_dict[\"num_invitro_tasks\"],\n",
    "            num_physchem= config_dict[\"num_physchem_properties\"],\n",
    "            permute= config_dict[\"permute\"],\n",
    "            named_descriptor_set=config_dict[\"named_descriptor_set\"],\n",
    "            inference_mode = True\n",
    "\n",
    "        )\n",
    "\n",
    "test_dataset = BertSmilesDataset(\n",
    "            input_path= config_dict['test_file'],\n",
    "            featurizer= featurizer,\n",
    "            single_seq_len= config_dict[\"max_seq_length\"],\n",
    "            total_seq_len= config_dict[\"max_seq_length\"],\n",
    "            label_column= config_dict[\"label_column\"],\n",
    "            is_same= config_dict[\"is_same_smiles\"],\n",
    "            num_invitro_tasks = config_dict[\"num_invitro_tasks\"],\n",
    "            num_physchem= config_dict[\"num_physchem_properties\"],\n",
    "            permute= config_dict[\"permute\"],\n",
    "            named_descriptor_set=config_dict[\"named_descriptor_set\"],\n",
    "            inference_mode = True\n",
    "\n",
    ")\n",
    "\n",
    "train_dataloader = MolbertDataLoader(train_dataset, \n",
    "                                    batch_size=config_dict[\"batch_size\"],\n",
    "                                    pin_memory=False,\n",
    "                                    num_workers=24, \n",
    "                                    shuffle = True)\n",
    "\n",
    "validation_dataloader = MolbertDataLoader(validation_dataset, \n",
    "                                    batch_size=config_dict[\"batch_size\"],\n",
    "                                    pin_memory=False,\n",
    "                                    num_workers=24, \n",
    "                                    shuffle = False)\n",
    "\n",
    "test_dataloader = MolbertDataLoader(test_dataset, \n",
    "                                    batch_size=config_dict[\"batch_size\"],\n",
    "                                    pin_memory=False,\n",
    "                                    num_workers=24, \n",
    "                                    shuffle = False)\n",
    "\n",
    "config_dict[\"num_batches\"] = len(train_dataloader)\n",
    "                                                     \n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def compute_ece(y_true, y_prob, n_bins=10, equal_intervals = True):\n",
    "    # Calculate bin boundaries\n",
    "    if equal_intervals == True: # ECE\n",
    "        bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    else:                       # ACE\n",
    "        bin_boundaries = np.percentile(y_prob, np.linspace(0, 100, n_bins + 1))\n",
    "    \n",
    "    # Calculate bin indices\n",
    "    bin_indices = np.digitize(y_prob, bin_boundaries[1:-1])\n",
    "    \n",
    "    ece = 0\n",
    "    total_samples = len(y_true)\n",
    "    \n",
    "    # Calculate ECE\n",
    "    for bin_idx in range(n_bins):\n",
    "        # Filter samples within the bin\n",
    "        bin_mask = bin_indices == bin_idx\n",
    "        bin_samples = np.sum(bin_mask)\n",
    "        \n",
    "        if bin_samples > 0:\n",
    "            # Calculate accuracy and confidence for the bin\n",
    "            bin_accuracy = np.mean(y_true[bin_mask])\n",
    "            bin_confidence = np.mean(y_prob[bin_mask])\n",
    "        \n",
    "            # Update ECE\n",
    "            ece += (bin_samples / total_samples) * np.abs(bin_accuracy - bin_confidence)\n",
    "\n",
    "    return ece\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma, pos_weight):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.w_p = pos_weight\n",
    "\n",
    "\n",
    "    def forward(self,y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Focal Loss function for binary classification.\n",
    "\n",
    "        Arguments:\n",
    "        y_true -- true binary labels (0 or 1), torch.Tensor\n",
    "        y_pred -- predicted probabilities for the positive class, torch.Tensor\n",
    "\n",
    "        Returns:\n",
    "        Focal Loss\n",
    "        \"\"\"\n",
    "        # Compute class weight\n",
    "        p = torch.sigmoid(y_pred)\n",
    "\n",
    "        # Compute focal loss for positive and negative examples\n",
    "        focal_loss_pos = - self.w_p * (1 - p) ** self.gamma * y_true * torch.log(p.clamp(min=1e-8))\n",
    "        focal_loss_pos_neg = - p ** self.gamma * (1 - y_true) * torch.log((1 - p).clamp(min=1e-8))\n",
    "\n",
    "        return focal_loss_pos + focal_loss_pos_neg\n",
    "    \n",
    "class MolbertModel(pl.LightningModule):\n",
    "    def __init__(self, args: Namespace):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.training_step_ytrue, self.training_step_ypred = [],[]\n",
    "        self.val_step_ytrue, self.val_step_ypred = [],[]\n",
    "\n",
    "        self.hparams = args\n",
    "        self.get_creterian(args)\n",
    "\n",
    "        # get model, load pretrained weights, and freeze encoder        \n",
    "        model = SmilesMolbertModel(self.hparams)\n",
    "        if self.hparams.pretrained_model:\n",
    "            checkpoint = torch.load(self.hparams.pretrained_model_path, map_location=lambda storage, loc: storage)\n",
    "            model.load_state_dict(checkpoint['state_dict'], strict = False)\n",
    "        \n",
    "        if self.hparams.freeze_level:\n",
    "            # Freeze model\n",
    "            MolbertModel.freeze_network(model, self.hparams.freeze_level)\n",
    "\n",
    "        self.encoder = model.model.bert\n",
    "        self.Masked_LM_task = model.model.tasks[0]\n",
    "        self.invitro_task = model.model.tasks[1]\n",
    "\n",
    "        #3checkpoint = torch.load(self.hparams.pretrained_crash_model, map_location=lambda storage, loc: storage)\n",
    "        #self.load_state_dict(checkpoint['state_dict'], strict = True)\n",
    "\n",
    "    def forward(self, batch_inputs):\n",
    "        #input_ids =  batch_inputs[\"input_ids\"]\n",
    "        #token_type_ids = batch_inputs[\"token_type_ids\"]\n",
    "        #attention_mask = batch_inputs[\"attention_mask\"]\n",
    "\n",
    "        sequence_output, pooled_output = self.encoder(**batch_inputs)\n",
    "        Masked_token_pred = self.Masked_LM_task(sequence_output, pooled_output)\n",
    "        invitro_pred = self.invitro_task(sequence_output, pooled_output)\n",
    "\n",
    "        return Masked_token_pred, invitro_pred\n",
    "    \n",
    "    def get_creterian(self, config):\n",
    "        # pos weights\n",
    "    \n",
    "        if self.hparams.alpha > 0:\n",
    "            pos_weights = pd.read_csv(config[\"pos_weights\"])\n",
    "            if self.hparams.num_of_tasks == 1:\n",
    "                pos_weights = pos_weights.set_index(\"Targets\").reindex([config[\"selected_tasks\"]]).weights.values\n",
    "            else:\n",
    "                pos_weights = pos_weights.set_index(\"Targets\").reindex(config[\"selected_tasks\"]).weights.values\n",
    "            pos_weights = (config[\"alpha\"] * pos_weights) + (1 - config[\"alpha\"])*1\n",
    "            self.pos_weights = torch.tensor(pos_weights, device = config[\"device\"])\n",
    "        else:\n",
    "            self.pos_weights = torch.tensor([1.0]* config_dict[\"num_of_tasks\"], device = config[\"device\"])\n",
    "\n",
    "        alpha_null = torch.isnan(self.pos_weights).any()\n",
    "        assert not alpha_null, \"There are null values in the pos_weight tensor\"\n",
    "\n",
    "        # class weights\n",
    "        if self.hparams.beta > 0:\n",
    "            if self.hparams.num_of_tasks > 1:\n",
    "                class_weights = pd.read_csv(config[\"class_weights\"])\n",
    "                class_weights = class_weights.set_index(\"Targets\").reindex(config[\"selected_tasks\"]).weights.values\n",
    "                class_weights = (config[\"beta\"] * class_weights) + (1 - config[\"beta\"])*1\n",
    "                self.class_weights = torch.tensor(class_weights, device = config[\"device\"])\n",
    "            else:\n",
    "                self.class_weights = torch.tensor([1.0], device = config[\"device\"])\n",
    "\n",
    "            beta_null = torch.isnan(self.class_weights).any()\n",
    "            assert not beta_null, \"There are null values in the class_weight tensor\"\n",
    "\n",
    "            # train_weighted loss, validation no weights\n",
    "            self.weighted_creterien =  nn.BCEWithLogitsLoss(reduction=\"none\", \n",
    "                                                            pos_weight= self.pos_weights,\n",
    "                                                            weight= self.class_weights)\n",
    "        else:\n",
    "            self.weighted_creterien =  nn.BCEWithLogitsLoss(reduction=\"none\", \n",
    "                                                            pos_weight= self.pos_weights)\n",
    "        \n",
    "        self.FL = FocalLoss(gamma=config['gamma'], pos_weight= self.pos_weights)\n",
    "        self.non_weighted_creterian =  nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "    \n",
    "    def l2_regularization(self):\n",
    "        device = torch.device('cuda')\n",
    "        l2_reg = torch.tensor(0., requires_grad=True, device=device)\n",
    "        \n",
    "        # Apply only on weights, exclude bias\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                l2_reg = l2_reg + torch.norm(param, p=2)\n",
    "        return l2_reg\n",
    "    \n",
    "    def add_weight_decay(self, skip_list=()):\n",
    "        decay = []\n",
    "        no_decay = []\n",
    "        for name, param in self.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "            if len(param.shape) == 1 or name in skip_list:\n",
    "                no_decay.append(param)\n",
    "            else:\n",
    "                decay.append(param)\n",
    "            \n",
    "        return [\n",
    "            {'params': no_decay, 'weight_decay': 0.},\n",
    "            {'params': decay, 'weight_decay': self.hparams.l2_lambda}]\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer_grouped_parameters = self.add_weight_decay(skip_list=())\n",
    "\n",
    "        if self.hparams.optim == 'SGD':\n",
    "            optimizer = torch.optim.SGD(optimizer_grouped_parameters, \n",
    "                                             lr=self.hparams.learning_rate)\n",
    "        if self.hparams.optim == 'Adam':\n",
    "            optimizer = torch.optim.Adam(optimizer_grouped_parameters, \n",
    "                                             lr=self.hparams.learning_rate)\n",
    "        if self.hparams.optim == 'AdamW':    \n",
    "            optimizer = AdamW(optimizer_grouped_parameters, \n",
    "                                lr=self.hparams.learning_rate, \n",
    "                                eps=self.hparams.adam_epsilon)\n",
    "        \n",
    "        scheduler = self._initialise_lr_scheduler(optimizer)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def _initialise_lr_scheduler(self, optimizer):\n",
    "\n",
    "        \n",
    "        num_training_steps = self.hparams.num_batches // self.hparams.accumulate_grad_batches * self.hparams.max_epochs\n",
    "        warmup_steps = int(num_training_steps * self.hparams.warmup_proportion)\n",
    "\n",
    "        if self.hparams.learning_rate_scheduler == 'linear_with_warmup':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps\n",
    "            )\n",
    "        elif self.hparams.learning_rate_scheduler == 'cosine_with_hard_restarts_warmup':\n",
    "            scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps, num_cycles=1\n",
    "            )\n",
    "        elif self.hparams.learning_rate_scheduler == 'cosine_schedule_with_warmup':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps\n",
    "            )\n",
    "        elif self.hparams.learning_rate_scheduler == 'constant_schedule_with_warmup':\n",
    "            scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)\n",
    "\n",
    "        elif self.hparams.learning_rate_scheduler == 'cosine_annealing_warm_restarts':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, warmup_steps)\n",
    "        elif self.hparams.learning_rate_scheduler == 'reduce_on_plateau':\n",
    "            scheduler = ReduceLROnPlateau(optimizer)\n",
    "        elif self.hparams.learning_rate_scheduler == 'constant':\n",
    "            scheduler = StepLR(optimizer, 10, gamma=1.0)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'learning_rate_scheduler needs to be one of '\n",
    "                f'linear_with_warmup, cosine_with_hard_restarts_warmup, cosine_schedule_with_warmup, '\n",
    "                f'constant_schedule_with_warmup, cosine_annealing_warm_restarts, reduce_on_plateau, '\n",
    "                f'step_lr. '\n",
    "                f'Given: {self.hparams.learning_rate_scheduler}'\n",
    "            )\n",
    "\n",
    "        logger.info(\n",
    "            f'SCHEDULER: {self.hparams.learning_rate_scheduler} '\n",
    "            f'num_batches={self.hparams.num_batches} '\n",
    "            f'num_training_steps={num_training_steps} '\n",
    "            f'warmup_steps={warmup_steps}'\n",
    "        )\n",
    "\n",
    "        return {'scheduler': scheduler, 'monitor': 'valid_loss', 'interval': 'step', 'frequency': 1}\n",
    "    \n",
    "    def _compute_loss(self, y, y_hat):\n",
    "        if self.hparams.num_of_tasks == 1:\n",
    "            y = y.unsqueeze(1)\n",
    "        # compute losses, wiht masking\n",
    "        if self.hparams.missing == 'nan':\n",
    "            nan_mask = torch.isnan(y)\n",
    "            y[nan_mask] = -1\n",
    "            #y = torch.nan_to_num(y, nan = -1), for newer version\n",
    "        \n",
    "        # masks\n",
    "        valid_label_mask = (y != -1).float()\n",
    "        pos_label_mask = (y == 1)\n",
    "        negative_label_mask = (y == 0)\n",
    "\n",
    "        if self.hparams.loss_type == \"BCE\":\n",
    "            weighted_loss = self.weighted_creterien(y_hat, y) * valid_label_mask\n",
    "        if self.hparams.loss_type == \"Focal_loss\":\n",
    "            weighted_loss = self.FL(y_hat, y)* valid_label_mask\n",
    "        Non_weighted_loss = self.non_weighted_creterian(y_hat, y) * valid_label_mask\n",
    "        \n",
    "        # Non_weighted_loss, positive negative loss\n",
    "        pos_loss = Non_weighted_loss * pos_label_mask\n",
    "        neg_loss = Non_weighted_loss * negative_label_mask\n",
    "        pos_loss = pos_loss.sum() / pos_label_mask.sum()\n",
    "        neg_loss = neg_loss.sum() / negative_label_mask.sum()\n",
    "    \n",
    "        # compute mean loss\n",
    "        Non_weighted_loss = Non_weighted_loss.sum() / valid_label_mask.sum()\n",
    "        weighted_loss = weighted_loss.sum() / valid_label_mask.sum()\n",
    "\n",
    "        return weighted_loss, Non_weighted_loss, pos_loss, neg_loss\n",
    "    \n",
    "    def MaskedLM_loss(self, batch_labels, batch_predictions):\n",
    "\n",
    "        loss_fn = CrossEntropyLoss(ignore_index=-1)\n",
    "        vocab_size = self.hparams.vocab_size\n",
    "        loss = loss_fn(batch_predictions.view(-1, vocab_size), \n",
    "                batch_labels['lm_label_ids'].view(-1))  \n",
    "        return loss  \n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        step_name = \"train\"\n",
    "        # compute forward pass\n",
    "        (batch_inputs, batch_labels), valid = batch\n",
    "        self.logger.log_metrics({f\"{step_name}_valid_SMILES\":valid.sum().item()}, step = True)\n",
    "\n",
    "        y_invitro = batch_labels[\"invitro\"].squeeze()\n",
    "        Masked_token_pred, invitro_pred = self.forward(batch_inputs)\n",
    "\n",
    "        # classification loss\n",
    "        weighted_loss, Non_weighted_loss, pos_loss, neg_loss = self._compute_loss(y_invitro, invitro_pred) \n",
    "        masking_loss = self.MaskedLM_loss(batch_labels, Masked_token_pred) \n",
    "        total_loss = weighted_loss + masking_loss\n",
    "        \n",
    "        self.training_step_ytrue.append(y_invitro.long().detach().cpu())\n",
    "        self.training_step_ypred.append(torch.sigmoid(invitro_pred.detach().cpu()))\n",
    "\n",
    "        self.logger.log_metrics({f\"{step_name}_total_loss\":total_loss.item()}, step = True)\n",
    "        self.logger.log_metrics({f\"{step_name}_weighted_loss\":weighted_loss.item()}, step = True)\n",
    "        self.logger.log_metrics({f\"{step_name}_masking_loss\":masking_loss.item()}, step = True)\n",
    "        self.logger.log_metrics({f\"{step_name}_Non_weighted_loss\":Non_weighted_loss.item()}, step = True)\n",
    "        \n",
    "        #self.logger.log_metrics({f\"{step_name}_pos_loss\":pos_loss}, step = True)\n",
    "        #self.logger.log_metrics({f\"{step_name}_neg_loss\":neg_loss}, step = True)\n",
    "\n",
    "\n",
    "        return {\"loss\": total_loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        step_name = \"val\"\n",
    "        with torch.no_grad():\n",
    "            (batch_inputs, batch_labels), _ = batch\n",
    "            y_invitro = batch_labels[\"invitro\"].squeeze()\n",
    "            Masked_token_pred, invitro_pred = self.forward(batch_inputs)\n",
    "\n",
    "            # classification loss\n",
    "            weighted_loss, Non_weighted_loss, pos_loss, neg_loss = self._compute_loss(y_invitro, invitro_pred) \n",
    "            masking_loss = self.MaskedLM_loss(batch_labels, Masked_token_pred) \n",
    "            total_loss = weighted_loss + masking_loss\n",
    "\n",
    "            self.val_step_ytrue.append(y_invitro.long().detach().cpu())\n",
    "            self.val_step_ypred.append(torch.sigmoid(invitro_pred).detach().cpu())\n",
    "\n",
    "        self.logger.log_metrics({f\"{step_name}_total_loss\":total_loss.item()}, step = True)\n",
    "        self.logger.log_metrics({f\"{step_name}_weighted_loss\":weighted_loss.item()}, step = True)\n",
    "        self.logger.log_metrics({f\"{step_name}_masking_loss\":masking_loss.item()}, step = True)\n",
    "        self.logger.log_metrics({f\"{step_name}_Non_weighted_loss\":Non_weighted_loss.item()}, step = True)\n",
    "        #self.logger.log_metrics({f\"{step_name}_pos_loss\":pos_loss}, step = True)\n",
    "        #self.logger.log_metrics({f\"{step_name}_neg_loss\":neg_loss}, step = True)\n",
    "        return {f'{step_name}_loss':total_loss}\n",
    "    '''\n",
    "    def on_epoch_start(self):\n",
    "        # Check if current epoch is greater than or equal to the desired epoch to unfreeze\n",
    "        if self.current_epoch >= self.hparams.unfreeze_epoch:\n",
    "            self.unfreeze_model()\n",
    "\n",
    "            # Decrease the learning rate\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = self.hparams.BERT_lr\n",
    "            \n",
    "            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, \n",
    "                                                                        last_epoch = self.scheduler.last_epoch,\n",
    "                                                                        T_max = 10, \n",
    "                                                                        eta_min=1e-6)\n",
    "    '''        \n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        wandb.log({f'train_loss_epoch':avg_loss.item()})\n",
    "\n",
    "        # Log the learning rate at the end of each epoch\n",
    "        lr = self.trainer.optimizers[0].param_groups[0]['lr']\n",
    "        wandb.log({'learning_rate': lr})\n",
    "        \n",
    "        # Collect predictions and true labels for the complete training set\n",
    "        train_true = torch.cat(self.training_step_ytrue, dim=0)\n",
    "        train_preds = torch.cat(self.training_step_ypred, dim=0)\n",
    "\n",
    "        score_list =  self.compute_metrics(train_true, train_preds)\n",
    "        metric = ['roc_score', 'blc_acc', 'sensitivity', 'specificity', 'AUPR', 'f1_score', 'average_precision','ECE_score','ACE_score']\n",
    "            \n",
    "        for i, score in enumerate(score_list):\n",
    "            wandb.log({f'train_{metric[i]}':score.item()})\n",
    "        \n",
    "        # Clear the lists to free memory for the next epoch\n",
    "        self.training_step_ytrue.clear()\n",
    "        self.training_step_ypred.clear()\n",
    "        del train_true,train_preds\n",
    "\n",
    "        return {'train_loss':avg_loss}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        wandb.log({f'val_loss_epoch':avg_loss.item()})\n",
    "        #Collect predictions and true labels for the complete training set\n",
    "        val_true = torch.cat(self.val_step_ytrue, dim=0)\n",
    "        val_preds = torch.cat(self.val_step_ypred, dim=0)\n",
    "\n",
    "        score_list =  self.compute_metrics(val_true,val_preds)\n",
    "        metric = ['roc_score', 'blc_acc', 'sensitivity', 'specificity', 'AUPR', 'f1_score', 'average_precision','ECE_score','ACE_score']\n",
    "            \n",
    "        for i, score in enumerate(score_list):\n",
    "            wandb.log({f'val_{metric[i]}':score.item()})\n",
    "\n",
    "        # Clear the lists to free memory for the next epoch\n",
    "        self.val_step_ytrue.clear()\n",
    "        self.val_step_ypred.clear()\n",
    "        del val_true, val_preds\n",
    "\n",
    "        return {'val_loss':avg_loss}\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "\n",
    "        weight_norm = self.l2_regularization()\n",
    "        tensorboard_logs = {'weight_norm': weight_norm}\n",
    "        wandb.log(tensorboard_logs)\n",
    "\n",
    "       # Then clean the cache\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        for gpu_id in range(num_gpus):\n",
    "            with torch.cuda.device(f'cuda:{gpu_id}'):\n",
    "                torch.cuda.empty_cache()\n",
    "        # then collect the garbage\n",
    "        gc.collect()\n",
    "        print(\"!!!!!!!!! ALL CLEAR !!!!!!!!!!!!!!!!!\")\n",
    "\n",
    "    def compute_metrics(self, y_true, y_pred): \n",
    "        self.eval()\n",
    "\n",
    "        targets =  y_true.cpu().detach().tolist()\n",
    "        preds = y_pred.cpu().detach().tolist()\n",
    "\n",
    "        targets = np.array(targets).reshape(-1,self.hparams.num_of_tasks)\n",
    "        preds = np.array(preds).reshape(-1,self.hparams.num_of_tasks)\n",
    "\n",
    "        #if self.hparams.missing == 'nan':\n",
    "        #    mask = ~np.isnan(targets)\n",
    "        \n",
    "        mask = (targets != -1)\n",
    "\n",
    "        roc_score, blc_acc, sensitivity, specificity, AUPR, f1, average_precision = [],[],[],[],[],[],[]\n",
    "        ECE_score, ACE_score = [],[]\n",
    "\n",
    "        for i in range(self.hparams.num_of_tasks):\n",
    "            \n",
    "            try:\n",
    "                # get valid targets, and convert logits to prob\n",
    "                valid_targets = targets[:,i][mask[:,i]]\n",
    "                valid_preds = expit(preds[:,i][mask[:,i]])\n",
    "                ECE= compute_ece(valid_targets, valid_preds, n_bins=10, equal_intervals = True)\n",
    "                ACE = compute_ece(valid_targets, valid_preds, n_bins=10, equal_intervals = False)\n",
    "                ECE_score.append(ECE)\n",
    "                ACE_score.append(ACE)\n",
    "            except:\n",
    "                ECE_score.append(np.nan)\n",
    "                ACE_score.append(np.nan)\n",
    "\n",
    "            try:\n",
    "                # ROC_AUC\n",
    "                fpr, tpr, th = roc_curve(valid_targets, valid_preds)\n",
    "                roc_score.append(auc(fpr, tpr))\n",
    "\n",
    "                # Balanced accuracy\n",
    "                balanced_accuracy = (tpr + (1 - fpr)) / 2\n",
    "                blc_acc.append(np.max(balanced_accuracy))\n",
    "\n",
    "                # sensitivity, specificity\n",
    "                optimal_threshold_index = np.argmax(balanced_accuracy)\n",
    "                optimal_threshold = th[optimal_threshold_index]\n",
    "                sensitivity.append(tpr[optimal_threshold_index])\n",
    "                specificity.append(1 - fpr[optimal_threshold_index])\n",
    "\n",
    "                # AUPR, F1\n",
    "                precision, recall, thresholds = precision_recall_curve(valid_targets, valid_preds)\n",
    "                AUPR.append(auc(recall, precision))\n",
    "                f1_sc = f1_score(valid_targets, self.prob_to_labels(valid_preds, optimal_threshold))\n",
    "                f1.append(f1_sc)\n",
    "                average_precision.append(average_precision_score(valid_targets, valid_preds))\n",
    "                \n",
    "            except:\n",
    "                roc_score.append(np.nan)\n",
    "                AUPR.append(np.nan)\n",
    "                average_precision.append(np.nan)\n",
    "                #print('Performance metric is null')\n",
    "                \n",
    "        self.train()\n",
    "        return np.nanmean(roc_score), np.nanmean(blc_acc), np.nanmean(sensitivity), np.nanmean(specificity), np.nanmean(AUPR), np.nanmean(f1), np.nanmean(average_precision),np.nanmean(ECE_score),np.nanmean(ACE_score)\n",
    "\n",
    "    \n",
    "    def prob_to_labels(self, pred, threshold):\n",
    "\t    return (pred >= threshold).astype('int')\n",
    "\n",
    "    def unfreeze_model(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    @staticmethod\n",
    "    def freeze_network(model, freeze_level: int):\n",
    "        \"\"\"\n",
    "        Freezes specific layers of the model depending on the freeze_level argument:\n",
    "\n",
    "         0: freeze nothing\n",
    "        -1: freeze all BERT weights but not the task head\n",
    "        -2: freeze the pooling layer\n",
    "        -3: freeze the embedding layer\n",
    "        -4: freeze the task head but not the base layer\n",
    "        n>0: freeze the bottom n layers of the base model.\n",
    "        \"\"\"\n",
    "\n",
    "        model_bert = model.model.bert\n",
    "        model_tasks = model.model.tasks\n",
    "\n",
    "        model_bert_encoder = model.model.bert.encoder\n",
    "        model_bert_pooler = model.model.bert.pooler\n",
    "        model_bert_embeddings = model.model.bert.embeddings\n",
    "\n",
    "        if freeze_level == 0:\n",
    "            # freeze nothing\n",
    "            return\n",
    "\n",
    "        elif freeze_level > 0:\n",
    "            # freeze the encoder/transformer\n",
    "            n_encoder_layers = len(model_bert_encoder.layer)\n",
    "\n",
    "            # we'll always freeze layers bottom up - starting from layers closest to the embeddings\n",
    "            frozen_layers = min(freeze_level, n_encoder_layers)\n",
    "            #\n",
    "            for i in range(frozen_layers):\n",
    "                layer = model_bert_encoder.layer[i]\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        elif freeze_level == -1:\n",
    "            # freeze everything bert\n",
    "            for param in model_bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        elif freeze_level == -2:\n",
    "            # freeze the pooling layer\n",
    "            for param in model_bert_pooler.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        elif freeze_level == -3:\n",
    "            # freeze the embedding layer\n",
    "            for param in model_bert_embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        elif freeze_level == -4:\n",
    "            # freeze the task head\n",
    "            for param in model_tasks.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "# if representations does not exists & model weights are availble \n",
    "representaitons_path = config_dict['representation_dir'] + f\"invitro_pretrained_{epoch}.csv\"\n",
    "model_weights = config_dict['model_weights_dir'] + f\"epoch={epoch}-val_f1_score=0.00.ckpt\"\n",
    "\n",
    "if (os.path.exists(representaitons_path) == False) & (os.path.exists(model_weights) == True):\n",
    "    seed_everything(config_dict[\"seed\"])\n",
    "    model = MolbertModel(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SmilesMolbertModel(\n",
       "  (model): FlexibleBertModel(\n",
       "    (bert): SuperPositionalBertModel(\n",
       "      (embeddings): SuperPositionalBertEmbeddings(\n",
       "        (word_embeddings): Embedding(42, 768, padding_idx=0)\n",
       "        (position_embeddings): SuperPositionalEmbedding()\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (tasks): ModuleList(\n",
       "      (0): MaskedLMTask(\n",
       "        (loss): CrossEntropyLoss()\n",
       "        (masked_lm_head): BertLMPredictionHead(\n",
       "          (transform): BertPredictionHeadTransform(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (decoder): Linear(in_features=768, out_features=42, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): PhyschemTask(\n",
       "        (loss): MSELoss()\n",
       "        (physchem_head): PhysChemHead(\n",
       "          (physchem_clf): Sequential(\n",
       "            (0): Dropout(p=0.1, inplace=False)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=768, out_features=200, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): InvitroTasks(\n",
       "        (loss): MSELoss()\n",
       "        (invitro_head): invitro_head(\n",
       "          (invitro_clf): Sequential(\n",
       "            (0): Dropout(p=0.1, inplace=False)\n",
       "            (1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=2048, out_features=1234, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SmilesMolbertModel(config_dict)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
